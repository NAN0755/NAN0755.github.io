<!doctype html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />



  <meta name="google-site-verification" content="-haaFnsYIN3YdKTfE8bF5vzp879Tfd-BcLykx1LtSvk" />







  <meta name="baidu-site-verification" content="rCu1ZaC1sv" />







  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="DL,Paper,CV," />








  <link rel="shortcut icon" type="image/x-icon" href="/images/Z_Letter.ico?v=5.1.1" />






<meta name="description" content="Author: zengzeyuDate: 2019.05.11  YOLOv1简介YOLO为一种新的目标检测方法，该方法的特点是实现快速检测的同时还达到较高的准确率。作者将目标检测任务看作目标区域预测和类别预测的回归问题。该方法采用单个神经网络直接预测物品边界和类别概率，实现端到端的物品检测。同时，该方法检测速非常快，基础版可以达到45帧/s的实时检测；FastYOLO可以达到155帧/s。与">
<meta name="keywords" content="DL,Paper,CV">
<meta property="og:type" content="article">
<meta property="og:title" content="YOLO系列论文翻译">
<meta property="og:url" content="http://zengzeyu.com/2019/06/29/19_6_29/YOLO/index.html">
<meta property="og:site_name" content="Zeyu&#39;s Blog">
<meta property="og:description" content="Author: zengzeyuDate: 2019.05.11  YOLOv1简介YOLO为一种新的目标检测方法，该方法的特点是实现快速检测的同时还达到较高的准确率。作者将目标检测任务看作目标区域预测和类别预测的回归问题。该方法采用单个神经网络直接预测物品边界和类别概率，实现端到端的物品检测。同时，该方法检测速非常快，基础版可以达到45帧/s的实时检测；FastYOLO可以达到155帧/s。与">
<meta property="og:image" content="http://zengzeyu.com/images/19_6_29/YOLO/20190516165811568.png">
<meta property="og:image" content="http://zengzeyu.com/images/19_6_29/YOLO/20190516170312825.png">
<meta property="og:image" content="http://zengzeyu.com/images/19_6_29/YOLO/v2-ee4db90336d60d251d7254f9918c3a48_r.jpg">
<meta property="og:image" content="http://zengzeyu.com/images/19_6_29/YOLO/20190516170638756.png">
<meta property="og:image" content="http://zengzeyu.com/images/19_6_29/YOLO/20190516170728941.png">
<meta property="og:image" content="http://zengzeyu.com/images/19_6_29/YOLO/20190516170807390.png">
<meta property="og:image" content="http://zengzeyu.com/images/19_6_29/YOLO/20190516171321243.png">
<meta property="og:image" content="http://zengzeyu.com/images/19_6_29/YOLO/20190516171321243.png">
<meta property="og:image" content="http://zengzeyu.com/images/19_6_29/YOLO/20190516171357525.png">
<meta property="og:image" content="http://zengzeyu.com/images/19_6_29/YOLO/2019051617140744.png">
<meta property="og:image" content="http://zengzeyu.com/images/19_6_29/YOLO/20190516165655272.png">
<meta property="og:image" content="http://zengzeyu.com/images/19_6_29/YOLO/20190516165730919.png">
<meta property="og:image" content="http://zengzeyu.com/images/19_6_29/YOLO/v2-e79c2f41d984c69cd3aa805f86c6abe9_hd.png">
<meta property="og:image" content="http://zengzeyu.com/images/19_6_29/YOLO/v2-e79c2f41d984c69cd3aa805f86c6abe9_hd.png">
<meta property="og:image" content="http://zengzeyu.com/images/19_6_29/YOLO/20190516171655935.png">
<meta property="og:image" content="http://zengzeyu.com/images/19_6_29/YOLO/20190516171714280.png">
<meta property="og:image" content="http://zengzeyu.com/images/19_6_29/YOLO/20190516171744850.png">
<meta property="og:updated_time" content="2019-06-30T03:11:33.273Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="YOLO系列论文翻译">
<meta name="twitter:description" content="Author: zengzeyuDate: 2019.05.11  YOLOv1简介YOLO为一种新的目标检测方法，该方法的特点是实现快速检测的同时还达到较高的准确率。作者将目标检测任务看作目标区域预测和类别预测的回归问题。该方法采用单个神经网络直接预测物品边界和类别概率，实现端到端的物品检测。同时，该方法检测速非常快，基础版可以达到45帧/s的实时检测；FastYOLO可以达到155帧/s。与">
<meta name="twitter:image" content="http://zengzeyu.com/images/19_6_29/YOLO/20190516165811568.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: 'IEEVBE18SX',
      apiKey: '2ed347de4115ebbf86b2fdb5ed80c2c3',
      indexName: 'my_blog',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://zengzeyu.com/2019/06/29/19_6_29/YOLO/"/>





  <title>YOLO系列论文翻译 | Zeyu's Blog</title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  





  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?5bb12350481c6ccda395fbe33bd0dc13";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>











  
  
    
  

  <div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Zeyu's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  
  <div class="algolia-popup popup search-popup">
    <div class="algolia-search">
      <div class="algolia-search-input-icon">
        <i class="fa fa-search"></i>
      </div>
      <div class="algolia-search-input" id="algolia-search-input"></div>
    </div>

    <div class="algolia-results">
      <div id="algolia-stats"></div>
      <div id="algolia-hits"></div>
      <div id="algolia-pagination" class="algolia-pagination"></div>
    </div>

    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
  </div>




    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://zengzeyu.com/2019/06/29/19_6_29/YOLO/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zeyu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/uploads/bear.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zeyu's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">YOLO系列论文翻译</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-06-29T19:12:00+08:00">
                2019-06-29
              </time>
            

            

            
          </span>

          

          
            
          

          
          
             <span id="/2019/06/29/19_6_29/YOLO/" class="leancloud_visitors" data-flag-title="YOLO系列论文翻译">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数 </span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        <blockquote>
<p>Author: zengzeyu<br>Date: 2019.05.11</p>
</blockquote>
<h2 id="YOLOv1"><a href="#YOLOv1" class="headerlink" title="YOLOv1"></a>YOLOv1</h2><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>YOLO为一种新的目标检测方法，该方法的特点是实现快速检测的同时还达到较高的准确率。作者将目标检测任务看作目标区域预测和类别预测的回归问题。该方法采用单个神经网络直接预测物品边界和类别概率，实现端到端的物品检测。同时，该方法检测速非常快，基础版可以达到45帧/s的实时检测；FastYOLO可以达到155帧/s。与当前最好系统相比，YOLO目标区域定位误差更大，但是背景预测的假阳性优于当前最好的方法。</p>
<h3 id="1-前言"><a href="#1-前言" class="headerlink" title="1. 前言"></a>1. 前言</h3><p>人类视觉系统快速且精准，只需瞄一眼（You Only Look Once，YOLO）即可识别图像中物品及其位置。</p>
<p>传统目标检测系统采用deformable parts models (DPM)方法，通过滑动框方法提出目标区域，然后采用分类器来实现识别。近期的R-CNN类方法采用region proposal methods，首先生成潜在的bounding boxes，然后采用分类器识别这些bounding boxes区域。最后通过post-processing来去除重复bounding boxes来进行优化。这类方法流程复杂，存在速度慢和训练困难的问题。</p>
<p>本文中，我们将目标检测问题转换为直接从图像中提取bounding boxes和类别概率的单个回归问题，只需一眼（you only look once，YOLO）即可检测目标类别和位置。</p>
<p>YOLO采用单个卷积神经网络来预测多个bounding boxes和类别概率，如Figure-1所示。本方法相对于传统方法有如下有优点：</p>
<p>一，非常快。YOLO预测流程简单，速度很快。我们的基础版在Titan X GPU上可以达到45帧/s； 快速版可以达到150帧/s。因此，YOLO可以实现实时检测。</p>
<p>二，YOLO采用全图信息来进行预测。与滑动窗口方法和region proposal-based方法不同，YOLO在训练和预测过程中可以利用全图信息。Fast R-CNN检测方法会错误的将背景中的斑块检测为目标，原因在于Fast R-CNN在检测中无法看到全局图像。相对于Fast R-CNN，YOLO背景预测错误率低一半。</p>
<p>三，YOLO可以学习到目标的概括信息（generalizable representation），具有一定普适性。我们采用自然图片训练YOLO，然后采用艺术图像来预测。YOLO比其它目标检测方法（DPM和R-CNN）准确率高很多。</p>
<p>YOLO的准确率没有最好的检测系统准确率高。YOLO可以快速识别图像中的目标，但是准确定位目标（特别是小目标）有点困难。<br><img src="/images/19_6_29/YOLO/20190516165811568.png" alt="在这里插入图片描述"></p>
<h3 id="2-统一检测-Unified-Detection"><a href="#2-统一检测-Unified-Detection" class="headerlink" title="2. 统一检测(Unified Detection)"></a>2. 统一检测(Unified Detection)</h3><p>作者将目标检测的流程统一为单个神经网络。该神经网络采用整个图像信息来预测目标的bounding boxes的同时识别目标的类别，实现端到端实时目标检测任务。</p>
<p>如图Figure-2所示，YOLO首先将图像分为S×S的格子（grid cell）。如果一个目标的中心落入格子，该格子就负责检测该目标。每一个格子（grid cell）预测bounding boxes（B）和该boxes的置信值（confidence score）。置信值代表box包含一个目标的置信度。然后，我们定义置信值为 $Pr(Object) * IOU^{truth}_{pred}$。如果没有目标，则置信值为零。另外，我们希望预测的置信值和ground truth的intersection over union (IOU)相同。</p>
<p>每一个bounding box包含5个值：x，y，w，h和confidence。（x，y）代表与格子相关的box的中心。（w，h）为与全图信息相关的box的宽和高。confidence代表预测boxes的IOU和ground truth。</p>
<p>每个格子（grid cell）预测条件概率值C（$Pr(Class_i | Object)$）。概率值C代表了格子包含一个目标的概率，每一格子只预测一类概率。在测试时，每个box通过类别概率和box置信度相乘来得到特定类别置信分数：<br>$Pr(Class<em>i|Object)<em>Pr(Object)</em>IOU^{truth}</em>{pred} = Pr(Class<em>i)*IOU^{truth}</em>{pred}$</p>
<p>这个分数代表该类别出现在box中的概率和box和目标的合适度。在PASCAL VOC数据集上评价时，我们采用S=7，B=2，C=20（该数据集包含20个类别），最终预测结果为7×7×30的tensor。<br><img src="/images/19_6_29/YOLO/20190516170312825.png" alt="在这里插入图片描述"></p>
<h4 id="2-1-网络结构"><a href="#2-1-网络结构" class="headerlink" title="2.1 网络结构"></a>2.1 网络结构</h4><p>模型采用卷积神经网络结构。开始的卷积层提取图像特征，全连接层预测输出概率。模型结构类似于GoogleNet，如图Figure-3所示。作者还训练了YOLO的快速版本（Fast YOLO）。Fast YOLO模型卷积层和filter更少。最终输出为7×7×30的tensor。<br><img src="/images/19_6_29/YOLO/v2-ee4db90336d60d251d7254f9918c3a48_r.jpg" alt="在这里插入图片描述"></p>
<h4 id="2-2-训练方法"><a href="#2-2-训练方法" class="headerlink" title="2.2 训练方法"></a>2.2 训练方法</h4><p>作者采用ImageNet 1000-class 数据集来预训练卷积层。预训练阶段，采用图2-2网络中的前20卷积层，外加average-pooling 层和全连接层。模型训练了一周，获得了top-5 accuracy为0.88（ImageNet2012 validation set），与GoogleNet模型准确率相当。然后，将模型转换为检测模型。作者向预训练模型中加入了4个卷积层和两层全连接层，提高了模型输入分辨率（224×224-&gt;448×448）。顶层预测类别概率和bounding box协调值。bounding box的宽和高通过输入图像宽和高归一化到0-1区间。顶层采用linear activation，其它层使用 leaky rectified linear。作者采用sum-squared error为目标函数来优化，增加bounding box loss权重，减少置信度权重，实验中，设定为$\lambda<em>{coord} = 5 and \lambda</em>{noobj}=0.5$。</p>
<p>训练阶段的总loss函数如下：</p>
<p><img src="/images/19_6_29/YOLO/20190516170638756.png" alt="在这里插入图片描述"></p>
<p>作者在PASCAL VOC2007和PASCAL VOC2012数据集上进行了训练和测试。训练135轮，batch size为64，动量为0.9，学习速率延迟为0.0005. Learning schedule为：第一轮，学习速率从0.001缓慢增加到0.01（因为如果初始为高学习速率，会导致模型发散）；保持0.01速率到75轮；然后在后30轮中，下降到0.001；最后30轮，学习速率为0.0001.</p>
<p>作者还采用了dropout和 data augmentation来预防过拟合。dropout值为0.5；data augmentation包括：random scaling，translation，adjust exposure和saturation。</p>
<h4 id="2-3-预测"><a href="#2-3-预测" class="headerlink" title="2.3 预测"></a>2.3 预测</h4><p>对于PASCAL VOC数据集，模型需要对每张图片预测98个bounding box和对应的类别。对于大部分目标只包含一个box；其它有些面积大的目标包含了多个boxes，采用了Non-maximal suppression（非最大值抑制）来提高准确率。</p>
<h4 id="2-4-缺点"><a href="#2-4-缺点" class="headerlink" title="2.4 缺点"></a>2.4 缺点</h4><ol>
<li>YOLO的每一个网格只预测两个boxes，一种类别。这导致模型对相邻目标预测准确率下降。因此，YOLO对成队列的目标（如 一群鸟）识别准确率较低。</li>
<li>YOLO是从数据中学习预测bounding boxes，因此，对新的或者不常见角度的目标无法识别。</li>
<li>YOLO的loss函数对small bounding boxes和large bounding boxes的error平等对待，影响了模型识别准确率。因为对于小的bounding boxes，small error影响更大。</li>
</ol>
<h3 id="3-效果对比"><a href="#3-效果对比" class="headerlink" title="3. 效果对比"></a>3. 效果对比</h3><p>文中比较了YOLO和其它目标检测方法（Deformable parts models，R-CNN，Faster R-CNN，Deep MultiBox，OverFeat，MultiGrasp）</p>
<h3 id="4-实验结果"><a href="#4-实验结果" class="headerlink" title="4. 实验结果"></a>4. 实验结果</h3><h4 id="4-1-与其它检测方法效果对比"><a href="#4-1-与其它检测方法效果对比" class="headerlink" title="4.1 与其它检测方法效果对比"></a>4.1 与其它检测方法效果对比</h4><p>如表Table-1所示，在准确率保证的情况下，YOLO速度快于其它方法。</p>
<p><img src="/images/19_6_29/YOLO/20190516170728941.png" alt="在这里插入图片描述"></p>
<h4 id="4-2-VOC2007-错误项目分析"><a href="#4-2-VOC2007-错误项目分析" class="headerlink" title="4.2 VOC2007 错误项目分析"></a>4.2 VOC2007 错误项目分析</h4><p>文中比较了YOLO和Faster R-CNN的错误情况，结果如图4-1所示。YOLO定位错误率高于Fast R-CNN；Fast R-CNN背景预测错误率高于YOLO。</p>
<p>预测结果包括以下几类：</p>
<p>正确：类别正确，IOU&gt;0.5</p>
<p>定位：类别正确，0.1&lt;IOU&lt;0.5</p>
<p>类似：类别相似，IOU&gt;0.1</p>
<p>其它：类别错误，IOU&gt;0.1</p>
<p>背景：IOU&lt;0.1<br><img src="/images/19_6_29/YOLO/20190516170807390.png" alt="在这里插入图片描述"></p>
<p>图4-1 错误项目分析</p>
<h4 id="4-3-结合Fast-R-CNN和YOLO"><a href="#4-3-结合Fast-R-CNN和YOLO" class="headerlink" title="4.3 结合Fast R-CNN和YOLO"></a>4.3 结合Fast R-CNN和YOLO</h4><p>YOLO和Fast R-CNN预测错误类型不同，因此可以结合两类模型，提升结果。结果如表4-2所示。</p>
<p>表4-2 模型结合<br><img src="/images/19_6_29/YOLO/20190516171321243.png" alt="在这里插入图片描述"></p>
<h4 id="4-4-VOC-2012结果"><a href="#4-4-VOC-2012结果" class="headerlink" title="4.4 VOC 2012结果"></a>4.4 VOC 2012结果</h4><p>VOC2012数据集上测试结果如表4-3所示。</p>
<p>表4-3 VOC2012数据集测试结果</p>
<p><img src="/images/19_6_29/YOLO/20190516171321243.png" alt="在这里插入图片描述"></p>
<h4 id="4-5-普适性"><a href="#4-5-普适性" class="headerlink" title="4.5 普适性"></a>4.5 普适性</h4><p>我们在其它数据集（艺术品目标检测）：Picasso Dataset和People-Art Dataset测试了YOLO的性能，结果如图4-2和图4-3所示。</p>
<p>图4-2 艺术品目标检测结果一</p>
<p><img src="/images/19_6_29/YOLO/20190516171357525.png" alt="在这里插入图片描述"></p>
<p>图4-3 艺术品目标检测结果二 </p>
<p><img src="/images/19_6_29/YOLO/2019051617140744.png" alt="在这里插入图片描述"></p>
<h3 id="5-实时检测"><a href="#5-实时检测" class="headerlink" title="5. 实时检测"></a>5. 实时检测</h3><p>作者测试了YOLO的实时检测效果，结果参见 YouTube channel: <a href="https://goo.gl/bEs6Cj（可惜要翻墙才能看）。" target="_blank" rel="external">https://goo.gl/bEs6Cj（可惜要翻墙才能看）。</a></p>
<h3 id="6-结论"><a href="#6-结论" class="headerlink" title="6. 结论"></a>6. 结论</h3><p>YOLO为一种基于单独神经网络模型的目标检测方法，具有特点可以高准确率快速检测，同时具有一定鲁棒性，可以适用于实时目标检测。</p>
<p><strong>参考：</strong></p>
<p><a href="https://zhuanlan.zhihu.com/p/25045711" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/25045711</a></p>
<hr>
<h2 id="YOLOv2"><a href="#YOLOv2" class="headerlink" title="YOLOv2"></a>YOLOv2</h2><h3 id="简介-1"><a href="#简介-1" class="headerlink" title="简介"></a>简介</h3><p>YOLO9000是可以检测超过9000种类别的实时检测系统。首先，作者在YOLO基础上进行了一系列的改进，产生了YOLOv2。YOLOv2在PASCAL VOC和COCO数据集上获得了目前最好的结果（state of the art）。然后，采用多尺度训练方法，YOLOv2可以根据速度和精确度需求调整输入尺寸。67FPS时，YOLOv2在VOC2007数据集上可以达到76.8mAP；40FPS，可以达到78.6mAP，比目前最好的Faster R-CNN和SSD精确度更高，检测速度更快。最后提出了目标检测和分类的共训练方法。采用该方法，作者分别在COCO目标检测数据集和ImageNet分类数据集上训练了YOLO9000。联合训练使YOLO9000可以预测没有labelled的目标检测数据。YOLO9000在ImageNet验证集（200类）上获得了19.7mAP。其中，156类没有出现在COCO训练集中，YOLO9000获得了16.0mAP。YOLO9000可以实时识别超过9000类别。</p>
<h3 id="1-前言-1"><a href="#1-前言-1" class="headerlink" title="1. 前言"></a>1. 前言</h3><p>目标检测系统要求快速，准确以及能识别大范围种类数量。但是，目前基于深度神经网络方法的目前检测系统能识别的物品种类较少。其原因在于：相对于物品分类数据集，目标检测数据集中的物品种类较少。标记目标识别数据集所耗费的精力远大于标记物品分类数据集。物品分类数据集包含成千上万种超过数百万张图片，而目标识别数据集就很小了。</p>
<p>本文中，作者提出了一种结合不同类型数据集的方法。基于该方法，作者提出了一种新的联合训练方法，结合目前物品分类数据集的优点，将其应用于训练目标检测模型。模型可以从目标检测数据集中学会准确定位目标，同时从物品分类数据集中学会识别更多的种类，增强模型的鲁棒性。</p>
<p>采用该方法，作者训练了可以识别超过9000种物品的实时目标检测与识别系统-YOLO9000。首先，作者在YOLO的基础上进行了改进，产生了YOLOv2（state of the art）。然后，作者采用数据集结合方法和联合训练方法，采用ImageNet和COCO数据集训练该模型，使该模型可以识别和检测超过9000种类别。</p>
<h3 id="2-优化-YOLOv2"><a href="#2-优化-YOLOv2" class="headerlink" title="2. 优化-YOLOv2"></a>2. 优化-YOLOv2</h3><p>YOLO相对于目前最好的目标检测系统存在的问题是精确度不够。错误项目分析显示，相对于Fast R-CNN，YOLO在目标定位方面错误率较高。因此，对于YOLO的改进集中于在保持分类准确率的基础上增强定位精确度。改进的项目如Table 2所示。<br><img src="/images/19_6_29/YOLO/20190516165655272.png" alt="在这里插入图片描述"></p>
<h4 id="2-1-Batch-Normalization"><a href="#2-1-Batch-Normalization" class="headerlink" title="2.1 Batch Normalization"></a>2.1 Batch Normalization</h4><p>Batch Normalization可以提高模型收敛速度，减少过拟合。作者在所有卷积层应用了Batch Normalization，使结果提升了2%。同时，Batch Normalization的应用，去除了dropout，而不会过拟合。</p>
<h4 id="2-2-High-Resolution-Classifier"><a href="#2-2-High-Resolution-Classifier" class="headerlink" title="2.2 High Resolution Classifier"></a>2.2 High Resolution Classifier</h4><p>目前最好的图像分类器采用基于ImageNet数据集预训练模型。大部分类器输入图像尺寸小于256×256。原始YOLO接受图像尺寸为224×224。在YOLOv2中，作者首先采用448×448分辨率的ImageNet数据finetune使网络适应高分辨率输入；然后将该网络用于目标检测任务finetune。高分辨率输入使结果提升了4%mAP。</p>
<h4 id="2-3-Convolutional-With-Anchor-Boxes"><a href="#2-3-Convolutional-With-Anchor-Boxes" class="headerlink" title="2.3 Convolutional With Anchor Boxes"></a>2.3 Convolutional With Anchor Boxes</h4><p>YOLO采用全连接层来直接预测bounding boxes，而Fast R-CNN采用人工选择的bounding boxes。Fast R-CNN中的 region proposal network仅采用卷积层来预测固定的boxes（anchor boxes）的偏移和置信度。</p>
<p>作者去除了YOLO的全连接层，采用固定框（anchor boxes）来预测bounding boxes。首先，去除了一个pooling层来提高卷积层输出分辨率。然后，修改网络输入尺寸：由448×448改为416，使特征图只有一个中心。物品（特别是大的物品）更有可能出现在图像中心。YOLO的卷积层下采样率为32，因此输入尺寸变为416,输出尺寸为13×13。</p>
<p>采用anchor boxes，提升了精确度。YOLO每张图片预测98个boxes，但是采用anchor boxes，每张图片可以预测超过1000个boxes。YOLO模型精确度为69.5mAP，recall为81%；采用anchor boxes方法后，结果为69.2mAP，recall为88%。</p>
<h4 id="2-4-Dimension-Clusters"><a href="#2-4-Dimension-Clusters" class="headerlink" title="2.4 Dimension Clusters"></a>2.4 Dimension Clusters</h4><p>在YOLO模型上采用anchor boxes有两个关键。第一，box维度为人工选择。模型可以学会适应boxes，但是如果人工选择更好的boxes，可以让模型更加容易学习。我们采用K-means聚类方法来自动选择最佳的初始boxes。我们希望的是人工选择的boxes提高IOU分数，因此，我们公式定义为：$d(box, centroid) = 1 - IOU(box, centroid)$。k-means结果如图Figure-2所示，作者选择了k=5。</p>
<p><img src="/images/19_6_29/YOLO/20190516165730919.png" alt="在这里插入图片描述"></p>
<h4 id="2-5-Direct-location-prediction"><a href="#2-5-Direct-location-prediction" class="headerlink" title="2.5 Direct location prediction"></a>2.5 Direct location prediction</h4><p>在YOLO模型上采用anchor boxes的第二个关键是模型不稳定性，特别是在前面几轮训练。大部分不稳定因素来源于预测boxes位置（x，y）。</p>
<p>作者将预测偏移量改变为YOLO的预测grid cell的位置匹配性（location coordinate），将预测值限定在0-1范围内，增强稳定性。网络对feature map中的每个cell预测5个bounding boxes。对每一个bounding boxes，模型预测5个匹配性值（$t<em>{x},t</em>{y} ,t<em>{w} ,t</em>{h} ,t_{o}$）。采用k-means聚类方法选择boxes维度和直接预测bounding boxes中心位置提高YOLO将近5%准确率</p>
<h4 id="2-6-Fine-Grained-Features"><a href="#2-6-Fine-Grained-Features" class="headerlink" title="2.6 Fine-Grained Features"></a>2.6 Fine-Grained Features</h4><p>改进后的YOLO对13×13的feature map进行目标检测。更精确的特征（finer grained features）可以提高对于小目标的检测。作者向网络加入passtrough层以增加特征。passthrough类似于ResNet，将高分辨率特征和低分辨率特征结合，使26×26×512的特征图转化为13×13×2048的特征图。该改进增加了1%的性能。</p>
<h4 id="2-7-Multi-Scale-Training（多尺度训练）"><a href="#2-7-Multi-Scale-Training（多尺度训练）" class="headerlink" title="2.7 Multi-Scale Training（多尺度训练）"></a>2.7 Multi-Scale Training（多尺度训练）</h4><p>最初的YOLO输入尺寸为448×448，加入anchor boxes后，输入尺寸为416×416。模型只包含卷积层和pooling 层，因此可以随时改变输入尺寸。</p>
<p>作者在训练时，每隔几轮便改变模型输入尺寸，以使模型对不同尺寸图像具有鲁棒性。每个10 batches，模型随机选择一种新的输入图像尺寸（320，352，…，608，32的倍数，因为模型下采样因子为32），改变模型输入尺寸，继续训练。</p>
<p>该训练规则强迫模型取适应不同的输入分辨率。模型对于小尺寸的输入处理速度更快，因此YOLOv2可以按照需求调节速度和准确率。在低分辨率情况下（288×288），YOLOv2可以在保持和Fast R-CNN持平的准确率的情况下，处理速度可以达到90FPS。在高分辨率情况下，YOLOv2在VOC2007数据集上准确率可以达到state of the art（78.6mAP），如表2-2所示。</p>
<h3 id="3-检测更加快速-Faster"><a href="#3-检测更加快速-Faster" class="headerlink" title="3. 检测更加快速(Faster)"></a>3. 检测更加快速(Faster)</h3><p>大部分检测框架是基于VGG-16作为特征提取网络，但是VGG-16比较复杂，耗费计算量大。YOLO框架使用了类似googlenet的网络结构，计算量比VGG-16小，准确率比VGG16略低。</p>
<h4 id="3-1-Darknet-19"><a href="#3-1-Darknet-19" class="headerlink" title="3.1 Darknet-19"></a>3.1 Darknet-19</h4><p>作者设计了一个新的分类网络（Darknet-19）来作为YOLOv2的基础模型。Darknet-19模型结构如表3-1所示。</p>
<p><img src="/images/19_6_29/YOLO/v2-e79c2f41d984c69cd3aa805f86c6abe9_hd.png" alt="img"></p>
<h4 id="3-2-分类任务训练"><a href="#3-2-分类任务训练" class="headerlink" title="3.2 分类任务训练"></a>3.2 分类任务训练</h4><p>作者采用ImageNet1000类数据集来训练分类模型。训练过程中，采用了 random crops, rotations, and hue, saturation, and exposure shifts等data augmentation方法。预训练后，作者采用高分辨率图像（448×448）对模型进行finetune。</p>
<h4 id="3-3-检测任务训练"><a href="#3-3-检测任务训练" class="headerlink" title="3.3 检测任务训练"></a>3.3 检测任务训练</h4><p>作者将分类模型的最后一层卷积层去除，替换为三层卷积层（3×3,1024 filters），最后一层为1×1卷积层，filters数目为需要检测的数目。对于VOC数据集，我们需要预测5个boxes，每个boxes包含5个适应度值，每个boxes预测20类别。因此，输出为125（5<em>20+5</em>5） filters。最后还加入了passthough 层。</p>
<h3 id="4-更强-stronger"><a href="#4-更强-stronger" class="headerlink" title="4. 更强(stronger)"></a>4. 更强(stronger)</h3><p>作者提出了将分类数据和检测数据综合的联合训练机制。该机制使用目标检测标签的数据训练模型学习定位目标和检测部分类别的目标；再使用分类标签的数据取扩展模型对多类别的识别能力。在训练的过程中，混合目标检测和分类的数据集。当网络接受目标检测的训练数据，反馈网络采用YOLOv2 loss函数；当网络接受分类训练数据，反馈网络只更新部分网络参数。</p>
<p>这类训练方法有一定的难度。目标识别数据集仅包含常见目标和标签（比如狗，船）；分类数据集包含更广和更深的标签。比如狗，ImageNet上包含超过100种的狗的类别。如果要联合训练，需要将这些标签进行合并。</p>
<p>大部分分类方法采用softmax输出所有类别的概率。采用softmax的前提假设是类别之间不相互包含（比如，犬和牧羊犬就是相互包含）。因此，我们需要一个多标签的模型来综合数据集，使类别之间不相互包含。</p>
<h4 id="4-1-多层分类-Hierarchical-classification"><a href="#4-1-多层分类-Hierarchical-classification" class="headerlink" title="4.1 多层分类(Hierarchical classification)"></a>4.1 多层分类(Hierarchical classification)</h4><p>数据处理…</p>
<h3 id="5-结论"><a href="#5-结论" class="headerlink" title="5. 结论"></a>5. 结论</h3><p>作者通过对YOLO网络结构和训练方法的改进，提出了YOLOv2和YOLO9000两种实时目标检测系统。YOLOv2在YOLO的基础上进行了一系列的改进，在快速的同时达到state of the art。同时，YOLOv2可以适应不同的输入尺寸，<strong>根据需要调整检测准确率和检测速度（值得参考）</strong>。作者综合了ImageNet数据集和COCO数据集，采用联合训练的方式训练，使该系统可以识别超过9000种物品。除此之外，作者提出的WordTree可以综合多种数据集 的方法可以应用于其它计算机数觉任务中。</p>
<p><strong>参考：</strong></p>
<p><a href="https://zhuanlan.zhihu.com/p/25052190" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/25052190</a></p>
<hr>
<h2 id="YOLOv3"><a href="#YOLOv3" class="headerlink" title="YOLOv3"></a>YOLOv3</h2><h3 id="简介-2"><a href="#简介-2" class="headerlink" title="简介"></a>简介</h3><p>本文为YOLO提供了一系列更新！它包含一堆小设计，可以使系统的性能得到更新；也包含一个新训练的、非常棒的神经网络，虽然比上一版更大一些，但精度也提高了。不用担心，虽然体量大了点，它的速度还是有保障的。在输入320×320的图片后，YOLOv3能在22毫秒内完成处理，并取得28.2mAP的成绩。它的精度和SSD相当，但速度要快上3倍。和旧版数据相比，v3版进步明显。在Titan X环境下，YOLOv3的检测精度为57.9AP5057.9AP50，用时51ms；而RetinaNet的精度只有57.5AP，但却需要198ms，相当于YOLOv3的3.8倍。</p>
<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>无</p>
<h3 id="2-方法"><a href="#2-方法" class="headerlink" title="2. 方法"></a>2. 方法</h3><p><img src="/images/19_6_29/YOLO/v2-e79c2f41d984c69cd3aa805f86c6abe9_hd.png" alt="在这里插入图片描述"></p>
<h4 id="2-1-Bounding-Box-Prediction"><a href="#2-1-Bounding-Box-Prediction" class="headerlink" title="2.1 Bounding Box Prediction"></a>2.1 Bounding Box Prediction</h4><p>这里和原来v2基本没区别。仍然使用聚类产生anchor box的长宽（下式的pwpw和phph）。网络预测四个值：tx，ty，tw，th。我们知道，YOLO网络最后输出是一个M×M的feature map，对应于M×M个cell。如果某个cell距离image的top left corner距离为(cx,cy)（也就是cell的坐标），那么该cell内的bounding box的位置和形状参数为：</p>
<p><img src="/images/19_6_29/YOLO/20190516171655935.png" alt="在这里插入图片描述"></p>
<p>PS：这里有一个问题，不管FasterRCNN还是YOLO，都不是直接回归bounding box的长宽（就像这样：$bw=p_w*e^{t_w}$），而是要做一个对数变换，实际预测的是$log(⋅)$。这里小小解释一下。</p>
<p>这是因为如果不做变换，直接预测相对形变$t_w$，那么要求$t_w&gt;0$，因为你的框框的长宽不可能是负数。这样，是在做一个有不等式条件约束的优化问题，没法直接用SGD来做。所以先取一个对数变换，将其不等式约束去掉，就可以了。<br><img src="/images/19_6_29/YOLO/20190516171714280.png" alt="在这里插入图片描述"></p>
<p>在训练的时候，使用平方误差损失。</p>
<p>另外，YOLO会对每个bounding box给出是否是object的置信度预测，用来区分objects和背景。这个值使用logistic回归。当某个bounding box与ground truth的IoU大于其他所有bounding box时，target给11；如果某个bounding box不是IoU最大的那个，但是IoU也大于了某个阈值（我们取0.5），那么我们忽略它（既不惩罚，也不奖励），这个做法是从Faster RCNN借鉴的。我们对每个ground truth只分配一个最好的bounding box与其对应（这与Faster RCNN不同）。如果某个bounding box没有被assign到任何一个ground truth对应，那么它对边框位置大小的回归和class的预测没有贡献，我们只惩罚它的objectness，即试图减小其confidence。</p>
<h4 id="2-2-分类预测-Class-Prediction"><a href="#2-2-分类预测-Class-Prediction" class="headerlink" title="2.2 分类预测(Class Prediction)"></a>2.2 分类预测(Class Prediction)</h4><p>我们不用softmax做分类了，而是使用独立的logisitc做二分类。这种方法的好处是可以处理重叠的多标签问题，如Open Image Dataset。在其中，会出现诸如<code>Woman</code>和<code>Person</code>这样的重叠标签。</p>
<h4 id="2-3-多尺度预测-Prediction-Across-Scales"><a href="#2-3-多尺度预测-Prediction-Across-Scales" class="headerlink" title="2.3 多尺度预测(Prediction Across Scales)"></a>2.3 多尺度预测(Prediction Across Scales)</h4><p>之前YOLO的一个弱点就是缺少多尺度变换，使用<a href="https://arxiv.org/abs/1612.03144" target="_blank" rel="external">FPN</a>中的思路，v3在3个不同的尺度上做预测。在COCO上，我们每个尺度都预测3个框框，所以一共是9个。所以输出的feature map的大小是N×N×[3×(4+1+80)]]。</p>
<p>然后我们从两层前那里拿feature map，upsample 2x，并与更前面输出的feature map通过element-wide的相加做merge。这样我们能够从后面的层拿到更多的高层语义信息，也能从前面的层拿到细粒度的信息（更大的feature map，更小的感受野）。然后在后面接一些conv做处理，最终得到和上面相似大小的feature map，只不过spatial dimension变成了2倍。</p>
<p>照上一段所说方法，再一次在final scale尺度下给出预测。</p>
<h4 id="2-4-特征提取器-Feature-Extractor"><a href="#2-4-特征提取器-Feature-Extractor" class="headerlink" title="2.4 特征提取器(Feature Extractor)"></a>2.4 特征提取器(Feature Extractor)</h4><p>加入Res_Block的Darknet-53。<br><img src="/images/19_6_29/YOLO/20190516171744850.png" alt="在这里插入图片描述"></p>
<h3 id="3-How-We-Do"><a href="#3-How-We-Do" class="headerlink" title="3. How We Do"></a>3. How We Do</h3><p>把YOLO v3和其他方法比较，优势在于快快快。当你不太在乎IoU一定要多少多少的时候，YOLO可以做到又快又好。作者还在文章的结尾发起了这样的牢骚：使用了多尺度预测，v3对于小目标的检测结果明显变好了。不过对于medium和large的目标，表现相对不好。这是需要后续工作进一步挖局的地方。</p>
<h3 id="4-Things-We-Tried-That-Didn’t-Work"><a href="#4-Things-We-Tried-That-Didn’t-Work" class="headerlink" title="4. Things We Tried That Didn’t Work"></a>4. Things We Tried That Didn’t Work</h3><p>作者还贴心地给出了什么方法没有奏效。</p>
<ul>
<li>anchor box坐标(x,y)(x,y)的预测。预测anchor box的offset，no stable，不好。</li>
<li>线性offset预测，而不是logistic。精度下降。</li>
<li>focal loss。精度下降。</li>
<li>双IoU阈值，像Faster RCNN那样。效果不好。</li>
</ul>
<p><strong>参考：</strong></p>
<p><a href="https://xmfbit.github.io/2018/04/01/paper-yolov3/" target="_blank" rel="external">https://xmfbit.github.io/2018/04/01/paper-yolov3/</a></p>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/DL/" rel="tag"># DL</a>
          
            <a href="/tags/Paper/" rel="tag"># Paper</a>
          
            <a href="/tags/CV/" rel="tag"># CV</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/03/24/19_3_24/My-Paper-Lib-2018/" rel="next" title="My Paper Lib 2018">
                <i class="fa fa-chevron-left"></i> My Paper Lib 2018
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/06/29/19_6_29/boostserialize(序列化)和boostdeserialize(反序列化)类对象型数据并通过socket网络传输 - 副本/" rel="prev" title="boost serialize(序列化)和boost deserialize(反序列化)类对象型数据并通过socket网络传输">
                boost serialize(序列化)和boost deserialize(反序列化)类对象型数据并通过socket网络传输 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/uploads/bear.jpg"
               alt="Zeyu" />
          <p class="site-author-name" itemprop="name">Zeyu</p>
           
              <p class="site-description motion-element" itemprop="description">Done is better than perfect.</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives/">
                <span class="site-state-item-count">23</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">8</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">17</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/zeyu-hello" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="mailto:zengzeyu@hotmail.com" target="_blank" title="E-mail">
                  
                    <i class="fa fa-fw fa-envelope"></i>
                  
                  E-mail
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#YOLOv1"><span class="nav-number">1.</span> <span class="nav-text">YOLOv1</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#简介"><span class="nav-number">1.1.</span> <span class="nav-text">简介</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-前言"><span class="nav-number">1.2.</span> <span class="nav-text">1. 前言</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-统一检测-Unified-Detection"><span class="nav-number">1.3.</span> <span class="nav-text">2. 统一检测(Unified Detection)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-网络结构"><span class="nav-number">1.3.1.</span> <span class="nav-text">2.1 网络结构</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-训练方法"><span class="nav-number">1.3.2.</span> <span class="nav-text">2.2 训练方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-预测"><span class="nav-number">1.3.3.</span> <span class="nav-text">2.3 预测</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-4-缺点"><span class="nav-number">1.3.4.</span> <span class="nav-text">2.4 缺点</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-效果对比"><span class="nav-number">1.4.</span> <span class="nav-text">3. 效果对比</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-实验结果"><span class="nav-number">1.5.</span> <span class="nav-text">4. 实验结果</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-1-与其它检测方法效果对比"><span class="nav-number">1.5.1.</span> <span class="nav-text">4.1 与其它检测方法效果对比</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-2-VOC2007-错误项目分析"><span class="nav-number">1.5.2.</span> <span class="nav-text">4.2 VOC2007 错误项目分析</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-3-结合Fast-R-CNN和YOLO"><span class="nav-number">1.5.3.</span> <span class="nav-text">4.3 结合Fast R-CNN和YOLO</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-4-VOC-2012结果"><span class="nav-number">1.5.4.</span> <span class="nav-text">4.4 VOC 2012结果</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-5-普适性"><span class="nav-number">1.5.5.</span> <span class="nav-text">4.5 普适性</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-实时检测"><span class="nav-number">1.6.</span> <span class="nav-text">5. 实时检测</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-结论"><span class="nav-number">1.7.</span> <span class="nav-text">6. 结论</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#YOLOv2"><span class="nav-number">2.</span> <span class="nav-text">YOLOv2</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#简介-1"><span class="nav-number">2.1.</span> <span class="nav-text">简介</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-前言-1"><span class="nav-number">2.2.</span> <span class="nav-text">1. 前言</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-优化-YOLOv2"><span class="nav-number">2.3.</span> <span class="nav-text">2. 优化-YOLOv2</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-Batch-Normalization"><span class="nav-number">2.3.1.</span> <span class="nav-text">2.1 Batch Normalization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-High-Resolution-Classifier"><span class="nav-number">2.3.2.</span> <span class="nav-text">2.2 High Resolution Classifier</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-Convolutional-With-Anchor-Boxes"><span class="nav-number">2.3.3.</span> <span class="nav-text">2.3 Convolutional With Anchor Boxes</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-4-Dimension-Clusters"><span class="nav-number">2.3.4.</span> <span class="nav-text">2.4 Dimension Clusters</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-5-Direct-location-prediction"><span class="nav-number">2.3.5.</span> <span class="nav-text">2.5 Direct location prediction</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-6-Fine-Grained-Features"><span class="nav-number">2.3.6.</span> <span class="nav-text">2.6 Fine-Grained Features</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-7-Multi-Scale-Training（多尺度训练）"><span class="nav-number">2.3.7.</span> <span class="nav-text">2.7 Multi-Scale Training（多尺度训练）</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-检测更加快速-Faster"><span class="nav-number">2.4.</span> <span class="nav-text">3. 检测更加快速(Faster)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-Darknet-19"><span class="nav-number">2.4.1.</span> <span class="nav-text">3.1 Darknet-19</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-分类任务训练"><span class="nav-number">2.4.2.</span> <span class="nav-text">3.2 分类任务训练</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-检测任务训练"><span class="nav-number">2.4.3.</span> <span class="nav-text">3.3 检测任务训练</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-更强-stronger"><span class="nav-number">2.5.</span> <span class="nav-text">4. 更强(stronger)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-1-多层分类-Hierarchical-classification"><span class="nav-number">2.5.1.</span> <span class="nav-text">4.1 多层分类(Hierarchical classification)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-结论"><span class="nav-number">2.6.</span> <span class="nav-text">5. 结论</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#YOLOv3"><span class="nav-number">3.</span> <span class="nav-text">YOLOv3</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#简介-2"><span class="nav-number">3.1.</span> <span class="nav-text">简介</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#前言"><span class="nav-number">3.2.</span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-方法"><span class="nav-number">3.3.</span> <span class="nav-text">2. 方法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-Bounding-Box-Prediction"><span class="nav-number">3.3.1.</span> <span class="nav-text">2.1 Bounding Box Prediction</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-分类预测-Class-Prediction"><span class="nav-number">3.3.2.</span> <span class="nav-text">2.2 分类预测(Class Prediction)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-多尺度预测-Prediction-Across-Scales"><span class="nav-number">3.3.3.</span> <span class="nav-text">2.3 多尺度预测(Prediction Across Scales)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-4-特征提取器-Feature-Extractor"><span class="nav-number">3.3.4.</span> <span class="nav-text">2.4 特征提取器(Feature Extractor)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-How-We-Do"><span class="nav-number">3.4.</span> <span class="nav-text">3. How We Do</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-Things-We-Tried-That-Didn’t-Work"><span class="nav-number">3.5.</span> <span class="nav-text">4. Things We Tried That Didn’t Work</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy;  2017 - 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-id-card"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zeyu</span>
</div>



        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>


        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>

  
  <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.1"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.1"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.1"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.1"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.1"></script>



  


  




	





  





  





  






  




  
  
  
  <link rel="stylesheet" href="/lib/algolia-instant-search/instantsearch.min.css">

  
  
  <script src="/lib/algolia-instant-search/instantsearch.min.js"></script>
  

  <script src="/js/src/algolia-search.js?v=5.1.1"></script>



  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script>
  <script>AV.initialize("g992xG0S8k31XQHoAEAhRQbE-gzGzoHsz", "BdWgkDuuoBKUAUvH1b0e3RJF");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  

  
  


  

  

</body>
</html>
