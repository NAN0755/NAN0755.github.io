<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Zeyu&#39;s Blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://zengzeyu.com/"/>
  <updated>2020-07-15T04:12:14.329Z</updated>
  <id>http://zengzeyu.com/</id>
  
  <author>
    <name>Zeyu</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>TensorRT 教程（二）：TensorRT 源码简介</title>
    <link href="http://zengzeyu.com/2020/07/09/tensorrt_02_introduction/"/>
    <id>http://zengzeyu.com/2020/07/09/tensorrt_02_introduction/</id>
    <published>2020-07-09T11:15:02.000Z</published>
    <updated>2020-07-15T04:12:14.329Z</updated>
    
    <content type="html"><![CDATA[<p>NVIDIA TensorRT是一种高性能神经网络推理(Inference)引擎，用于在生产环境中部署深度学习应用程序，应用有图像分类、分割和目标检测等，可提供最大的推理吞吐量和效率。TensorRT是第一款可编程推理加速器，能加速现有和未来的网络架构。 <a id="more"></a></p><h2 id="1-TensorRT-库构成"><a href="#1-TensorRT-库构成" class="headerlink" title="1. TensorRT 库构成"></a>1. TensorRT 库构成</h2><p>以编译后源码压缩包安装方式进行安装的TensorRT库主要有以下文件夹：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">├── data</span><br><span class="line">├── doc</span><br><span class="line">├── include  # 所有头文件，可以查看所有函数的接口和说明</span><br><span class="line">├── lib      # 所有动态链接库.so文件</span><br><span class="line">├── python   # python API 例子</span><br><span class="line">└── samples  # c++ 例子</span><br></pre></td></tr></table></figure><h3 id="1-1-include文件夹"><a href="#1-1-include文件夹" class="headerlink" title="1.1 include文件夹"></a>1.1 <code>include</code>文件夹</h3><p><code>include</code>文件夹内部包含文件如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">├── include</span><br><span class="line">│   ├── NvCaffeParser.h</span><br><span class="line">│   ├── NvInfer.h</span><br><span class="line">│   ├── NvInferPlugin.h</span><br><span class="line">│   ├── NvInferPluginUtils.h</span><br><span class="line">│   ├── NvInferRuntimeCommon.h</span><br><span class="line">│   ├── NvInferRuntime.h</span><br><span class="line">│   ├── NvInferVersion.h</span><br><span class="line">│   ├── NvOnnxConfig.h</span><br><span class="line">│   ├── NvOnnxParser.h</span><br><span class="line">│   ├── NvOnnxParserRuntime.h</span><br><span class="line">│   ├── NvUffParser.h</span><br><span class="line">│   └── NvUtils.h</span><br></pre></td></tr></table></figure><p>下面按照文件之间依赖关系，大致介绍上述头文件。</p><ul><li><code>NvInferRuntimeCommon.h</code> ：定义了 TRT 运行时所需的基础数据结构（如<code>Dims</code>，<code>PluginField</code>）和大部分基类（<code>class ILogger</code>， <code>class IPluginV2</code>）</li><li><code>NvInferRuntime.h</code>: 继承<code>NvInferRuntimeCommon.h</code>中的基类，定义了runtime时的拓展功能子类</li><li><code>NvInfer.h</code>：继承<code>NvInferRuntime.h</code>中的基类，定义了大部分可以直接加入的神经网络层（如<code>class IConvolutionLayer</code>，<code>class IPoolingLayer</code>，<code>class IPoolingLayer</code>）</li><li><code>NvInferPluginUtils.h</code>：定义<code>plugin layer</code>所需的基础数据结构</li><li><code>NvInferPlugin.h</code>：初始化注册所有苦衷包含<code>plugin layer</code></li><li>其他的从文件名即可看出时分别Caffe，ONNX，UFF 的解析器parser，分别对应训练后的Caffe，Pytorch，TensorFlow网络</li></ul><h3 id="1-2-lib以及其他文件夹"><a href="#1-2-lib以及其他文件夹" class="headerlink" title="1.2 lib以及其他文件夹"></a>1.2 lib以及其他文件夹</h3><p> NVIDIA官方在 release TRT-7.0 版本时说会放出源码，然而还是以编译后动态链接库的方式放出库文件，所以，老黄的话也不能信，这是人家吃饭的家伙，应该不会放出源码。动态链接库里包含了一些核心的优化代码，比如说如何将 <em>conv+bn+relu</em> 三个层优化成一层，做了哪些工作，这些都在lib文件夹下的<code>.so</code> 里；还有建立整个网络之后使用什么方法寻找一个精度和速度平衡的最终 TRT runtime engine。</p><p>samples 下的例子，如果是写 c++ 代码的同学，一定要看看，很多现实中遇到的问题，例子里都有，而且还有一些现成的官方代码，比如说<code>Mask-RCNN</code>帮助写了很多<code>plugin layer</code>，就不用自己实现了。</p><p>Python 下的例子包含一些主要用的功能，比如说 parse 网络参数，生成 TRT engine 等。</p><h2 id="2-TRT使用流程简介"><a href="#2-TRT使用流程简介" class="headerlink" title="2. TRT使用流程简介"></a>2. TRT使用流程简介</h2><p>TRT 大致的工作流程是使用已经训练好的网络参数，将其转换成 TRT engine 序列化代码，供 GPU 进行计算，下图下面将简要介绍三个阶段。</p><p><img data-src="/images/20_07_09/fit1.png"></p><h3 id="2-1-Training"><a href="#2-1-Training" class="headerlink" title="2.1 Training"></a>2.1 Training</h3><p>也就是模型训练阶段，这部分如上图所示，目前主流的训练框架都支持。区别只是在从训练框架中提取模型参数权重的方式和所用中间 parser 库不同。</p><h3 id="2-2-Developing-A-Deployment-Solution"><a href="#2-2-Developing-A-Deployment-Solution" class="headerlink" title="2.2 Developing A Deployment Solution"></a>2.2 Developing A Deployment Solution</h3><p> 此阶段就是如何生成一个优化后的 TRT 模型，也是最重要的阶段。这一阶段目标点是，如何在更快地保证 TRT模型运行速度的前提下，保证相对与训练模型精度在可接受范围内。由于模型训练阶段一般使用 float32 精度（当然也有混合精度训练和量化训练），TRT 还支持其他可选项，比如如下图所示的：</p><ul><li><code>Precision Calibration</code>: TRT 目前支持 INT8 量化</li><li><code>Layer &amp; Tensor Fusion</code>: 如上文提到的 conv+bn+relu的融合</li><li><code>Kernel Auto-Tuning</code>: 略</li><li><code>Dynamic Tensor Memory</code> : 略</li><li><code>Multi-Stream Execution</code>: 支持多模型的stream流并行处理</li></ul><p><img data-src="/images/20_07_09/fit2.png"></p><p>通过上面的这些步骤，得到优化后的 TRT 模型，就可以通过 serialize 之后保存下来，使得下一次可直接 deserialize 保存下来的 TRT 模型进行 inference。</p><h3 id="2-3-Deploying-A-Solution"><a href="#2-3-Deploying-A-Solution" class="headerlink" title="2.3  Deploying A Solution"></a>2.3  Deploying A Solution</h3><p>将上一阶段序列化之后保存下来的模型，部署到实际生产环境中。由于TRT模型在 <em>Developing A Deployment Solution</em></p><p>是针对平台进行优化的，因此如果在平台A上得到的模型在平台B上不一定适用，所以可以理解为 TRT 针对的是 platform-wise 级别的优化，比如在1080Ti上得到的模型在2080Ti上甚至不能使用。</p><h2 id="3-TRT是如何工作的？"><a href="#3-TRT是如何工作的？" class="headerlink" title="3. TRT是如何工作的？"></a>3. TRT是如何工作的？</h2><p>TRT 内部有一整套流程来帮助加速 inference，大致内容已经在2.2节中有阐述，有兴趣可看原英文说明。对此，我们并不关心，因为所有这些方法都在未开源的代码中，也没办法自己 hack，所以开始进行 build 之后，我们所有能做的只是等TRT model 的生成。</p><h2 id="4-TRT能做些什么？"><a href="#4-TRT能做些什么？" class="headerlink" title="4. TRT能做些什么？"></a>4. TRT能做些什么？</h2><p>总的来说，TRT 可以在4个方面帮助开发者进行工作：导入，校准，生成和部署（import, calibration, generate and deploy）模型。</p><ol><li>导入模型参数<br>TRT可以直接定义模型，但是我们通常并不这么做，因为对于现有深度学习训练框架，如Pytorch和TensorFlow，我们通常使用对应的parser直接获得所需的模型结构和参数，然后进行下一步。TRT包括的parser有如下：Caffe， UFF，ONNX。</li><li>配置optimization profile<br>例如是否配置layer的动态输入输出。</li><li>builder<br>从我的角度理解，builder更像是一个外部的配置管理，包括模型的运行资源和INT8等配置。</li><li>engine<br>通过builder生成的TRT engine就是我们最终需要的文件，这个文件可以在inference阶段直接载入之后进行运行。</li></ol><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文介绍了TRT库的组成，工作流程和工作原理。</p><p>下一篇文章开始，我们将以Pytorch训练得到的Mask RCNN为例，将TRT的工作流程展开讲解，方便对各个细节进行研究。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;NVIDIA TensorRT是一种高性能神经网络推理(Inference)引擎，用于在生产环境中部署深度学习应用程序，应用有图像分类、分割和目标检测等，可提供最大的推理吞吐量和效率。TensorRT是第一款可编程推理加速器，能加速现有和未来的网络架构。
    
    </summary>
    
    
    
      <category term="DL" scheme="http://zengzeyu.com/tags/DL/"/>
    
      <category term="CV" scheme="http://zengzeyu.com/tags/CV/"/>
    
      <category term="TensorRT" scheme="http://zengzeyu.com/tags/TensorRT/"/>
    
      <category term="Model Deployment" scheme="http://zengzeyu.com/tags/Model-Deployment/"/>
    
  </entry>
  
  <entry>
    <title>TensorRT 教程（一）：安装TensorRT</title>
    <link href="http://zengzeyu.com/2020/07/09/tensorrt_01_installation/"/>
    <id>http://zengzeyu.com/2020/07/09/tensorrt_01_installation/</id>
    <published>2020-07-09T11:13:01.000Z</published>
    <updated>2020-07-15T04:10:42.803Z</updated>
    
    <content type="html"><![CDATA[<p>TensorRT 是一个用于在 NVIDIA GPU 上进行高性能计算的 C++ 库。TensorRT 在运行时，加载一个已经训练好的神经网络，创建一个经过内部高度优化的引擎（engine），来进行快速计算。TensorRT 同时提供 C++ 和 Python API 接口。TensorRT 同时支持 Windows/Ubuntu/iOS 系统，本教程将基于 Ubuntu 进行讲解。<a id="more"></a></p><h2 id="1-安装依赖"><a href="#1-安装依赖" class="headerlink" title="1. 安装依赖"></a>1. 安装依赖</h2><p>TensorRT 具有以下依赖，请先安装：</p><ul><li>CUDA 9.0， 10.0 ，或者 10.2</li><li>cuDNN 7.6.5</li><li>Python 2 / Python 3 （可选）</li></ul><h2 id="2-下载TensorRT"><a href="#2-下载TensorRT" class="headerlink" title="2. 下载TensorRT"></a>2. 下载TensorRT</h2><p>根据 TensorRT 官网介绍，使用压缩包安装可方便后续开发，当然也可选择 <strong>Debian/NPM</strong> 安装方式。本文将按照 <strong>tar</strong> 压缩包方式进行安装。按照下面流程即可快速安装，记得先注册一个 NVIDIA 账号：</p><blockquote><ol><li>进入官网：<a href="https://developer.nvidia.com/tensorrt" target="_blank" rel="noopener">https://developer.nvidia.com/tensorrt</a></li><li>点击 <strong>Download Now</strong>，然后登录NVIDIA账号</li><li>选择需要安装的TensorRT版本（此处选择 7.0）</li><li>同意软件安装许可</li><li>点击软件包下载链接，随后开始下载</li></ol></blockquote><h2 id="3-安装-TensorRT"><a href="#3-安装-TensorRT" class="headerlink" title="3. 安装 TensorRT"></a>3. 安装 TensorRT</h2><h3 id="3-1-安装-TensorRT-源码"><a href="#3-1-安装-TensorRT-源码" class="headerlink" title="3.1 安装 TensorRT 源码"></a>3.1 安装 TensorRT 源码</h3><p>将下载好的压缩包，放在一个路径下，比如在我的电脑：<code>/home/TensorRT-7</code>。然后将 TensorRT 的 <code>lib</code> 文件路径加入系统环境变量:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo gedit ~&#x2F;.bashrc&#96;  # sudo vi ~&#x2F;.bashrc</span><br><span class="line">export LD_LIBRARY_PATH&#x3D;$LD_LIBRARY_PATH:&lt;TensorRT-$&#123;version&#125;&#x2F;lib&gt;</span><br><span class="line">source ~&#x2F;.bashrc</span><br></pre></td></tr></table></figure><h3 id="3-2-安装-Python-接口"><a href="#3-2-安装-Python-接口" class="headerlink" title="3.2 安装 Python 接口"></a>3.2 安装 Python 接口</h3><p>如果需要用 Python API 进行编程，按照如下进行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd TensorRT-$&#123;version&#125;&#x2F;python</span><br><span class="line">sudo pip2 install tensorrt-*-cp27-none-linux_x86_64.whl  # python2</span><br><span class="line"># sudo pip3 install tensorrt-*-cp3x-none-linux_x86_64.whl  # python3</span><br></pre></td></tr></table></figure><h3 id="3-3-安装UFF转换库"><a href="#3-3-安装UFF转换库" class="headerlink" title="3.3 安装UFF转换库"></a>3.3 安装UFF转换库</h3><p>如果你想将 TensorFlow 训练后的网络通过 UFF 编码方式转换给 TensorRT 进行运行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd TensorRT-$&#123;version&#125;&#x2F;uff</span><br><span class="line">sudo pip2 install uff-0.6.5-py2.py3-none-any.whl   # python2</span><br><span class="line"># sudo pip3 install uff-0.6.5-py2.py3-none-any.whl  # python3</span><br></pre></td></tr></table></figure><h3 id="3-4-安装-graphsurgeon"><a href="#3-4-安装-graphsurgeon" class="headerlink" title="3.4 安装 graphsurgeon"></a>3.4 安装 graphsurgeon</h3><p><strong>graphsurgeon</strong> 是对UFF编码网络进行定制化操作的库，比如插入或删除神经网络某一层<code>layer</code>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd TensorRT-$&#123;version&#125;&#x2F;graphsurgeon</span><br><span class="line">sudo pip2 install graphsurgeon-0.4.1-py2.py3-none-any.whl  # python2</span><br><span class="line"># sudo pip3 install graphsurgeon-0.4.1-py2.py3-none-any.whl  # python3</span><br></pre></td></tr></table></figure><h2 id="4-验证安装"><a href="#4-验证安装" class="headerlink" title="4. 验证安装"></a>4. 验证安装</h2><ol><li>查看TensorRT的安装目录下文件是否齐全，可使用命令<code>tree -d</code>，会看到包含以下文件夹：<code>lib</code>，<code>include</code>，<code>data</code>…</li><li>运行例子<code>sampleMNIST</code>:<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd &lt;TensorRT root directory&gt;&#x2F;samples&#x2F;sampleMNIST</span><br><span class="line">make</span><br><span class="line">.&#x2F;sample_mnist</span><br></pre></td></tr></table></figure>如果上述命令无ERROR输出,则证明安装成功。</li></ol><p>至此，TensorRT安装成功！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;TensorRT 是一个用于在 NVIDIA GPU 上进行高性能计算的 C++ 库。TensorRT 在运行时，加载一个已经训练好的神经网络，创建一个经过内部高度优化的引擎（engine），来进行快速计算。TensorRT 同时提供 C++ 和 Python API 接口。TensorRT 同时支持 Windows/Ubuntu/iOS 系统，本教程将基于 Ubuntu 进行讲解。
    
    </summary>
    
    
    
      <category term="DL" scheme="http://zengzeyu.com/tags/DL/"/>
    
      <category term="CV" scheme="http://zengzeyu.com/tags/CV/"/>
    
      <category term="TensorRT" scheme="http://zengzeyu.com/tags/TensorRT/"/>
    
      <category term="Model Deployment" scheme="http://zengzeyu.com/tags/Model-Deployment/"/>
    
  </entry>
  
  <entry>
    <title>Mask R-CNN 网络结构详解</title>
    <link href="http://zengzeyu.com/2020/07/09/Mask%20R-CNN/"/>
    <id>http://zengzeyu.com/2020/07/09/Mask%20R-CNN/</id>
    <published>2020-07-09T11:12:00.000Z</published>
    <updated>2020-07-15T04:33:20.424Z</updated>
    
    <content type="html"><![CDATA[<p>Mask R-CNN 作为经典的two-stage目标检测模型被广泛使用，相关的讲解论文已经有很多，本文将着重在结合实际代码模型结构以及 layer 方面的讲解，这样会对模型有更加清晰的认识。本文以 <a href="https://github.com/facebookresearch/detectron2/blob/master/GETTING_STARTED.md#training--evaluation-in-command-line" target="_blank" rel="noopener">Detectron2 mask_rcnn_R_50_FPN_1x.yaml</a> 默认 config 生成的 Mask R-CNN 进行讲解。<a id="more"></a></p><h2 id="1-Mask-R-CNN-整体结构"><a href="#1-Mask-R-CNN-整体结构" class="headerlink" title="1. Mask R-CNN 整体结构"></a>1. Mask R-CNN 整体结构</h2><p><img data-src="/images/20_07_09/arch_mask.png"></p><p>从上图可看出 Mask R-CNN 主要由以下4大块构成：</p><ol><li><code>convolutional bakcbone</code>: 用于提取 image feature，生成 feature map，例如 FPN</li><li><code>RPN</code>： 从 feature map 中生成 objects proposals</li><li><code>box head</code>：预测 box 的分类和回归</li><li><code>mask head</code>：预测 pixel mask</li></ol><h2 id="2-Convolutional-Backbone"><a href="#2-Convolutional-Backbone" class="headerlink" title="2. Convolutional Backbone"></a>2. Convolutional Backbone</h2><p>特征提取网络可使用任何能提取 image feature 的网络，本文以 FPN 为例。FPN 全称 Feature Pyramid Network，即特征金字塔网络，其结构如下图所示。</p><p><img data-src="/images/20_07_09/Screenshot_from_2020-07-09_11-57-52.png"></p><p>Detectron2 生成的以 ResNet50 为 backbone 的 FPN 具体参数如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">FPN(</span><br><span class="line">  (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">  (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">  (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">  (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">  (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">  (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">  (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">  (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">  (top_block): LastLevelMaxPool()</span><br><span class="line">  (bottom_up): ResNet50 #   for simplification </span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>带有<code>lateral</code> 关键字的 layer 是上图中的<code>1x1 conv</code>，带有<code>output</code> 是上图中的<code>predict</code>部分，<code>output</code> layer 后接RPN，所以可以看到每个<code>output</code>的输出 channel number 与 RPN 的输入 channel number 相同，此处都为 256。</p><h2 id="3-RPN"><a href="#3-RPN" class="headerlink" title="3. RPN"></a>3. RPN</h2><p>RPN 全称 Region Proposal Network，其作用过程如下图所示。</p><p><img data-src="/images/20_07_09/Screenshot_from_2020-07-09_14-32-36.png"></p><p>RPN的详细结构和配置如下所示。</p><p><img data-src="/images/20_07_09/v2-2b71b622dc1a698b204ed43eca746519_1440w.jpg"></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">RPN(</span><br><span class="line">    (anchor_generator): DefaultAnchorGenerator(</span><br><span class="line">      (cell_anchors): BufferList()</span><br><span class="line">    )</span><br><span class="line">    (rpn_head): StandardRPNHead(</span><br><span class="line">      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">    )</span><br><span class="line">  )</span><br></pre></td></tr></table></figure><p>从<code>detectron2</code>中<a href="https://github.com/facebookresearch/detectron2/blob/af0462a8632e4391c70ce45eb06080cb43e29206/detectron2/modeling/proposal_generator/rpn.py#L92" target="_blank" rel="noopener"><code>StandardRPNHead</code>类的定义</a> 可看出具体参数的数字对应的意义：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 3x3 conv for the hidden representation</span></span><br><span class="line"> self.conv = nn.Conv2d(in_channels, in_channels, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line"> <span class="comment"># 1x1 conv for predicting objectness logits</span></span><br><span class="line"> self.objectness_logits = nn.Conv2d(in_channels, num_anchors, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line"> <span class="comment"># 1x1 conv for predicting box2box transform deltas</span></span><br><span class="line"> self.anchor_deltas = nn.Conv2d(in_channels, num_anchors * box_dim, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><ul><li><code>(objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))</code>： 3 代表anchor数目</li><li><code>(anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))</code>： 12 = 3*4，3代表anchor数目，4代表box的 x, y, w, h 相对于 anchor 的偏移量</li></ul><h2 id="4-Box-Branch"><a href="#4-Box-Branch" class="headerlink" title="4. Box Branch"></a>4. Box Branch</h2><p>Box branch 为对 box 的分类和回归任务，其具体配置如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">(box_pooler): ROIPooler(</span><br><span class="line">      (level_poolers): ModuleList(</span><br><span class="line">        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)</span><br><span class="line">        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)</span><br><span class="line">        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)</span><br><span class="line">        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (box_head): FastRCNNConvFCHead(</span><br><span class="line">      (fc1): Linear(in_features=12544, out_features=1024, bias=True)</span><br><span class="line">      (fc2): Linear(in_features=1024, out_features=1024, bias=True)</span><br><span class="line">    )</span><br><span class="line">    (box_predictor): FastRCNNOutputLayers(</span><br><span class="line">      (cls_score): Linear(in_features=1024, out_features=81, bias=True)</span><br><span class="line">      (bbox_pred): Linear(in_features=1024, out_features=320, bias=True)</span><br><span class="line">    )</span><br></pre></td></tr></table></figure><h3 id="4-1-ROIAlign"><a href="#4-1-ROIAlign" class="headerlink" title="4.1 ROIAlign"></a>4.1 ROIAlign</h3><p>Faster R-CNN 采用的是 ROI Pooling 从 feature map 中提取特征，但是由于 ROI Pooling 本身存在精度问题，导致特征没法对齐，所以 Mask R-CNN 中 RPN 之后的取特征操作采用了 ROI Align。关于 ROI Pooling 和 ROI Align 的具体区别请细读Mask R-CNN原文。</p><ul><li><code>output_size</code>： 输出的feature map 的尺寸</li><li><code>spatial_scale</code> ： 代表相对与输入原图的比例（0.25 -&gt; 1/4, 0.125 -&gt; 1/8, 0.0625 -&gt; 1/16, 0.03125 -&gt; 1/32）</li></ul><h3 id="4-2-Box-Head"><a href="#4-2-Box-Head" class="headerlink" title="4.2 Box Head"></a>4.2 Box Head</h3><ul><li><code>(fc1): Linear(in_features=12544, out_features=1024, bias=True)</code>： 12544 = 256*7*7，256代表 ROIAlign 之后的 feature map 通道数目，7*7 为 ROIAlign 的输出feature map 尺寸经过 <code>reshape</code> 操作成一维的大小。</li><li><code>(fc2): Linear(in_features=1024, out_features=1024, bias=True)</code>： 略</li></ul><h3 id="4-3-Box-Predictor"><a href="#4-3-Box-Predictor" class="headerlink" title="4.3 Box Predictor"></a>4.3 Box Predictor</h3><ul><li><code>(cls_score): Linear(in_features=1024, out_features=81, bias=True)</code>： 81代表80类前景目标 + 1背景</li><li><code>(bbox_pred): Linear(in_features=1024, out_features=320, bias=True)</code>： 320 = 80*4，80代表类别,4代表回归量</li></ul><h2 id="5-Mask-Predictor"><a href="#5-Mask-Predictor" class="headerlink" title="5. Mask Predictor"></a>5. Mask Predictor</h2><p>Faster R-CNN 模型结构到上一节 box branch 就结束了，也就是没有本节的 mask branch。mask branch 与 box branch 最大的区别在于 box branch 包含很多额外的layer，如<code>reshape</code>，<code>NMS</code>等。Mask branch 可看做在有限区域（ROIAlign）内的语义分割任务。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">(mask_pooler): ROIPooler(</span><br><span class="line">      (level_poolers): ModuleList(</span><br><span class="line">        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)</span><br><span class="line">        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)</span><br><span class="line">        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)</span><br><span class="line">        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (mask_head): MaskRCNNConvUpsampleHead(</span><br><span class="line">      (mask_fcn1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">      (mask_fcn2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">      (mask_fcn3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">      (mask_fcn4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span><br><span class="line">      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))</span><br><span class="line">      (predictor): Conv2d(256, 80, kernel_size=(1, 1), stride=(1, 1))</span><br><span class="line">    )</span><br></pre></td></tr></table></figure><p>ROIAlign 同 Box Predictor 内的 ROIAlign。</p><p>Detectron2 中的 mask head 使用的是<code>MaskRCNNConvUpsampleHead</code>，可以看到在<code>predictor</code>之前加了一层<code>ConvTranspose2d</code>，增大了输入最后 predictor 的 feature map， 使得局部定位更加精确 。</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Mask R-CNN 模型包含的4块结构拆分，对于每一块的分别进行分析之后，可增加对网路的认识。结合Detectron2的条理清晰的代码，能对每一层有更加清晰的认识。提取feature的网络可替换成任意其他网络，RPN 作用是利用 feature map 结合 anchor 找到最佳的 proposal，剩下的 box branch 和 mask branch 可根据任务进行调整，如增加网路的深度等。</p><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ol><li><a href="https://arxiv.org/abs/1703.06870" target="_blank" rel="noopener">Mask R-CNN</a></li><li><a href="https://github.com/facebookresearch/detectron2/tree/master" target="_blank" rel="noopener">Detectron2</a></li><li><a href="https://arxiv.org/abs/1612.03144" target="_blank" rel="noopener">Feature Pyramid Networks for Object Detection</a></li><li><a href="https://arxiv.org/abs/1506.01497" target="_blank" rel="noopener">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</a></li><li><a href="https://zhuanlan.zhihu.com/p/84565843" target="_blank" rel="noopener">对anchor机制的探索</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Mask R-CNN 作为经典的two-stage目标检测模型被广泛使用，相关的讲解论文已经有很多，本文将着重在结合实际代码模型结构以及 layer 方面的讲解，这样会对模型有更加清晰的认识。本文以 &lt;a href=&quot;https://github.com/facebookresearch/detectron2/blob/master/GETTING_STARTED.md#training--evaluation-in-command-line&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Detectron2 mask_rcnn_R_50_FPN_1x.yaml&lt;/a&gt; 默认 config 生成的 Mask R-CNN 进行讲解。
    
    </summary>
    
    
    
      <category term="DL" scheme="http://zengzeyu.com/tags/DL/"/>
    
      <category term="CV" scheme="http://zengzeyu.com/tags/CV/"/>
    
      <category term="Network" scheme="http://zengzeyu.com/tags/Network/"/>
    
  </entry>
  
  <entry>
    <title>浅谈PointNet和PointNet++网络及其应用</title>
    <link href="http://zengzeyu.com/2019/07/14/PointNet%20and%20PointNet++/"/>
    <id>http://zengzeyu.com/2019/07/14/PointNet%20and%20PointNet++/</id>
    <published>2019-07-14T11:12:00.000Z</published>
    <updated>2020-07-15T04:35:16.198Z</updated>
    
    <content type="html"><![CDATA[<p>PointNet 和 PointNet++ 网络由 Charles R. Qi 于2017年提出，分别发表于CVPR 2017和NIPS 2017会议上。PointNet++ 是 PoinNet 的升级版，也更复杂，两篇论文的现在都可检索，且分别作为一个章节被写入了 Qi 的博士论文中，如果想更加具体地理解各个细节，可从博士论文下手（参考文献会附上资源）。本文预先假设读者已经阅读过这两篇论文，并且对点云特性有一定了解，所以细节方面不过多赘述，更多地是探讨思路。<a id="more"></a></p><h2 id="1-网络结构"><a href="#1-网络结构" class="headerlink" title="1. 网络结构"></a>1. 网络结构</h2><h3 id="1-1-PointNet"><a href="#1-1-PointNet" class="headerlink" title="1.1 PointNet"></a>1.1 PointNet</h3><p><img data-src="/images/19_7_14/PointNet/20190702225201.png"></p><p>PointNet 按照任务划分 classification 和segmentation两种，如上图所示，途中上半部分为classification network（下文以C网络代替），下半部分为 segmentation network（下文以S网络代替）。两个任务的网络输入都是 $ n * 3 $ ，代表 $n$ 个三维坐标点，C网络输出为经过 softmax 之后的 $k$ 个值，而S网络为 $n$ 个点的逐点的score。</p><p>其中一个值得注意的区别是，两个网络中对max pooling层出来之后<em>global feature</em>不同。我的理解为是：C网络由于是简单的输入点云的分类任务，所以只需要得到一个最明显的feature来代表这个点云就可胜任分类任务；而S网络由于是逐点的语义分割，所以将<em>global feature</em> concatenate到每一点后面，作用是使每一个点都同时具有自身点的feature和global feature，更有利于进行逐点的分类。</p><h3 id="1-2-PointNet"><a href="#1-2-PointNet" class="headerlink" title="1.2 PointNet++"></a>1.2 PointNet++</h3><p><img data-src="/images/19_7_14/PointNet/20190702230651.png"></p><p>PointNet++按照任务也分为 classification （C网络）和 segmentation （S网络）两种，输入和输出分别与PointNet中的两个网络一致。首先，比较PointNet++两个任务网络的区别：在得到最高层的 feature 之后，C网络使用了一个小型的 PointNet + FCN 网络提取得到最后的分类 score；S网络通过 skip link connection 操作不断与底层 low-level 信息融合，最终得到逐点分分类语义分割结果。</p><p>PointNet++ 的思路与U-Net很相似：利用encoder-decoder结构，逐层提高feature level，在到达最高层之后通过 skip link connection 操作恢复局部信息，从而达到既能获取 high-level context feature 也能获取 low-level context feature。</p><h2 id="2-应用场景"><a href="#2-应用场景" class="headerlink" title="2. 应用场景"></a>2. 应用场景</h2><p>从PointNet的结构可以看出，由于没有local context信息，因此对复杂的点云没办法处理，也是PointNet结尾总结的future work，即在PointNet++中进行了升级。由于使用了max-pooling这个具有强对偶性和特征筛选的操作，使得PointNet只能处理较为简单的点云，这些点云都有一个特征：<strong>属于同一物体或者处于某个局部空间内</strong>。在点云中属于同一个物体和处于某一局部空间内可看作等价。因此，从PointNet的应用目的角度，使用PointNet的点云分割和检测网络主要有以下两种：</p><ul><li>放在网络前部，作为局部点云特征提取器（feature extractor）</li><li>放在网络后部，作为具体任务的refine或语义分割，如局部点云语义分割或3D BBox等目标的回归</li></ul><p>至于为什么使用PointNet而不是使用PointNet++，从结构上可以看出PointNet++会有一个邻域中心和邻域点云索引的过程，会使速度下降严重，另外从本节第一段话也得知，对于局部点云这些简单的数据组成，使用PointNet也足够胜任。下面将就PointNet和PointNet++作为不同任务，简单罗列下提到的Paper中的使用。</p><h3 id="2-1-特征提取器"><a href="#2-1-特征提取器" class="headerlink" title="2.1 特征提取器"></a>2.1 特征提取器</h3><p>Motivation：特征提取器（feature extractor）有别于人工特征设计，特征提取器的目的是在特征构建层面，使用网络让其自行学习一些人工无法构建的特征，从而提升特征的表达，最终达到提升网络性能的目的。说句题外话，人工构建特征都是基于人的先验知识来构建的特征，比如说点的数目，最大高度等等，这些具有强人类先验知识特征构建便于人类理解，但是对于数据表达来说，在这个过程丢失了很多信息。因此，我们希望送进网络的是raw data，网络能自行的学习特征。</p><p>值得注意的是，特征提取器网络一般采用PointNet而不是PointNet++，除了如上文所说速度考虑以外，作为特征提取器一般会在对整张点云进行栅格化（grid）或体素化（voxel）后的单元内进行，这样就符合了上文另一个条件——<strong>输入点云都处于局部空间内</strong>。相关的论文有如下：VoxelNet，SECOND，PointPillars, PointFusion。</p><p>也有例外，如IPOD，使用PointNet++对整张点云提取特征。</p><p><strong>VoxelNet</strong></p><p>使用PointNet对体素化后的单元提取特征。</p><p><img data-src="/images/19_7_14/PointNet//QQ%E6%88%AA%E5%9B%BE20190714173113.png"></p><p><strong>SECOND</strong></p><p>使用PointNet对体素化后的单元提取特征，同VoxelNet。</p><p><img data-src="/images/19_7_14/PointNet//QQ%E6%88%AA%E5%9B%BE20190714173302.png"></p><p><strong>PointPillars</strong></p><p>使用PointNet对栅格化后的单元提取特征。</p><p><img data-src="/images/19_7_14/PointNet//QQ%E6%88%AA%E5%9B%BE20190714173415.png"></p><p><strong>PointFusion</strong></p><p><img data-src="/images/19_7_14/PointNet//QQ%E6%88%AA%E5%9B%BE20190714173614.png"></p><p><strong>IPOD</strong>（PointNet++）</p><p>使用PointNet++对整帧点云提取特征。</p><p><img data-src="/images/19_7_14/PointNet//QQ%E6%88%AA%E5%9B%BE20190714175459.png"></p><h3 id="2-2-逐点语义分割-3D-BBox-回归"><a href="#2-2-逐点语义分割-3D-BBox-回归" class="headerlink" title="2.2 逐点语义分割 / 3D BBox 回归"></a>2.2 逐点语义分割 / 3D BBox 回归</h3><p>PointNet或PointNet++作为具体任务网络与论文挂钩，但主要分为逐点语义分割和3D BBox回归两种。当然还有其它任务，后面也会罗列。</p><p><strong>PointRCNN</strong>(PointNet++)</p><p>PointNet++输入整帧点云进行语义分割和3D BBox proposal提取。</p><p><img data-src="/images/19_7_14/PointNet//QQ%E6%88%AA%E5%9B%BE20190714175824.png"></p><p><strong>Frutum-PointNet</strong></p><p>第1个PointNet：对得到的锥形3D proposal进行前景点语义分割</p><p>第2个PointNet：对前景点回归出3D Bbox</p><p><img data-src="/images/19_7_14/PointNet//QQ%E6%88%AA%E5%9B%BE20190714180316.png"></p><p><strong>RoarNet</strong></p><p>第1个PointNet：对同一个目标筛选出置信度最高的proposal和大致回归出目标3D Bbox的位置</p><p>第2个PointNet：回归出3D Bbox</p><p><img data-src="/images/19_7_14/PointNet//QQ%E6%88%AA%E5%9B%BE20190714180556.png"></p><p><strong>FVNet</strong></p><p>PointNet 对 proposal 局部点云回归 3D Bbox。</p><p><img data-src="/images/19_7_14/PointNet//QQ%E6%88%AA%E5%9B%BE20190714183205.png"></p><h2 id="3-结论"><a href="#3-结论" class="headerlink" title="3. 结论"></a>3. 结论</h2><ol><li><p>从前文分析可得出，不管点云网络的任务是什么，对于复杂场景点云一般采用PointNet++进行处理，而简单场景点云则采用PointNet。</p></li><li><p>如果只从点云分类和分割两个任务角度分析，分类任务只需要max pooling操作之后的特征信息就可完成，而分割任务则需要更加详细的local context信息。</p></li></ol><p>这篇文章主要从PointNet角度去分析基于点云的目标检测任务，从提取 3D proposal 形式的角度在将来会以另一篇文章继续进行讨论。</p><p>以上。</p><p><strong>参考文献：</strong></p><ol><li><a href="https://searchworks.stanford.edu/view/12741586" target="_blank" rel="noopener">DEEP LEARNING ON POINT CLOUDS FOR 3D SCENE UNDERSTANDING(Ph.D Thesis)</a></li><li><a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Qi_PointNet_Deep_Learning_CVPR_2017_paper.pdf" target="_blank" rel="noopener">PointNet : Deep Learning on Point Sets for 3D Classification and Segmentation</a></li><li><a href="https://arxiv.org/pdf/1706.02413.pdf" target="_blank" rel="noopener">PointNet ++ : Deep Hierarchical Feature Learning on Point Sets in a Metric Space</a></li><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/3333.pdf" target="_blank" rel="noopener">VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection</a></li><li><a href="https://www.mdpi.com/1424-8220/18/10/3337/pdf" target="_blank" rel="noopener">SECOND: Sparsely Embedded Convolutional Detection</a></li><li><a href="https://arxiv.org/pdf/1812.05784" target="_blank" rel="noopener">PointPillars: Fast Encoders for Object Detection from Point Clouds</a></li><li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Xu_PointFusion_Deep_Sensor_CVPR_2018_paper.pdf" target="_blank" rel="noopener">PointFusion : Deep Sensor Fusion for 3D Bounding Box Estimation</a></li><li><a href="https://arxiv.org/pdf/1812.05276" target="_blank" rel="noopener">IPOD: Intensive Point-based Object Detector for Point Cloud</a></li><li><a href="https://arxiv.org/pdf/1812.04244" target="_blank" rel="noopener">PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud</a></li><li><a href="https://arxiv.org/pdf/1711.08488" target="_blank" rel="noopener">Frustum PointNets for 3D Object Detection from RGB-D Data</a></li><li><a href="http://zengzeyu.com/2019/03/24/19_3_24/My-Paper-Lib-2018/">RoarNet: A Robust 3D Object Detection based on RegiOn Approximation Refinement</a></li><li><a href="https://arxiv.org/pdf/1903.10750" target="_blank" rel="noopener">FVNet: 3D Front-View Proposal Generation for Real-Time Object Detection from Point Clouds</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PointNet 和 PointNet++ 网络由 Charles R. Qi 于2017年提出，分别发表于CVPR 2017和NIPS 2017会议上。PointNet++ 是 PoinNet 的升级版，也更复杂，两篇论文的现在都可检索，且分别作为一个章节被写入了 Qi 的博士论文中，如果想更加具体地理解各个细节，可从博士论文下手（参考文献会附上资源）。本文预先假设读者已经阅读过这两篇论文，并且对点云特性有一定了解，所以细节方面不过多赘述，更多地是探讨思路。
    
    </summary>
    
    
    
      <category term="DL" scheme="http://zengzeyu.com/tags/DL/"/>
    
      <category term="CV" scheme="http://zengzeyu.com/tags/CV/"/>
    
      <category term="Point Cloud" scheme="http://zengzeyu.com/tags/Point-Cloud/"/>
    
      <category term="PointNet" scheme="http://zengzeyu.com/tags/PointNet/"/>
    
      <category term="3D Perception" scheme="http://zengzeyu.com/tags/3D-Perception/"/>
    
  </entry>
  
  <entry>
    <title>My Paper Lib 2019</title>
    <link href="http://zengzeyu.com/2019/06/29/My-Paper-Lib-2019/"/>
    <id>http://zengzeyu.com/2019/06/29/My-Paper-Lib-2019/</id>
    <published>2019-06-29T14:23:00.000Z</published>
    <updated>2020-07-15T04:38:17.566Z</updated>
    
    <content type="html"><![CDATA[<p>This is my 2019 paper lib. <a id="more"></a></p><table><thead><tr><th>No.</th><th>PAPER</th><th>SOURCE</th></tr></thead><tbody><tr><td>1</td><td>Visualizing the Loss Landscape of Neural Nets</td><td><a href="https://arxiv.org/pdf/1712.09913" target="_blank" rel="noopener">PDF</a>/video/<a href="https://github.com/tomgoldstein/loss-landscape" target="_blank" rel="noopener">code</a></td></tr><tr><td>2</td><td>3D Backbone Network for 3D Object Detection</td><td><a href="https://arxiv.org/pdf/1901.08373" target="_blank" rel="noopener">PDF</a>/video/code</td></tr><tr><td>3</td><td>PersonLab : Person Pose Estimation and Instance Segmentation with a Bottom-Up , Part-Based , Geometric Embedding Model</td><td><a href="https://eccv2018.org/openaccess/content_ECCV_2018/papers/George_Papandreou_PersonLab_Person_Pose_ECCV_2018_paper.pdf" target="_blank" rel="noopener">PDF</a>/video/<a href="https://github.com/octiapp/KerasPersonLab" target="_blank" rel="noopener">code</a></td></tr><tr><td>4</td><td>DeeperLab : Single-Shot Image Parser</td><td><a href="https://arxiv.org/pdf/1902.05093" target="_blank" rel="noopener">PDF</a>/video/code</td></tr><tr><td>5</td><td>Multi-Task Learning as Multi-Objective Optimization</td><td><a href="https://arxiv.org/pdf/1810.04650" target="_blank" rel="noopener">PDF</a>/video/<a href="https://github.com/IntelVCL/MultiObjectiveOptimization" target="_blank" rel="noopener">code</a></td></tr><tr><td>6</td><td>Rethinking on Multi-Stage Networks for Human Pose Estimation</td><td><a href="https://arxiv.org/pdf/1901.00148" target="_blank" rel="noopener">PDF</a>/<a href="">video</a>/<a href="">code</a></td></tr><tr><td>7</td><td>RePr: Improved Training of Convolutional Filters</td><td><a href="https://arxiv.org/pdf/1811.07275" target="_blank" rel="noopener">PDF</a>/video/code</td></tr><tr><td></td><td>——— Date: 03-24 ———-</td><td></td></tr><tr><td>8</td><td>Group Normalization</td><td><a href="https://arxiv.org/pdf/1803.08494" target="_blank" rel="noopener">PDF</a>/video/<a href="https://pytorch.org/docs/stable/nn.html#groupnorm" target="_blank" rel="noopener">code</a></td></tr><tr><td>9</td><td>Weight Standardization</td><td><a href="https://arxiv.org/pdf/1903.10520" target="_blank" rel="noopener">PDF</a>/video/<a href="https://github.com/joe-siyuan-qiao/WeightStandardization" target="_blank" rel="noopener">code</a></td></tr><tr><td>10</td><td>Pruning Filters for Efficient ConvNets</td><td><a href="https://arxiv.org/pdf/1608.08710" target="_blank" rel="noopener">PDF</a>/video/code</td></tr><tr><td>11</td><td>High Performance Convolutional Neural Networks for Document Processing</td><td><a href="https://hal.inria.fr/inria-00112631/document" target="_blank" rel="noopener">PDF</a>/video/code</td></tr><tr><td>12</td><td>Group normalization</td><td><a href="https://arxiv.org/pdf/1803.08494" target="_blank" rel="noopener">PDF</a>/video/code</td></tr><tr><td>13</td><td>Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks(DCGAN)</td><td><a href="https://arxiv.org/pdf/1511.06434" target="_blank" rel="noopener">PDF</a>/video/<a href="https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html" target="_blank" rel="noopener">code</a></td></tr><tr><td>14</td><td>Generative Adversarial Networks(GAN)</td><td><a href="https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf" target="_blank" rel="noopener">PDF</a>/video/code</td></tr><tr><td>15</td><td>Adversarial Learning for Semi-Supervised Semantic Segmentation</td><td><a href="https://arxiv.org/pdf/1802.07934" target="_blank" rel="noopener">PDF</a>/video/<a href="https://github.com/hfslyc/AdvSemiSeg" target="_blank" rel="noopener">code</a></td></tr><tr><td>16</td><td>Wasserstein GAN</td><td><a href="https://arxiv.org/pdf/1701.07875" target="_blank" rel="noopener">PDF</a>/video/code</td></tr><tr><td>17</td><td>FVNet: 3D Front-View Proposal Generation for Real-Time Object Detection from Point Clouds</td><td><a href="https://arxiv.org/pdf/1903.10750" target="_blank" rel="noopener">PDF</a>/video/code</td></tr><tr><td>18</td><td>YOLOv3: An Incremental Improvement</td><td><a href="https://pjreddie.com/media/files/papers/YOLOv3.pdf" target="_blank" rel="noopener">PDF</a>/video/<a href="https://github.com/eriklindernoren/PyTorch-YOLOv3" target="_blank" rel="noopener">code</a></td></tr><tr><td></td><td>———- Date: 05-03 ———-</td><td></td></tr><tr><td>19</td><td>TextField: Learning A Deep Direction Field for Irregular Scene Text Detection</td><td><a href="https://arxiv.org/pdf/1812.01393v1" target="_blank" rel="noopener">PDF</a>/video/code</td></tr><tr><td>20</td><td>PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud</td><td><a href="https://arxiv.org/pdf/1812.04244" target="_blank" rel="noopener">PDF</a>/video/<a href="https://github.com/sshaoshuai/PointRCNN" target="_blank" rel="noopener">code</a></td></tr><tr><td>21</td><td>Deep learning on point clouds for 3D scene understanding</td><td><a href="https://searchworks.stanford.edu/view/12741586" target="_blank" rel="noopener">PDF</a>/video/code</td></tr><tr><td>22</td><td>YOLO9000: Better, faster, stronger</td><td><a href="https://arxiv.org/pdf/1612.08242v1" target="_blank" rel="noopener">PDF</a>/video/<a href="http://pjreddie.com/darknet/yolo/" target="_blank" rel="noopener">code</a></td></tr><tr><td>23</td><td>LaserNet: An Efficient Probabilistic 3D Object Detector for Autonomous Driving</td><td><a href="https://arxiv.org/pdf/1903.08701v1" target="_blank" rel="noopener">PDF</a>/video/code</td></tr><tr><td>24</td><td>DEEP LEARNING ON POINT CLOUDS FOR 3D SCENE UNDERSTANDING(Ph.D Thesis)</td><td><a href="https://searchworks.stanford.edu/view/12741586" target="_blank" rel="noopener">PDF</a>/video/code</td></tr><tr><td>25</td><td>Multi-View 3D Object Detection Network for Autonomous Driving</td><td><a href="https://arxiv.org/pdf/1611.07759.pdf" target="_blank" rel="noopener">PDF</a>/video/code</td></tr><tr><td>26</td><td>Deep 3d Representation Learning(Ph.D Thesis)</td><td><a href="http://cseweb.ucsd.edu/~haosu/papers/thesis_finalversion.pdf" target="_blank" rel="noopener">PDF</a>/video/code</td></tr><tr><td></td><td>———- Date: 06-29 ———-</td><td>-</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This is my 2019 paper lib.
    
    </summary>
    
    
    
      <category term="DL" scheme="http://zengzeyu.com/tags/DL/"/>
    
      <category term="Paper" scheme="http://zengzeyu.com/tags/Paper/"/>
    
  </entry>
  
  <entry>
    <title>boost serialize(序列化)和boost deserialize(反序列化)类对象型数据并通过socket网络传输</title>
    <link href="http://zengzeyu.com/2019/06/29/boostserialize(%E5%BA%8F%E5%88%97%E5%8C%96)%E5%92%8Cboostdeserialize(%E5%8F%8D%E5%BA%8F%E5%88%97%E5%8C%96)%E7%B1%BB%E5%AF%B9%E8%B1%A1%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%B9%B6%E9%80%9A%E8%BF%87socket%E7%BD%91%E7%BB%9C%E4%BC%A0%E8%BE%93/"/>
    <id>http://zengzeyu.com/2019/06/29/boostserialize(%E5%BA%8F%E5%88%97%E5%8C%96)%E5%92%8Cboostdeserialize(%E5%8F%8D%E5%BA%8F%E5%88%97%E5%8C%96)%E7%B1%BB%E5%AF%B9%E8%B1%A1%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%B9%B6%E9%80%9A%E8%BF%87socket%E7%BD%91%E7%BB%9C%E4%BC%A0%E8%BE%93/</id>
    <published>2019-06-29T11:12:00.000Z</published>
    <updated>2020-07-15T04:38:56.171Z</updated>
    
    <content type="html"><![CDATA[<p>在自动驾驶领域，传感器与主机通过网线连接，实现二者的实时通信。同时，在接收到雷达数据之后，后台处理系统到前端用户显示界面，也需要通过上述方法进行通信，因为后台处理系统一般都不自带显示器，例如PX-2，TX2等平台就提供网线接口。<br>本文以<a href="http://robosense.cn/rslidar/RS-LiDAR-16" target="_blank" rel="noopener">16线激光雷达</a>生成数据，处理后台系统处理后点云为例，进行分割分类数据传输到前端用户界面进行显示。<a id="more"></a><br>主要用到了以下两个方面：</p><ul><li>boost::serialize 和 boost::deserialize 函数</li><li>socket sendto() 和 recvfrom() 函数</li></ul><p>本文首先介绍<strong>不使用boost序列化</strong>方法进行传输方法，之后再介绍<strong>boost序列化</strong>方法进行数据传输的方法。主要梳理boost::serialize(序列化)和boost::deserialize(反序列化)相关内容，socket部分稍作介绍。关于二者的相关知识链接，请参阅底部参考文献。</p><h2 id="1-非序列化数据传输方法"><a href="#1-非序列化数据传输方法" class="headerlink" title="1. 非序列化数据传输方法"></a>1. 非序列化数据传输方法</h2><h3 id="1-1-数据类格式"><a href="#1-1-数据类格式" class="headerlink" title="1.1 数据类格式"></a>1.1 数据类格式</h3><p>非序列化数据传输方法中数据类表示如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">CommunicationMsg</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">    Header header;</span><br><span class="line">    PoseMsg pose;</span><br><span class="line">    <span class="keyword">char</span> perceptions[<span class="number">20000</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>header</code> 和 <code>pose</code>为另外的数据类，<code>perceptions</code>为<code>char</code>型数据容器。在非序列化方法下，将<code>CommunicationMsg</code>通过<code>socket</code>进行传输时，必须保证接收端的<code>CommunicationMsg</code>也必须为同样的数据顺序格式，即<code>header</code>、<code>pose</code>和<code>perceptions</code>三者顺序以及结构必须保持一致。</p><h3 id="1-2-发送端代码"><a href="#1-2-发送端代码" class="headerlink" title="1.2 发送端代码"></a>1.2 发送端代码</h3><p>非序列化发送端代码如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">sendMsg</span><span class="params">(<span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;PerceptOutput&gt; &amp;datas, <span class="keyword">const</span> Header &amp;header, <span class="keyword">const</span> PoseMsg &amp;pose)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  CommunicationMsg send_msg;</span><br><span class="line">  send_msg.header = header;</span><br><span class="line">  send_msg.pose = pose;</span><br><span class="line">  <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;PerceptResultMsg&gt; msgs = convertToMsg(datas);</span><br><span class="line"></span><br><span class="line">    <span class="comment">/// origin send</span></span><br><span class="line">  <span class="built_in">std</span>::<span class="built_in">string</span> tmp_string = toString(msgs);</span><br><span class="line">  send_msg.header.length = <span class="keyword">static_cast</span>&lt;<span class="keyword">int</span>&gt;(tmp_string.<span class="built_in">size</span>());</span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="keyword">static_cast</span>&lt;<span class="keyword">int</span>&gt;(tmp_string.<span class="built_in">size</span>()); ++i)</span><br><span class="line">  &#123;</span><br><span class="line">      send_msg.perceptions[i] = tmp_string[i];</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">char</span> pot[<span class="number">20480</span>];</span><br><span class="line">  <span class="built_in">memset</span>(pot, <span class="number">0</span>, <span class="keyword">sizeof</span>(pot));</span><br><span class="line">  <span class="built_in">memcpy</span>(pot, &amp;send_msg, <span class="keyword">sizeof</span>(send_msg));</span><br><span class="line">  sendto(sender_sockfd_, pot, <span class="keyword">sizeof</span>(pot), <span class="number">0</span>, (struct sockaddr *) &amp;sender_dest_addr_, <span class="keyword">sizeof</span>(sender_dest_addr_));</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看到上述代码中的<code>pot</code>变量为固定长度<code>20480</code>，<code>20480</code>这个数字是根据自己的数据长度来定，唯一的原则就是要保证所要发送的数据长度必须小于该数大小。这里就体现出非序列化方法中固定数组长度<strong>弊端</strong>：</p><ol><li><strong>数据小于该数组长度时，造成网络带宽的浪费；</strong></li><li><strong>数据大于该数组长度时，造成数据溢出。</strong></li></ol><h3 id="1-3-接收端代码"><a href="#1-3-接收端代码" class="headerlink" title="1.3 接收端代码"></a>1.3 接收端代码</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">receMsg</span><span class="params">(<span class="built_in">std</span>::<span class="built_in">vector</span>&lt;PerceptResultMsg&gt; &amp;msgs, Header &amp;header, PoseMsg &amp;pose)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="keyword">char</span> cli_ip[INET_ADDRSTRLEN] = <span class="string">""</span>;<span class="comment">//INET_ADDRSTRLEN=16</span></span><br><span class="line">  <span class="keyword">socklen_t</span> cliaddr_len = <span class="keyword">sizeof</span>(receiver_dest_addr_);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">char</span> recv_buf[<span class="number">50000</span>] = &#123;<span class="number">0</span>&#125;;</span><br><span class="line">  <span class="keyword">int</span> recv_len = recvfrom(receiver_sockfd_, recv_buf, <span class="keyword">sizeof</span>(recv_buf), <span class="number">0</span>, (struct sockaddr *) &amp;receiver_dest_addr_, &amp;cliaddr_len);</span><br><span class="line">  inet_ntop(AF_INET, &amp;receiver_dest_addr_.sin_addr, cli_ip, INET_ADDRSTRLEN);</span><br><span class="line"></span><br><span class="line">  <span class="comment">/// origin receive </span></span><br><span class="line">  CommunicationMsg *tmp_msg = <span class="keyword">new</span> CommunicationMsg;</span><br><span class="line">  tmp_msg = (CommunicationMsg *) recv_buf;</span><br><span class="line">  <span class="keyword">int</span> str_len = tmp_msg-&gt;header.length;</span><br><span class="line">  <span class="built_in">std</span>::<span class="built_in">string</span> tmp_str;</span><br><span class="line">  tmp_str.resize(str_len);</span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; str_len; ++i)</span><br><span class="line">  &#123;</span><br><span class="line">      tmp_str[i] = tmp_msg-&gt;perceptions[i];</span><br><span class="line">  &#125;</span><br><span class="line">  msgs = toData(tmp_str);</span><br><span class="line">  header = tmp_msg-&gt;header;</span><br><span class="line">  pose = tmp_msg-&gt;pose;</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>接收端代码中<code>recv_buf</code>固定大小为<code>50000</code>，由于数据在完全接收之前不知道数据长度为多少，所以必须使用更大的数组容器来盛放接收数据。此处，能想到的可以讲数据长度变量和数据本身分开成两次先后接收，可以先接收到数据长度，然后使用该长度去接收数据，不过这样使得操作更加麻烦，可能用更加智能的方法，本文未涉及。<br>接收端将接收到的数据通过此行代码<code>  tmp_msg = (CommunicationMsg *) recv_buf;</code>进行强制类型转换，该方法监督粗暴，但后患无穷，具体会导致哪些不容易发现的bug，本文未进行深究，但此方法在使用中也是可行方案之一。这个方法依据数据在数组中的放置方式，完全按照该类结构来储存，所以可以采用此方法。此方法属于C/C++语言的自带特性中的强制类型转换应用。</p><h2 id="2-boost-serialize-（序列化）-和-boost-deserialize-（反序列化）"><a href="#2-boost-serialize-（序列化）-和-boost-deserialize-（反序列化）" class="headerlink" title="2. boost serialize （序列化） 和 boost deserialize （反序列化）"></a>2. boost serialize （序列化） 和 boost deserialize （反序列化）</h2><p>对于发送和接收消息，我觉得可以通过举一个例子来形象的理解：谍战中的密码报文发送与接收。当友军持有相同的密码解读字典时，发送密码一方通过将需要发送的信息，通过查询密码字典翻译为密码，然后通过发报机发送出去，接收一方首先记录下接收到的信息，然后在通过查询密码字典将信息翻译出来，从而得到真实的信息。只要理解了上面的原理，将该模型抽象出来，就会在生活中发现许多相似的场景：理解相同语言人之间的对话，收音机调到某一频率收听节目，与黄金等值的货币之间的兑换等等。这就是 boost 序列化的最形象的理解。</p><h3 id="2-1-类对象数据序列化"><a href="#2-1-类对象数据序列化" class="headerlink" title="2.1 类对象数据序列化"></a>2.1 类对象数据序列化</h3><p>数据格式的序列化可分为：XML格式，文本格式（text）和二进制格式（binary）。二进制格式虽然在传输过程中速度会更快，但是会破坏类对象型数据数据结构，所以不采用此方法。文本格式是最容易理解和输出可是化的格式，所见即所得，所以本文采用此格式进行数据传输。<br>使用文本格式序列化须包含以下头文件：<code>#include &lt;boost/archive/text_iarchive.hpp&gt;</code>和<code>#include &lt;boost/archive/text_oarchive.hpp&gt;</code>。另外，boost自带对STL库的数据支持，例如本文使用到的对<code>std::vector</code>支持，需要包含头文件<code>#include &lt;boost/serialization/vector.hpp&gt;</code>。<br>类对象数据是否侵入式序列化有两种方式：侵入式序列化（intrusive）和非侵入式序列化（non-intrusive）。</p><p>关于二者的介绍，[参考文献1]中进行了详细描述。本文将以第1章中数据为例，使用侵入式序列化进行简单的介绍。</p><p><strong>侵入式序列化</strong></p><p>侵入式序列化操作相关代码：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">std</span>::<span class="built_in">vector</span>&lt;PerceptResultMsg&gt; pcep_msg;</span><br><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> Archive&gt;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">serialize</span><span class="params">(Archive &amp; ar, <span class="keyword">const</span> <span class="keyword">unsigned</span> <span class="keyword">int</span> version)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    ar &amp; a;</span><br><span class="line">    ar &amp; b;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>以本文数据类型为例：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//boost</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;boost/archive/text_iarchive.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;boost/archive/text_oarchive.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;boost/serialization/vector.hpp&gt;</span></span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// header</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Header</span>&#123;</span></span><br><span class="line"><span class="comment">//public:</span></span><br><span class="line"><span class="comment">//    Header() &#123;&#125;</span></span><br><span class="line"><span class="comment">//    Header( unsigned short int&amp; in_head, unsigned short int&amp; in_object_num, unsigned int&amp; in_length,</span></span><br><span class="line"><span class="comment">//             unsigned long int&amp; in_frame_id, unsigned long int&amp; in_timestamp):</span></span><br><span class="line"><span class="comment">//            head(in_head), object_num(in_object_num), length(in_length), frame_id(in_frame_id), timestamp(in_timestamp)&#123;&#125;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">friend</span> <span class="class"><span class="keyword">class</span> <span class="title">boost</span>:</span>:serialization::access;</span><br><span class="line">    <span class="keyword">template</span>&lt;<span class="class"><span class="keyword">class</span> <span class="title">Archive</span>&gt;</span></span><br><span class="line"><span class="class">    <span class="title">void</span> <span class="title">serialize</span>(<span class="title">Archive</span>&amp; <span class="title">ar</span>, <span class="title">const</span> <span class="title">unsigned</span> <span class="title">int</span> <span class="title">version</span>)</span></span><br><span class="line"><span class="class">    &#123;</span></span><br><span class="line">        ar &amp; head;</span><br><span class="line">        ar &amp; object_num;</span><br><span class="line">        ar &amp; length;</span><br><span class="line">        ar &amp; frame_id;</span><br><span class="line">        ar &amp; timestamp;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">unsigned</span> short <span class="keyword">int</span> head;</span><br><span class="line">  <span class="keyword">unsigned</span> short <span class="keyword">int</span> object_num;</span><br><span class="line">  <span class="keyword">unsigned</span> <span class="keyword">int</span> length;</span><br><span class="line">  <span class="keyword">unsigned</span> <span class="keyword">long</span> <span class="keyword">int</span> frame_id;</span><br><span class="line">  <span class="keyword">unsigned</span> <span class="keyword">long</span> <span class="keyword">int</span> timestamp;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">CommunicationMsg</span>&#123;</span></span><br><span class="line">    <span class="keyword">friend</span> <span class="class"><span class="keyword">class</span> <span class="title">boost</span>:</span>:serialization::access;</span><br><span class="line">    Header header;</span><br><span class="line">    PoseMsg pose;</span><br><span class="line"><span class="comment">//    char perceptions[20000];</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// intrusive</span></span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;PerceptResultMsg&gt; pcep_msg;</span><br><span class="line">    <span class="keyword">template</span>&lt;<span class="keyword">typename</span> Archive&gt;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">serialize</span><span class="params">(Archive &amp; ar, <span class="keyword">const</span> <span class="keyword">unsigned</span> <span class="keyword">int</span> version)</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        ar &amp; header;</span><br><span class="line">        ar &amp; pose;</span><br><span class="line">        ar &amp; pcep_msg;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>既然是“侵入式”，那么就要对原有数据类进行更改，上述代码中的更改将<code>std::vector&lt;PerceptResultMsg&gt; pcep_msg</code>替代原有<code>char perceptions[20000]</code>为容器。对于侵入式序列化的类数据每一个数据成员，同样要进行侵入式序列化处理，因为boost serialize只对其支持的数据结构（基本数据类型和STL库里的数据类型）进行序列化操作，对于用户自定义的类型数据，无法自动进行序列化处理。代码中以<code>struct header</code>子数据成员为例进行了侵入式序列化操作。</p><h3 id="2-2-序列化操作代码"><a href="#2-2-序列化操作代码" class="headerlink" title="2.2 序列化操作代码"></a>2.2 序列化操作代码</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">std::string Serialize(const CommunicationMsg &amp;msg)</span><br><span class="line">&#123;</span><br><span class="line">  std::ostringstream archiveStream;</span><br><span class="line">  boost::archive::text_oarchive archive(archiveStream);</span><br><span class="line">  archive &lt;&lt; msg;</span><br><span class="line">  return archiveStream.str();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>将<code>CommunicationMsg</code>类型对象传入该函数可得到<code>string</code>类型数据传输流，用以 socket 通信。</p><h3 id="2-3-接收端反序列化"><a href="#2-3-接收端反序列化" class="headerlink" title="2.3 接收端反序列化"></a>2.3 接收端反序列化</h3><p>为了反序列化出原有<code>CommunicationMsg</code>类型数据对象，那么定义声明<code>CommunicationMsg</code>类型文件必须在接收端同样有一份，这样反序列化操作才能根据该文件得出经 socket 传输过来的数据属于哪个变量。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">CommunicationMsg <span class="title">DeSerialize</span><span class="params">(<span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">string</span> &amp;message)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    CommunicationMsg msg;</span><br><span class="line">    <span class="function"><span class="built_in">std</span>::<span class="built_in">istringstream</span> <span class="title">archiveStream</span><span class="params">(message)</span></span>;</span><br><span class="line">    boost::<span class="function">archive::text_iarchive <span class="title">archive</span><span class="params">(archiveStream)</span></span>;</span><br><span class="line">    archive &gt;&gt; msg;</span><br><span class="line">    <span class="keyword">return</span> msg;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="3-socket-通信"><a href="#3-socket-通信" class="headerlink" title="3. socket 通信"></a>3. socket 通信</h2><p>基于 socket 的通信，本文使用了以下两个函数：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sendto(sender_sockfd_, pot_1, <span class="keyword">sizeof</span>(pot_1), <span class="number">0</span>, (struct sockaddr *) &amp;sender_dest_addr_, <span class="keyword">sizeof</span>(sender_dest_addr_));</span><br><span class="line"></span><br><span class="line">recvfrom(receiver_sockfd_, recv_buf, <span class="keyword">sizeof</span>(recv_buf), <span class="number">0</span>, (struct sockaddr *) &amp;receiver_dest_addr_, &amp;cliaddr_len);</span><br></pre></td></tr></table></figure><h3 id="3-1-序列化数据发送端代码"><a href="#3-1-序列化数据发送端代码" class="headerlink" title="3.1 序列化数据发送端代码"></a>3.1 序列化数据发送端代码</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">sendMsg</span><span class="params">(<span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;PerceptOutput&gt; &amp;datas, <span class="keyword">const</span> Header &amp;header, <span class="keyword">const</span> PoseMsg &amp;pose)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  CommunicationMsg send_msg;</span><br><span class="line">  send_msg.header = header;</span><br><span class="line">  send_msg.pose = pose;</span><br><span class="line">  <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;PerceptResultMsg&gt; msgs = convertToMsg(datas);</span><br><span class="line"></span><br><span class="line">    <span class="comment">/// serialize send</span></span><br><span class="line">    <span class="keyword">try</span></span><br><span class="line">    &#123;</span><br><span class="line">        send_msg.pcep_msg.resize( msgs.<span class="built_in">size</span>() );</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; msgs.<span class="built_in">size</span>(); ++i)</span><br><span class="line">        &#123;</span><br><span class="line">            send_msg.pcep_msg[i] = msgs[i];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">std</span>::<span class="built_in">string</span> send_str = <span class="keyword">this</span>-&gt;Serialize( send_msg );</span><br><span class="line">        <span class="keyword">char</span> pot_1[send_str.length()];</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; send_str.length(); ++i)</span><br><span class="line">        &#123;</span><br><span class="line">            pot_1[i] = send_str.c_str()[i];</span><br><span class="line">        &#125;</span><br><span class="line">        sendto(sender_sockfd_, pot_1, <span class="keyword">sizeof</span>(pot_1), <span class="number">0</span>, (struct sockaddr *) &amp;sender_dest_addr_, <span class="keyword">sizeof</span>(sender_dest_addr_));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">catch</span> ( <span class="built_in">std</span>::exception&amp; e )</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">std</span>::<span class="built_in">cerr</span> &lt;&lt; e.what() &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>至于为什么使用异常处理模块，考虑到偶尔有无法序列化的对象，防止程序崩溃继续运行的手段，对于某一帧点云数据无法传输序列化，通过异常处理手段来跳过是可以忍受的。反序列化端接收代码同样如此。</p><h3 id="3-2-序列化数据接收端代码"><a href="#3-2-序列化数据接收端代码" class="headerlink" title="3.2 序列化数据接收端代码"></a>3.2 序列化数据接收端代码</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">receMsg</span><span class="params">(<span class="built_in">std</span>::<span class="built_in">vector</span>&lt;PerceptResultMsg&gt; &amp;msgs, Header &amp;header, PoseMsg &amp;pose)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="keyword">char</span> cli_ip[INET_ADDRSTRLEN] = <span class="string">""</span>;<span class="comment">//INET_ADDRSTRLEN=16</span></span><br><span class="line">  <span class="keyword">socklen_t</span> cliaddr_len = <span class="keyword">sizeof</span>(receiver_dest_addr_);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">char</span> recv_buf[<span class="number">50000</span>] = &#123;<span class="number">0</span>&#125;;</span><br><span class="line">  <span class="keyword">int</span> recv_len = recvfrom(receiver_sockfd_, recv_buf, <span class="keyword">sizeof</span>(recv_buf), <span class="number">0</span>, (struct sockaddr *) &amp;receiver_dest_addr_, &amp;cliaddr_len);</span><br><span class="line">  inet_ntop(AF_INET, &amp;receiver_dest_addr_.sin_addr, cli_ip, INET_ADDRSTRLEN);</span><br><span class="line"><span class="comment">/// ---------- boost deserialize ---------------</span></span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">string</span> rec_str;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; recv_len; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        rec_str += recv_buf[i];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">          CommunicationMsg boost_msg;</span><br><span class="line">          boost_msg = <span class="keyword">this</span>-&gt;Deserialize( rec_str );</span><br><span class="line">          <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"boost_msg.pcep_msg.size(): "</span> &lt;&lt; boost_msg.pcep_msg.<span class="built_in">size</span>() &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">          header = boost_msg.header;</span><br><span class="line">          pose = boost_msg.pose;</span><br><span class="line">          msgs = boost_msg.pcep_msg;</span><br><span class="line">      &#125;</span><br><span class="line">  <span class="keyword">catch</span> ( <span class="built_in">std</span>::exception&amp; e )</span><br><span class="line">      &#123;</span><br><span class="line">            <span class="built_in">std</span>::<span class="built_in">cerr</span> &lt;&lt; e.what() &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">      &#125;</span><br><span class="line">  <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>此次小的更新虽然只是一个很小部分代码更新，对于整体性能影响也不大，顶多对网络带宽占用率有一点降低，提升一点传输速度，但是在此处花费的时间将近一周，实在不应该。个人觉得应该学到的更多的是找到问题，解决问题的能力。对于调试大型工程代码有了一点自己的认识，从未接触过的领域，可以触类旁通的去理解，但实际问题还是需要实际的知识和工具来解决，不要局限在仅有的知识储备上来看问题，容易闭门造车。</p><p>以上。</p><hr><p>参考文献：</p><ol><li><a href="https://blog.csdn.net/zj510/article/details/8105408" target="_blank" rel="noopener">Boost - 序列化 (Serialization)</a></li><li><a href="https://www.boost.org/doc/libs/1_65_1/libs/serialization/doc/tutorial.html" target="_blank" rel="noopener">boost::serialize 官方教程</a></li><li><a href="https://blog.csdn.net/dugaoda/article/details/51005335" target="_blank" rel="noopener">使用boost库序列化传输对象</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在自动驾驶领域，传感器与主机通过网线连接，实现二者的实时通信。同时，在接收到雷达数据之后，后台处理系统到前端用户显示界面，也需要通过上述方法进行通信，因为后台处理系统一般都不自带显示器，例如PX-2，TX2等平台就提供网线接口。&lt;br&gt;本文以&lt;a href=&quot;http://robosense.cn/rslidar/RS-LiDAR-16&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;16线激光雷达&lt;/a&gt;生成数据，处理后台系统处理后点云为例，进行分割分类数据传输到前端用户界面进行显示。
    
    </summary>
    
    
    
      <category term="C++" scheme="http://zengzeyu.com/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>YOLO系列论文翻译</title>
    <link href="http://zengzeyu.com/2019/06/29/YOLO/"/>
    <id>http://zengzeyu.com/2019/06/29/YOLO/</id>
    <published>2019-06-29T11:12:00.000Z</published>
    <updated>2020-07-15T04:37:28.132Z</updated>
    
    <content type="html"><![CDATA[<p>YOLO为一种新的目标检测方法，该方法的特点是实现快速检测的同时还达到较高的准确率。作者将目标检测任务看作目标区域预测和类别预测的回归问题。该方法采用单个神经网络直接预测物品边界和类别概率，实现端到端的物品检测。同时，该方法检测速非常快，基础版可以达到45帧/s的实时检测；FastYOLO可以达到155帧/s。与当前最好系统相比，YOLO目标区域定位误差更大，但是背景预测的假阳性优于当前最好的方法。<a id="more"></a></p><h2 id="YOLOv1"><a href="#YOLOv1" class="headerlink" title="YOLOv1"></a>YOLOv1</h2><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>YOLO为一种新的目标检测方法，该方法的特点是实现快速检测的同时还达到较高的准确率。作者将目标检测任务看作目标区域预测和类别预测的回归问题。该方法采用单个神经网络直接预测物品边界和类别概率，实现端到端的物品检测。同时，该方法检测速非常快，基础版可以达到45帧/s的实时检测；FastYOLO可以达到155帧/s。与当前最好系统相比，YOLO目标区域定位误差更大，但是背景预测的假阳性优于当前最好的方法。</p><h3 id="1-前言"><a href="#1-前言" class="headerlink" title="1. 前言"></a>1. 前言</h3><p>人类视觉系统快速且精准，只需瞄一眼（You Only Look Once，YOLO）即可识别图像中物品及其位置。</p><p>传统目标检测系统采用deformable parts models (DPM)方法，通过滑动框方法提出目标区域，然后采用分类器来实现识别。近期的R-CNN类方法采用region proposal methods，首先生成潜在的bounding boxes，然后采用分类器识别这些bounding boxes区域。最后通过post-processing来去除重复bounding boxes来进行优化。这类方法流程复杂，存在速度慢和训练困难的问题。</p><p>本文中，我们将目标检测问题转换为直接从图像中提取bounding boxes和类别概率的单个回归问题，只需一眼（you only look once，YOLO）即可检测目标类别和位置。</p><p>YOLO采用单个卷积神经网络来预测多个bounding boxes和类别概率，如Figure-1所示。本方法相对于传统方法有如下有优点：</p><p>一，非常快。YOLO预测流程简单，速度很快。我们的基础版在Titan X GPU上可以达到45帧/s； 快速版可以达到150帧/s。因此，YOLO可以实现实时检测。</p><p>二，YOLO采用全图信息来进行预测。与滑动窗口方法和region proposal-based方法不同，YOLO在训练和预测过程中可以利用全图信息。Fast R-CNN检测方法会错误的将背景中的斑块检测为目标，原因在于Fast R-CNN在检测中无法看到全局图像。相对于Fast R-CNN，YOLO背景预测错误率低一半。</p><p>三，YOLO可以学习到目标的概括信息（generalizable representation），具有一定普适性。我们采用自然图片训练YOLO，然后采用艺术图像来预测。YOLO比其它目标检测方法（DPM和R-CNN）准确率高很多。</p><p>YOLO的准确率没有最好的检测系统准确率高。YOLO可以快速识别图像中的目标，但是准确定位目标（特别是小目标）有点困难。<br><img data-src="/images/19_6_29/YOLO/20190516165811568.png" alt="在这里插入图片描述"></p><h3 id="2-统一检测-Unified-Detection"><a href="#2-统一检测-Unified-Detection" class="headerlink" title="2. 统一检测(Unified Detection)"></a>2. 统一检测(Unified Detection)</h3><p>作者将目标检测的流程统一为单个神经网络。该神经网络采用整个图像信息来预测目标的bounding boxes的同时识别目标的类别，实现端到端实时目标检测任务。</p><p>如图Figure-2所示，YOLO首先将图像分为S×S的格子（grid cell）。如果一个目标的中心落入格子，该格子就负责检测该目标。每一个格子（grid cell）预测bounding boxes（B）和该boxes的置信值（confidence score）。置信值代表box包含一个目标的置信度。然后，我们定义置信值为 $Pr(Object) * IOU^{truth}_{pred}$。如果没有目标，则置信值为零。另外，我们希望预测的置信值和ground truth的intersection over union (IOU)相同。</p><p>每一个bounding box包含5个值：x，y，w，h和confidence。（x，y）代表与格子相关的box的中心。（w，h）为与全图信息相关的box的宽和高。confidence代表预测boxes的IOU和ground truth。</p><p>每个格子（grid cell）预测条件概率值C（$Pr(Class_i | Object)$）。概率值C代表了格子包含一个目标的概率，每一格子只预测一类概率。在测试时，每个box通过类别概率和box置信度相乘来得到特定类别置信分数：<br>$Pr(Class_i|Object)*Pr(Object)*IOU^{truth}_{pred} = Pr(Class_i)*IOU^{truth}_{pred}$</p><p>这个分数代表该类别出现在box中的概率和box和目标的合适度。在PASCAL VOC数据集上评价时，我们采用S=7，B=2，C=20（该数据集包含20个类别），最终预测结果为7×7×30的tensor。<br><img data-src="/images/19_6_29/YOLO/20190516170312825.png" alt="在这里插入图片描述"></p><h4 id="2-1-网络结构"><a href="#2-1-网络结构" class="headerlink" title="2.1 网络结构"></a>2.1 网络结构</h4><p>模型采用卷积神经网络结构。开始的卷积层提取图像特征，全连接层预测输出概率。模型结构类似于GoogleNet，如图Figure-3所示。作者还训练了YOLO的快速版本（Fast YOLO）。Fast YOLO模型卷积层和filter更少。最终输出为7×7×30的tensor。<br><img data-src="/images/19_6_29/YOLO/v2-ee4db90336d60d251d7254f9918c3a48_r.jpg" alt="在这里插入图片描述"></p><h4 id="2-2-训练方法"><a href="#2-2-训练方法" class="headerlink" title="2.2 训练方法"></a>2.2 训练方法</h4><p>作者采用ImageNet 1000-class 数据集来预训练卷积层。预训练阶段，采用图2-2网络中的前20卷积层，外加average-pooling 层和全连接层。模型训练了一周，获得了top-5 accuracy为0.88（ImageNet2012 validation set），与GoogleNet模型准确率相当。然后，将模型转换为检测模型。作者向预训练模型中加入了4个卷积层和两层全连接层，提高了模型输入分辨率（224×224-&gt;448×448）。顶层预测类别概率和bounding box协调值。bounding box的宽和高通过输入图像宽和高归一化到0-1区间。顶层采用linear activation，其它层使用 leaky rectified linear。作者采用sum-squared error为目标函数来优化，增加bounding box loss权重，减少置信度权重，实验中，设定为$\lambda_{coord} = 5 and \lambda_{noobj}=0.5$。</p><p>训练阶段的总loss函数如下：</p><p><img data-src="/images/19_6_29/YOLO/20190516170638756.png" alt="在这里插入图片描述"></p><p>作者在PASCAL VOC2007和PASCAL VOC2012数据集上进行了训练和测试。训练135轮，batch size为64，动量为0.9，学习速率延迟为0.0005. Learning schedule为：第一轮，学习速率从0.001缓慢增加到0.01（因为如果初始为高学习速率，会导致模型发散）；保持0.01速率到75轮；然后在后30轮中，下降到0.001；最后30轮，学习速率为0.0001.</p><p>作者还采用了dropout和 data augmentation来预防过拟合。dropout值为0.5；data augmentation包括：random scaling，translation，adjust exposure和saturation。</p><h4 id="2-3-预测"><a href="#2-3-预测" class="headerlink" title="2.3 预测"></a>2.3 预测</h4><p>对于PASCAL VOC数据集，模型需要对每张图片预测98个bounding box和对应的类别。对于大部分目标只包含一个box；其它有些面积大的目标包含了多个boxes，采用了Non-maximal suppression（非最大值抑制）来提高准确率。</p><h4 id="2-4-缺点"><a href="#2-4-缺点" class="headerlink" title="2.4 缺点"></a>2.4 缺点</h4><ol><li>YOLO的每一个网格只预测两个boxes，一种类别。这导致模型对相邻目标预测准确率下降。因此，YOLO对成队列的目标（如 一群鸟）识别准确率较低。</li><li>YOLO是从数据中学习预测bounding boxes，因此，对新的或者不常见角度的目标无法识别。</li><li>YOLO的loss函数对small bounding boxes和large bounding boxes的error平等对待，影响了模型识别准确率。因为对于小的bounding boxes，small error影响更大。</li></ol><h3 id="3-效果对比"><a href="#3-效果对比" class="headerlink" title="3. 效果对比"></a>3. 效果对比</h3><p>文中比较了YOLO和其它目标检测方法（Deformable parts models，R-CNN，Faster R-CNN，Deep MultiBox，OverFeat，MultiGrasp）</p><h3 id="4-实验结果"><a href="#4-实验结果" class="headerlink" title="4. 实验结果"></a>4. 实验结果</h3><h4 id="4-1-与其它检测方法效果对比"><a href="#4-1-与其它检测方法效果对比" class="headerlink" title="4.1 与其它检测方法效果对比"></a>4.1 与其它检测方法效果对比</h4><p>如表Table-1所示，在准确率保证的情况下，YOLO速度快于其它方法。</p><p><img data-src="/images/19_6_29/YOLO/20190516170728941.png" alt="在这里插入图片描述"></p><h4 id="4-2-VOC2007-错误项目分析"><a href="#4-2-VOC2007-错误项目分析" class="headerlink" title="4.2 VOC2007 错误项目分析"></a>4.2 VOC2007 错误项目分析</h4><p>文中比较了YOLO和Faster R-CNN的错误情况，结果如图4-1所示。YOLO定位错误率高于Fast R-CNN；Fast R-CNN背景预测错误率高于YOLO。</p><p>预测结果包括以下几类：</p><p>正确：类别正确，IOU&gt;0.5</p><p>定位：类别正确，0.1&lt;IOU&lt;0.5</p><p>类似：类别相似，IOU&gt;0.1</p><p>其它：类别错误，IOU&gt;0.1</p><p>背景：IOU&lt;0.1</p><p><img data-src="/images/19_6_29/YOLO/20190516170807390.png" alt="在这里插入图片描述"></p><p>图4-1 错误项目分析</p><h4 id="4-3-结合Fast-R-CNN和YOLO"><a href="#4-3-结合Fast-R-CNN和YOLO" class="headerlink" title="4.3 结合Fast R-CNN和YOLO"></a>4.3 结合Fast R-CNN和YOLO</h4><p>YOLO和Fast R-CNN预测错误类型不同，因此可以结合两类模型，提升结果。结果如表4-2所示。</p><p>表4-2 模型结合<br><img data-src="/images/19_6_29/YOLO/20190516171321243.png" alt="在这里插入图片描述"></p><h4 id="4-4-VOC-2012结果"><a href="#4-4-VOC-2012结果" class="headerlink" title="4.4 VOC 2012结果"></a>4.4 VOC 2012结果</h4><p>VOC2012数据集上测试结果如表4-3所示。</p><p>表4-3 VOC2012数据集测试结果</p><p><img data-src="/images/19_6_29/YOLO/20190516171321243.png" alt="在这里插入图片描述"></p><h4 id="4-5-普适性"><a href="#4-5-普适性" class="headerlink" title="4.5 普适性"></a>4.5 普适性</h4><p>我们在其它数据集（艺术品目标检测）：Picasso Dataset和People-Art Dataset测试了YOLO的性能，结果如图4-2和图4-3所示。</p><p>图4-2 艺术品目标检测结果一</p><p><img data-src="/images/19_6_29/YOLO/20190516171357525.png" alt="在这里插入图片描述"></p><p>图4-3 艺术品目标检测结果二 </p><p><img data-src="/images/19_6_29/YOLO/2019051617140744.png" alt="在这里插入图片描述"></p><h3 id="5-实时检测"><a href="#5-实时检测" class="headerlink" title="5. 实时检测"></a>5. 实时检测</h3><p>作者测试了YOLO的实时检测效果，结果参见 YouTube channel: <a href="https://goo.gl/bEs6Cj%EF%BC%88%E5%8F%AF%E6%83%9C%E8%A6%81%E7%BF%BB%E5%A2%99%E6%89%8D%E8%83%BD%E7%9C%8B%EF%BC%89%E3%80%82" target="_blank" rel="noopener">https://goo.gl/bEs6Cj（可惜要翻墙才能看）。</a></p><h3 id="6-结论"><a href="#6-结论" class="headerlink" title="6. 结论"></a>6. 结论</h3><p>YOLO为一种基于单独神经网络模型的目标检测方法，具有特点可以高准确率快速检测，同时具有一定鲁棒性，可以适用于实时目标检测。</p><p><strong>参考：</strong></p><p><a href="https://zhuanlan.zhihu.com/p/25045711" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/25045711</a></p><hr><h2 id="YOLOv2"><a href="#YOLOv2" class="headerlink" title="YOLOv2"></a>YOLOv2</h2><h3 id="简介-1"><a href="#简介-1" class="headerlink" title="简介"></a>简介</h3><p>YOLO9000是可以检测超过9000种类别的实时检测系统。首先，作者在YOLO基础上进行了一系列的改进，产生了YOLOv2。YOLOv2在PASCAL VOC和COCO数据集上获得了目前最好的结果（state of the art）。然后，采用多尺度训练方法，YOLOv2可以根据速度和精确度需求调整输入尺寸。67FPS时，YOLOv2在VOC2007数据集上可以达到76.8mAP；40FPS，可以达到78.6mAP，比目前最好的Faster R-CNN和SSD精确度更高，检测速度更快。最后提出了目标检测和分类的共训练方法。采用该方法，作者分别在COCO目标检测数据集和ImageNet分类数据集上训练了YOLO9000。联合训练使YOLO9000可以预测没有labelled的目标检测数据。YOLO9000在ImageNet验证集（200类）上获得了19.7mAP。其中，156类没有出现在COCO训练集中，YOLO9000获得了16.0mAP。YOLO9000可以实时识别超过9000类别。</p><h3 id="1-前言-1"><a href="#1-前言-1" class="headerlink" title="1. 前言"></a>1. 前言</h3><p>目标检测系统要求快速，准确以及能识别大范围种类数量。但是，目前基于深度神经网络方法的目前检测系统能识别的物品种类较少。其原因在于：相对于物品分类数据集，目标检测数据集中的物品种类较少。标记目标识别数据集所耗费的精力远大于标记物品分类数据集。物品分类数据集包含成千上万种超过数百万张图片，而目标识别数据集就很小了。</p><p>本文中，作者提出了一种结合不同类型数据集的方法。基于该方法，作者提出了一种新的联合训练方法，结合目前物品分类数据集的优点，将其应用于训练目标检测模型。模型可以从目标检测数据集中学会准确定位目标，同时从物品分类数据集中学会识别更多的种类，增强模型的鲁棒性。</p><p>采用该方法，作者训练了可以识别超过9000种物品的实时目标检测与识别系统-YOLO9000。首先，作者在YOLO的基础上进行了改进，产生了YOLOv2（state of the art）。然后，作者采用数据集结合方法和联合训练方法，采用ImageNet和COCO数据集训练该模型，使该模型可以识别和检测超过9000种类别。</p><h3 id="2-优化-YOLOv2"><a href="#2-优化-YOLOv2" class="headerlink" title="2. 优化-YOLOv2"></a>2. 优化-YOLOv2</h3><p>YOLO相对于目前最好的目标检测系统存在的问题是精确度不够。错误项目分析显示，相对于Fast R-CNN，YOLO在目标定位方面错误率较高。因此，对于YOLO的改进集中于在保持分类准确率的基础上增强定位精确度。改进的项目如Table 2所示。</p><p><img data-src="/images/19_6_29/YOLO/20190516165655272.png" alt="在这里插入图片描述"></p><h4 id="2-1-Batch-Normalization"><a href="#2-1-Batch-Normalization" class="headerlink" title="2.1 Batch Normalization"></a>2.1 Batch Normalization</h4><p>Batch Normalization可以提高模型收敛速度，减少过拟合。作者在所有卷积层应用了Batch Normalization，使结果提升了2%。同时，Batch Normalization的应用，去除了dropout，而不会过拟合。</p><h4 id="2-2-High-Resolution-Classifier"><a href="#2-2-High-Resolution-Classifier" class="headerlink" title="2.2 High Resolution Classifier"></a>2.2 High Resolution Classifier</h4><p>目前最好的图像分类器采用基于ImageNet数据集预训练模型。大部分类器输入图像尺寸小于256×256。原始YOLO接受图像尺寸为224×224。在YOLOv2中，作者首先采用448×448分辨率的ImageNet数据finetune使网络适应高分辨率输入；然后将该网络用于目标检测任务finetune。高分辨率输入使结果提升了4%mAP。</p><h4 id="2-3-Convolutional-With-Anchor-Boxes"><a href="#2-3-Convolutional-With-Anchor-Boxes" class="headerlink" title="2.3 Convolutional With Anchor Boxes"></a>2.3 Convolutional With Anchor Boxes</h4><p>YOLO采用全连接层来直接预测bounding boxes，而Fast R-CNN采用人工选择的bounding boxes。Fast R-CNN中的 region proposal network仅采用卷积层来预测固定的boxes（anchor boxes）的偏移和置信度。</p><p>作者去除了YOLO的全连接层，采用固定框（anchor boxes）来预测bounding boxes。首先，去除了一个pooling层来提高卷积层输出分辨率。然后，修改网络输入尺寸：由448×448改为416，使特征图只有一个中心。物品（特别是大的物品）更有可能出现在图像中心。YOLO的卷积层下采样率为32，因此输入尺寸变为416,输出尺寸为13×13。</p><p>采用anchor boxes，提升了精确度。YOLO每张图片预测98个boxes，但是采用anchor boxes，每张图片可以预测超过1000个boxes。YOLO模型精确度为69.5mAP，recall为81%；采用anchor boxes方法后，结果为69.2mAP，recall为88%。</p><h4 id="2-4-Dimension-Clusters"><a href="#2-4-Dimension-Clusters" class="headerlink" title="2.4 Dimension Clusters"></a>2.4 Dimension Clusters</h4><p>在YOLO模型上采用anchor boxes有两个关键。第一，box维度为人工选择。模型可以学会适应boxes，但是如果人工选择更好的boxes，可以让模型更加容易学习。我们采用K-means聚类方法来自动选择最佳的初始boxes。我们希望的是人工选择的boxes提高IOU分数，因此，我们公式定义为：$d(box, centroid) = 1 - IOU(box, centroid)$。k-means结果如图Figure-2所示，作者选择了k=5。</p><p><img data-src="/images/19_6_29/YOLO/20190516165730919.png" alt="在这里插入图片描述"></p><h4 id="2-5-Direct-location-prediction"><a href="#2-5-Direct-location-prediction" class="headerlink" title="2.5 Direct location prediction"></a>2.5 Direct location prediction</h4><p>在YOLO模型上采用anchor boxes的第二个关键是模型不稳定性，特别是在前面几轮训练。大部分不稳定因素来源于预测boxes位置（x，y）。</p><p>作者将预测偏移量改变为YOLO的预测grid cell的位置匹配性（location coordinate），将预测值限定在0-1范围内，增强稳定性。网络对feature map中的每个cell预测5个bounding boxes。对每一个bounding boxes，模型预测5个匹配性值（$t_{x},t_{y} ,t_{w} ,t_{h} ,t_{o}$）。采用k-means聚类方法选择boxes维度和直接预测bounding boxes中心位置提高YOLO将近5%准确率</p><h4 id="2-6-Fine-Grained-Features"><a href="#2-6-Fine-Grained-Features" class="headerlink" title="2.6 Fine-Grained Features"></a>2.6 Fine-Grained Features</h4><p>改进后的YOLO对13×13的feature map进行目标检测。更精确的特征（finer grained features）可以提高对于小目标的检测。作者向网络加入passtrough层以增加特征。passthrough类似于ResNet，将高分辨率特征和低分辨率特征结合，使26×26×512的特征图转化为13×13×2048的特征图。该改进增加了1%的性能。</p><h4 id="2-7-Multi-Scale-Training（多尺度训练）"><a href="#2-7-Multi-Scale-Training（多尺度训练）" class="headerlink" title="2.7 Multi-Scale Training（多尺度训练）"></a>2.7 Multi-Scale Training（多尺度训练）</h4><p>最初的YOLO输入尺寸为448×448，加入anchor boxes后，输入尺寸为416×416。模型只包含卷积层和pooling 层，因此可以随时改变输入尺寸。</p><p>作者在训练时，每隔几轮便改变模型输入尺寸，以使模型对不同尺寸图像具有鲁棒性。每个10 batches，模型随机选择一种新的输入图像尺寸（320，352，…，608，32的倍数，因为模型下采样因子为32），改变模型输入尺寸，继续训练。</p><p>该训练规则强迫模型取适应不同的输入分辨率。模型对于小尺寸的输入处理速度更快，因此YOLOv2可以按照需求调节速度和准确率。在低分辨率情况下（288×288），YOLOv2可以在保持和Fast R-CNN持平的准确率的情况下，处理速度可以达到90FPS。在高分辨率情况下，YOLOv2在VOC2007数据集上准确率可以达到state of the art（78.6mAP），如表2-2所示。</p><h3 id="3-检测更加快速-Faster"><a href="#3-检测更加快速-Faster" class="headerlink" title="3. 检测更加快速(Faster)"></a>3. 检测更加快速(Faster)</h3><p>大部分检测框架是基于VGG-16作为特征提取网络，但是VGG-16比较复杂，耗费计算量大。YOLO框架使用了类似googlenet的网络结构，计算量比VGG-16小，准确率比VGG16略低。</p><h4 id="3-1-Darknet-19"><a href="#3-1-Darknet-19" class="headerlink" title="3.1 Darknet-19"></a>3.1 Darknet-19</h4><p>作者设计了一个新的分类网络（Darknet-19）来作为YOLOv2的基础模型。Darknet-19模型结构如表3-1所示。</p><p><img data-src="/images/19_6_29/YOLO/v2-e79c2f41d984c69cd3aa805f86c6abe9_hd.png" alt="img"></p><h4 id="3-2-分类任务训练"><a href="#3-2-分类任务训练" class="headerlink" title="3.2 分类任务训练"></a>3.2 分类任务训练</h4><p>作者采用ImageNet1000类数据集来训练分类模型。训练过程中，采用了 random crops, rotations, and hue, saturation, and exposure shifts等data augmentation方法。预训练后，作者采用高分辨率图像（448×448）对模型进行finetune。</p><h4 id="3-3-检测任务训练"><a href="#3-3-检测任务训练" class="headerlink" title="3.3 检测任务训练"></a>3.3 检测任务训练</h4><p>作者将分类模型的最后一层卷积层去除，替换为三层卷积层（3×3,1024 filters），最后一层为1×1卷积层，filters数目为需要检测的数目。对于VOC数据集，我们需要预测5个boxes，每个boxes包含5个适应度值，每个boxes预测20类别。因此，输出为125（5<em>20+5</em>5） filters。最后还加入了passthough 层。</p><h3 id="4-更强-stronger"><a href="#4-更强-stronger" class="headerlink" title="4. 更强(stronger)"></a>4. 更强(stronger)</h3><p>作者提出了将分类数据和检测数据综合的联合训练机制。该机制使用目标检测标签的数据训练模型学习定位目标和检测部分类别的目标；再使用分类标签的数据取扩展模型对多类别的识别能力。在训练的过程中，混合目标检测和分类的数据集。当网络接受目标检测的训练数据，反馈网络采用YOLOv2 loss函数；当网络接受分类训练数据，反馈网络只更新部分网络参数。</p><p>这类训练方法有一定的难度。目标识别数据集仅包含常见目标和标签（比如狗，船）；分类数据集包含更广和更深的标签。比如狗，ImageNet上包含超过100种的狗的类别。如果要联合训练，需要将这些标签进行合并。</p><p>大部分分类方法采用softmax输出所有类别的概率。采用softmax的前提假设是类别之间不相互包含（比如，犬和牧羊犬就是相互包含）。因此，我们需要一个多标签的模型来综合数据集，使类别之间不相互包含。</p><h4 id="4-1-多层分类-Hierarchical-classification"><a href="#4-1-多层分类-Hierarchical-classification" class="headerlink" title="4.1 多层分类(Hierarchical classification)"></a>4.1 多层分类(Hierarchical classification)</h4><p>数据处理…</p><h3 id="5-结论"><a href="#5-结论" class="headerlink" title="5. 结论"></a>5. 结论</h3><p>作者通过对YOLO网络结构和训练方法的改进，提出了YOLOv2和YOLO9000两种实时目标检测系统。YOLOv2在YOLO的基础上进行了一系列的改进，在快速的同时达到state of the art。同时，YOLOv2可以适应不同的输入尺寸，<strong>根据需要调整检测准确率和检测速度（值得参考）</strong>。作者综合了ImageNet数据集和COCO数据集，采用联合训练的方式训练，使该系统可以识别超过9000种物品。除此之外，作者提出的WordTree可以综合多种数据集 的方法可以应用于其它计算机数觉任务中。</p><p><strong>参考：</strong></p><p><a href="https://zhuanlan.zhihu.com/p/25052190" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/25052190</a></p><hr><h2 id="YOLOv3"><a href="#YOLOv3" class="headerlink" title="YOLOv3"></a>YOLOv3</h2><h3 id="简介-2"><a href="#简介-2" class="headerlink" title="简介"></a>简介</h3><p>本文为YOLO提供了一系列更新！它包含一堆小设计，可以使系统的性能得到更新；也包含一个新训练的、非常棒的神经网络，虽然比上一版更大一些，但精度也提高了。不用担心，虽然体量大了点，它的速度还是有保障的。在输入320×320的图片后，YOLOv3能在22毫秒内完成处理，并取得28.2mAP的成绩。它的精度和SSD相当，但速度要快上3倍。和旧版数据相比，v3版进步明显。在Titan X环境下，YOLOv3的检测精度为57.9AP5057.9AP50，用时51ms；而RetinaNet的精度只有57.5AP，但却需要198ms，相当于YOLOv3的3.8倍。</p><h3 id="2-方法"><a href="#2-方法" class="headerlink" title="2. 方法"></a>2. 方法</h3><p><img data-src="/images/19_6_29/YOLO/v2-e79c2f41d984c69cd3aa805f86c6abe9_hd.png" alt="在这里插入图片描述"></p><h4 id="2-1-Bounding-Box-Prediction"><a href="#2-1-Bounding-Box-Prediction" class="headerlink" title="2.1 Bounding Box Prediction"></a>2.1 Bounding Box Prediction</h4><p>这里和原来v2基本没区别。仍然使用聚类产生anchor box的长宽（下式的pwpw和phph）。网络预测四个值：tx，ty，tw，th。我们知道，YOLO网络最后输出是一个M×M的feature map，对应于M×M个cell。如果某个cell距离image的top left corner距离为(cx,cy)（也就是cell的坐标），那么该cell内的bounding box的位置和形状参数为：</p><p><img data-src="/images/19_6_29/YOLO/20190516171655935.png" alt="在这里插入图片描述"></p><p>PS：这里有一个问题，不管FasterRCNN还是YOLO，都不是直接回归bounding box的长宽（就像这样：$bw=p_w*e^{t_w}$），而是要做一个对数变换，实际预测的是$log(⋅)$。这里小小解释一下。</p><p>这是因为如果不做变换，直接预测相对形变$t_w$，那么要求$t_w&gt;0$，因为你的框框的长宽不可能是负数。这样，是在做一个有不等式条件约束的优化问题，没法直接用SGD来做。所以先取一个对数变换，将其不等式约束去掉，就可以了。<br><img data-src="/images/19_6_29/YOLO/20190516171714280.png" alt="在这里插入图片描述"></p><p>在训练的时候，使用平方误差损失。</p><p>另外，YOLO会对每个bounding box给出是否是object的置信度预测，用来区分objects和背景。这个值使用logistic回归。当某个bounding box与ground truth的IoU大于其他所有bounding box时，target给11；如果某个bounding box不是IoU最大的那个，但是IoU也大于了某个阈值（我们取0.5），那么我们忽略它（既不惩罚，也不奖励），这个做法是从Faster RCNN借鉴的。我们对每个ground truth只分配一个最好的bounding box与其对应（这与Faster RCNN不同）。如果某个bounding box没有被assign到任何一个ground truth对应，那么它对边框位置大小的回归和class的预测没有贡献，我们只惩罚它的objectness，即试图减小其confidence。</p><h4 id="2-2-分类预测-Class-Prediction"><a href="#2-2-分类预测-Class-Prediction" class="headerlink" title="2.2 分类预测(Class Prediction)"></a>2.2 分类预测(Class Prediction)</h4><p>我们不用softmax做分类了，而是使用独立的logisitc做二分类。这种方法的好处是可以处理重叠的多标签问题，如Open Image Dataset。在其中，会出现诸如<code>Woman</code>和<code>Person</code>这样的重叠标签。</p><h4 id="2-3-多尺度预测-Prediction-Across-Scales"><a href="#2-3-多尺度预测-Prediction-Across-Scales" class="headerlink" title="2.3 多尺度预测(Prediction Across Scales)"></a>2.3 多尺度预测(Prediction Across Scales)</h4><p>之前YOLO的一个弱点就是缺少多尺度变换，使用<a href="https://arxiv.org/abs/1612.03144" target="_blank" rel="noopener">FPN</a>中的思路，v3在3个不同的尺度上做预测。在COCO上，我们每个尺度都预测3个框框，所以一共是9个。所以输出的feature map的大小是N×N×[3×(4+1+80)]]。</p><p>然后我们从两层前那里拿feature map，upsample 2x，并与更前面输出的feature map通过element-wide的相加做merge。这样我们能够从后面的层拿到更多的高层语义信息，也能从前面的层拿到细粒度的信息（更大的feature map，更小的感受野）。然后在后面接一些conv做处理，最终得到和上面相似大小的feature map，只不过spatial dimension变成了2倍。</p><p>照上一段所说方法，再一次在final scale尺度下给出预测。</p><h4 id="2-4-特征提取器-Feature-Extractor"><a href="#2-4-特征提取器-Feature-Extractor" class="headerlink" title="2.4 特征提取器(Feature Extractor)"></a>2.4 特征提取器(Feature Extractor)</h4><p>加入Res_Block的Darknet-53。</p><p><img data-src="/images/19_6_29/YOLO/20190516171744850.png" alt="在这里插入图片描述"></p><h3 id="3-How-We-Do"><a href="#3-How-We-Do" class="headerlink" title="3. How We Do"></a>3. How We Do</h3><p>把YOLO v3和其他方法比较，优势在于快快快。当你不太在乎IoU一定要多少多少的时候，YOLO可以做到又快又好。作者还在文章的结尾发起了这样的牢骚：使用了多尺度预测，v3对于小目标的检测结果明显变好了。不过对于medium和large的目标，表现相对不好。这是需要后续工作进一步挖局的地方。</p><h3 id="4-Things-We-Tried-That-Didn’t-Work"><a href="#4-Things-We-Tried-That-Didn’t-Work" class="headerlink" title="4. Things We Tried That Didn’t Work"></a>4. Things We Tried That Didn’t Work</h3><p>作者还贴心地给出了什么方法没有奏效。</p><ul><li>anchor box坐标(x,y)(x,y)的预测。预测anchor box的offset，no stable，不好。</li><li>线性offset预测，而不是logistic。精度下降。</li><li>focal loss。精度下降。</li><li>双IoU阈值，像Faster RCNN那样。效果不好。</li></ul><p><strong>参考：</strong></p><p><a href="https://xmfbit.github.io/2018/04/01/paper-yolov3/" target="_blank" rel="noopener">https://xmfbit.github.io/2018/04/01/paper-yolov3/</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;YOLO为一种新的目标检测方法，该方法的特点是实现快速检测的同时还达到较高的准确率。作者将目标检测任务看作目标区域预测和类别预测的回归问题。该方法采用单个神经网络直接预测物品边界和类别概率，实现端到端的物品检测。同时，该方法检测速非常快，基础版可以达到45帧/s的实时检测；FastYOLO可以达到155帧/s。与当前最好系统相比，YOLO目标区域定位误差更大，但是背景预测的假阳性优于当前最好的方法。
    
    </summary>
    
    
    
      <category term="DL" scheme="http://zengzeyu.com/tags/DL/"/>
    
      <category term="Paper" scheme="http://zengzeyu.com/tags/Paper/"/>
    
      <category term="CV" scheme="http://zengzeyu.com/tags/CV/"/>
    
  </entry>
  
  <entry>
    <title>My Paper Lib 2018</title>
    <link href="http://zengzeyu.com/2019/03/24/My-Paper-Lib-2018/"/>
    <id>http://zengzeyu.com/2019/03/24/My-Paper-Lib-2018/</id>
    <published>2019-03-24T10:59:46.000Z</published>
    <updated>2020-07-15T04:40:20.519Z</updated>
    
    <content type="html"><![CDATA[<p>This is my 2018 paper lib. <a id="more"></a></p><table><thead><tr><th>paper</th><th>source</th></tr></thead><tbody><tr><td>O-CNN : Octree-based Convolutional Neural Networks for 3D Shape Analysis</td><td><a href="https://arxiv.org/pdf/1712.01537.pdf" target="_blank" rel="noopener">PDF</a>/video/<a href="https://github.com/Microsoft/O-CNN" target="_blank" rel="noopener">code</a></td></tr><tr><td>OctNet: Learning Deep 3D Representations at High Resolutions</td><td><a href="http://www.cvlibs.net/publications/Riegler2017CVPR.pdf" target="_blank" rel="noopener">PDF</a>/<a href="https://www.youtube.com/watch?v=qYyephF2BBw" target="_blank" rel="noopener">video</a>/<a href="https://github.com/griegler/octnet" target="_blank" rel="noopener">code</a></td></tr><tr><td>Parallel Separable 3D Convolution for Video and Volumetric Data Understanding</td><td><a href="https://arxiv.org/pdf/1809.04096.pdf" target="_blank" rel="noopener">PDF</a>/video/code</td></tr><tr><td>PIXOR : Real-time 3D Object Detection from Point Clouds</td><td><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Yang_PIXOR_Real-Time_3D_CVPR_2018_paper.pdf" target="_blank" rel="noopener">PDF</a>/video/<a href="https://github.com/overfitover/pixor_pytorch" target="_blank" rel="noopener">code</a></td></tr><tr><td>PointCNN</td><td><a href="https://arxiv.org/pdf/1801.07791.pdf" target="_blank" rel="noopener">PDF</a>/video/<a href="https://github.com/yangyanli/PointCNN" target="_blank" rel="noopener">code</a></td></tr><tr><td>PointNet : Deep Learning on Point Sets for 3D Classification and Segmentation</td><td><a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Qi_PointNet_Deep_Learning_CVPR_2017_paper.pdf" target="_blank" rel="noopener">PDF</a>/<a href="https://www.youtube.com/watch?v=Cge-hot0Oc0" target="_blank" rel="noopener">video</a>/<a href="https://github.com/charlesq34/pointnet" target="_blank" rel="noopener">code</a></td></tr><tr><td>PointNet ++ : Deep Hierarchical Feature Learning on Point Sets in a Metric Space</td><td><a href="https://arxiv.org/pdf/1706.02413.pdf" target="_blank" rel="noopener">PDF</a>/video/<a href="https://github.com/charlesq34/pointnet2" target="_blank" rel="noopener">code</a></td></tr><tr><td>Receptive Field Block Net for Accurate and Fast Object Detection</td><td><a href="https://arxiv.org/pdf/1711.07767.pdf" target="_blank" rel="noopener">PDF</a>/video/<a href="https://github.com/ruinmessi/RFBNet" target="_blank" rel="noopener">code</a></td></tr><tr><td>Deep Residual Learning for Image Recognition(ResNet)</td><td><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf" target="_blank" rel="noopener">PDF</a>/<a href="https://www.youtube.com/watch?v=C6tLw-rPQ2o" target="_blank" rel="noopener">video</a>/code</td></tr><tr><td>Rethinking Atrous Convolution for Semantic Image Segmentation</td><td><a href="https://arxiv.org/pdf/1706.05587" target="_blank" rel="noopener">PDF</a>/video/code</td></tr><tr><td>Rich feature hierarchies for accurate object detection and semantic segmentation</td><td><a href="https://arxiv.org/pdf/1311.2524" target="_blank" rel="noopener">PDF</a>/video/code</td></tr><tr><td>Sparse 3D convolutional neural networks</td><td><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Liu_Sparse_Convolutional_Neural_2015_CVPR_paper.pdf" target="_blank" rel="noopener">PDF</a>/video/<a href="">code</a></td></tr><tr><td>SPLATNet : Sparse Lattice Networks for Point Cloud Processing</td><td><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Su_SPLATNet_Sparse_Lattice_CVPR_2018_paper.pdf" target="_blank" rel="noopener">PDF</a>/video/<a href="https://github.com/NVlabs/splatnet" target="_blank" rel="noopener">code</a></td></tr><tr><td>SqueezeSeg: Convolutional Neural Nets with Recurrent CRF for Real-Time Road-Object Segmentation from 3D LiDAR Point Cloud</td><td><a href="https://arxiv.org/pdf/1710.07368" target="_blank" rel="noopener">PDF</a>/video/<a href="https://github.com/BichenWuUCB/SqueezeSeg" target="_blank" rel="noopener">code</a></td></tr><tr><td>The Devil of Face Recognition is in the Noise</td><td><a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Liren_Chen_The_Devil_of_ECCV_2018_paper.pdf" target="_blank" rel="noopener">PDF</a>/video/code</td></tr><tr><td>Understanding Convolution for Semantic Segmentation</td><td><a href="https://arxiv.org/pdf/1702.08502" target="_blank" rel="noopener">PDF</a>/video/<a href="https://github.com/TuSimple/TuSimple-DUC" target="_blank" rel="noopener">code</a></td></tr><tr><td>V-Net : Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation</td><td><a href="https://arxiv.org/pdf/1606.04797" target="_blank" rel="noopener">PDF</a>/video/<a href="https://github.com/mattmacy/vnet.pytorch" target="_blank" rel="noopener">code</a></td></tr><tr><td>VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection</td><td><a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/3333.pdf" target="_blank" rel="noopener">PDF</a>/video/<a href="https://github.com/qianguih/voxelnet" target="_blank" rel="noopener">code</a></td></tr><tr><td>Xception: Deep Learning with Depthwise Separable Convolutions</td><td><a href="https://arxiv.org/pdf/1610.02357" target="_blank" rel="noopener">PDF</a>/video/code</td></tr><tr><td>PointFusion : Deep Sensor Fusion for 3D Bounding Box Estimation</td><td><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Xu_PointFusion_Deep_Sensor_CVPR_2018_paper.pdf" target="_blank" rel="noopener">PDF</a>/video/<a href="">code</a></td></tr><tr><td>Efficient Convolutions for Real-Time Semantic Segmentation of 3D Point Clouds</td><td><a href="http://www.cs.toronto.edu/~wenjie/papers/3dv18.pdf" target="_blank" rel="noopener">PDF</a>/video/<a href="">code</a></td></tr><tr><td>Exploring Spatial Context for 3D Semantic Segmentation of Point Clouds</td><td><a href="https://arxiv.org/pdf/1802.01500" target="_blank" rel="noopener">PDF</a>/<a href="https://www.youtube.com/watch?v=w--rpu2-HFs" target="_blank" rel="noopener">video</a>/<a href="">code</a></td></tr><tr><td>Factorized Convolutional Neural Networks</td><td><a href="http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w10/Wang_Factorized_Convolutional_Neural_ICCV_2017_paper.pdf" target="_blank" rel="noopener">PDF</a>/video/<a href="">code</a></td></tr><tr><td>Fast Bilateral Solver for Semantic Video Segmentation</td><td><a href="https://web.stanford.edu/class/cs331b/2016/projects/wang_kao.pdf" target="_blank" rel="noopener">PDF</a>/video/<a href="">code</a></td></tr><tr><td>Fast LIDAR-based Road Detection Using Fully Convolutional Neural Networks</td><td><a href="https://arxiv.org/pdf/1703.03613" target="_blank" rel="noopener">PDF</a>/video/<a href="">code</a></td></tr><tr><td>FishNet : A Versatile Backbone for Image , Region , and Pixel Level Prediction</td><td><a href="https://papers.nips.cc/paper/7356-fishnet-a-versatile-backbone-for-image-region-and-pixel-level-prediction.pdf" target="_blank" rel="noopener">PDF</a>/video/<a href="https://github.com/kevin-ssy/FishNet" target="_blank" rel="noopener">code</a></td></tr><tr><td>Flattened Convolutional Neural Networks for Feedforward Acceleration</td><td><a href="https://arxiv.org/pdf/1412.5474" target="_blank" rel="noopener">PDF</a>/video/<a href="https://github.com/jhjin/flattened-cnn" target="_blank" rel="noopener">code</a></td></tr><tr><td>Focal Loss for Dense Object Detection</td><td><a href="https://arxiv.org/pdf/1708.02002" target="_blank" rel="noopener">PDF</a>/video/<a href="https://github.com/unsky/focal-loss" target="_blank" rel="noopener">code</a></td></tr><tr><td>Frustum PointNets for 3D Object Detection from RGB-D Data</td><td><a href="https://arxiv.org/pdf/1711.08488" target="_blank" rel="noopener">PDF</a>/video/<a href="https://github.com/charlesq34/frustum-pointnets" target="_blank" rel="noopener">code</a></td></tr><tr><td>Full-Resolution Residual Networks for Semantic Segmentation in Street Scenes</td><td><a href="">PDF</a>/video/<a href="">code</a></td></tr><tr><td>Fully Convolutional Networks for Semantic Segmentation</td><td><a href="https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf" target="_blank" rel="noopener">PDF</a>/video/<a href="">code</a></td></tr><tr><td>Fully-Convolutional Point Networks for Large-Scale Point Clouds</td><td><a href="https://arxiv.org/pdf/1808.06840" target="_blank" rel="noopener">PDF</a>/video/<a href="https://github.com/drethage/fully-convolutional-point-network" target="_blank" rel="noopener">code</a></td></tr><tr><td>Fast R-CNN</td><td><a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf" target="_blank" rel="noopener">PDF</a>/video/<a href="https://github.com/rbgirshick/fast-rcnn" target="_blank" rel="noopener">code</a></td></tr><tr><td>Going Deeper with Convolutions（GoogLeNet）</td><td><a href="https://arxiv.org/pdf/1409.4842.pdf" target="_blank" rel="noopener">PDF</a>/video/<a href="">code</a></td></tr><tr><td>Ground Estimation and Point Cloud Segmentation using SpatioTemporal Conditional Random Field</td><td><a href="https://hal.inria.fr/hal-01579095/document" target="_blank" rel="noopener">PDF</a>/video/<a href="">code</a></td></tr><tr><td>HDNET : Exploiting HD Maps for 3D Object Detection</td><td><a href="http://proceedings.mlr.press/v87/yang18b/yang18b.pdf" target="_blank" rel="noopener">PDF</a>/video/<a href="">code</a></td></tr><tr><td>Inception-V4, Inception-ResNet ad the Impact of Residual Connections on Learning</td><td><a href="https://arxiv.org/pdf/1602.07261" target="_blank" rel="noopener">PDF</a>/video/<a href="">code</a></td></tr><tr><td>Instance-aware Semantic Segmentation via Multi-task Network Cascades</td><td><a href="http://openaccess.thecvf.com/content_cvpr_2016/papers/Dai_Instance-Aware_Semantic_Segmentation_CVPR_2016_paper.pdf" target="_blank" rel="noopener">PDF</a>/<a href="https://www.youtube.com/watch?v=bUjyXASy_Jo" target="_blank" rel="noopener">video</a>/<a href="https://github.com/daijifeng001/MNC" target="_blank" rel="noopener">code</a></td></tr><tr><td>Joint 3D Proposal Generation and Object Detection from View Aggregation(AVOD)</td><td><a href="https://arxiv.org/pdf/1712.02294" target="_blank" rel="noopener">PDF</a>/<a href="https://www.youtube.com/watch?v=Q1f-s6_yHtw" target="_blank" rel="noopener">video</a>/<a href="">code</a></td></tr><tr><td>Large-scale Point Cloud Semantic Segmentation with Superpoint Graphs</td><td><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Landrieu_Large-Scale_Point_Cloud_CVPR_2018_paper.pdf" target="_blank" rel="noopener">PDF</a>/video/<a href="https://github.com/loicland/superpoint_graph" target="_blank" rel="noopener">code</a></td></tr><tr><td>Learning 3D Shape Completion from Laser Scan Data with Weak Supervision</td><td><a href="http://www.cvlibs.net/publications/Stutz2018CVPR.pdf" target="_blank" rel="noopener">PDF</a>/video/<a href="https://github.com/davidstutz/cvpr2018-shape-completion" target="_blank" rel="noopener">code</a></td></tr><tr><td>Learning a Real-Time 3D Point Cloud Obstacle Discriminator via Bootstrapping</td><td><a href="https://www.velodynelidar.com/lidar/hdlpressroom/pdf/papers/journal_papers/Learning%20a%20Real-Time%203D%20Point%20Cloud%20Obstacle%20Discriminator%20via%20Bootstrapping.pdf" target="_blank" rel="noopener">PDF</a>/video/<a href="">code</a></td></tr><tr><td>Learning Spatio-Temporal Representation with Pseudo-3D Residual Networks</td><td><a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Qiu_Learning_Spatio-Temporal_Representation_ICCV_2017_paper.pdf" target="_blank" rel="noopener">PDF</a>/video/<a href="https://github.com/ZhaofanQiu/pseudo-3d-residual-networks" target="_blank" rel="noopener">code</a></td></tr><tr><td>Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics</td><td><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Kendall_Multi-Task_Learning_Using_CVPR_2018_paper.pdf" target="_blank" rel="noopener">PDF</a>/video/<a href="">code</a></td></tr><tr><td>Multi-View 3D Object Detection Network for Autonomous Driving</td><td><a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Chen_Multi-View_3D_Object_CVPR_2017_paper.pdf" target="_blank" rel="noopener">PDF</a>/video/<a href="https://github.com/bostondiditeam/MV3D" target="_blank" rel="noopener">code</a></td></tr><tr><td>Not All Pixels Are Equal : Difficulty-Aware Semantic Segmentation via Deep Layer Cascade</td><td><a href="https://arxiv.org/pdf/1704.01344" target="_blank" rel="noopener">PDF</a>/video/<a href="https://github.com/liuziwei7/region-conv" target="_blank" rel="noopener">code</a></td></tr><tr><td>3D Fully Convolutional Network for Vehicle Detection in Point Cloud</td><td><a href="https://arxiv.org/pdf/1611.08069" target="_blank" rel="noopener">PDF</a>/video/<a href="">code</a></td></tr><tr><td>3D Semantic Segmentation with Submanifold Sparse Convolutional Networks</td><td><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Graham_3D_Semantic_Segmentation_CVPR_2018_paper.pdf" target="_blank" rel="noopener">PDF</a>/video/<a href="">code</a></td></tr><tr><td>3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation</td><td><a href="https://arxiv.org/pdf/1606.06650" target="_blank" rel="noopener">PDF</a>/video/<a href="https://github.com/shiba24/3d-unet" target="_blank" rel="noopener">code</a></td></tr><tr><td>Deconvolutional Networks for Point-Cloud Vehicle Detection and Tracking in Driving Scenarios</td><td><a href="https://upcommons.upc.edu/bitstream/handle/2117/114460/1906-Deconvolutional-Networks-for-Point-Cloud-Vehicle-Detection-and-Tracking-in-Driving-Scenarios.pdf;jsessionid=60063FD0E6A7237ADDA02C102C9C9A5B?sequence=1" target="_blank" rel="noopener">PDF</a>/video/<a href="">code</a></td></tr><tr><td>Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour</td><td><a href="https://arxiv.org/pdf/1706.02677.pdf" target="_blank" rel="noopener">PDF</a>/video/<a href="">code</a></td></tr><tr><td>Acquisition of Localization Confidence for Accurate Object Detection(IouNet)</td><td><a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Borui_Jiang_Acquisition_of_Localization_ECCV_2018_paper.pdf" target="_blank" rel="noopener">PDF</a>/video/<a href="">code</a></td></tr><tr><td>A Hybrid Conditional Random Field for Estimating the Underlying Ground Surface from Airborne LiDAR Data</td><td><a href="https://www.cs.ubc.ca/~murphyk/Papers/tgars2009.pdf" target="_blank" rel="noopener">PDF</a>/video/<a href="">code</a></td></tr><tr><td>Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift</td><td><a href="https://arxiv.org/pdf/1502.03167.pdf" target="_blank" rel="noopener">PDF</a>/video/<a href="">code</a></td></tr><tr><td>BiSeNet : Bilateral Segmentation Network for Real-time Semantic Segmentation</td><td><a href="https://arxiv.org/pdf/1808.00897" target="_blank" rel="noopener">PDF</a>/video/<a href="">code</a></td></tr><tr><td>CNN for Very Fast Ground Segmentation in Velodyne LiDAR Data</td><td><a href="https://arxiv.org/pdf/1709.02128" target="_blank" rel="noopener">PDF</a>/video/<a href="">code</a></td></tr><tr><td>Complex-YOLO: An Euler-Region-Proposal for Real-time 3D Object Detection on Point Clouds</td><td><a href="https://arxiv.org/pdf/1803.06199" target="_blank" rel="noopener">PDF</a>/video/<a href="https://github.com/AI-liu/Complex-YOLO" target="_blank" rel="noopener">code</a></td></tr><tr><td>CornerNet: Detecting Objects as Paired Keypoints</td><td><a href="https://arxiv.org/pdf/1808.01244" target="_blank" rel="noopener">PDF</a>/video/<a href="https://github.com/princeton-vl/CornerNet" target="_blank" rel="noopener">code</a></td></tr><tr><td>Conditional Random Fields Meet Deep Neural Networks for Semantic Segmentation</td><td><a href="http://www.robots.ox.ac.uk/~tvg/publications/2017/CRFMeetCNN4SemanticSegmentation.pdf" target="_blank" rel="noopener">PDF</a>/video/<a href="">code</a></td></tr><tr><td>Decoupled Networks</td><td><a href="http://wyliu.com/papers/LiuCVPR18_DCNets.pdf" target="_blank" rel="noopener">PDF</a>/video/<a href="">code</a></td></tr><tr><td>Deep Feature Pyramid Reconfiguration for Object Detection</td><td><a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Tao_Kong_Deep_Feature_Pyramid_ECCV_2018_paper.pdf" target="_blank" rel="noopener">PDF</a>/video/<a href="">code</a></td></tr><tr><td>DropBlock : A regularization method for convolutional networks</td><td><a href="https://papers.nips.cc/paper/8271-dropblock-a-regularization-method-for-convolutional-networks.pdf" target="_blank" rel="noopener">PDF</a>/video/<a href="">code</a></td></tr><tr><td>RoarNet: A Robust 3D Object Detection based on RegiOn Approximation Refinement</td><td><a href="">PDF</a>/<a href="https://www.youtube.com/watch?v=ZmnhvLlJ6qg">video</a>/<a href="https://arxiv.org/pdf/1811.03818" target="_blank" rel="noopener">code</a></td></tr><tr><td>Dropout: A simple way to prevent neural networks from overfitting</td><td><a href="http://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf" target="_blank" rel="noopener">PDF</a>/video/<a href="">code</a></td></tr><tr><td>SECOND: Sparsely Embedded Convolutional Detection</td><td><a href="https://www.mdpi.com/1424-8220/18/10/3337/pdf" target="_blank" rel="noopener">PDF</a>/video/<a href="">code</a></td></tr><tr><td>Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</td><td><a href="https://arxiv.org/pdf/1506.01497.pdf" target="_blank" rel="noopener">PDF</a>/video/<a href="">code</a></td></tr><tr><td>Bag of Tricks for Image Classification with Convolutional Neural Networks</td><td><a href="https://arxiv.org/pdf/1812.01187" target="_blank" rel="noopener">PDF</a>/video/<a href="">code</a></td></tr><tr><td>Deformable ConvNets v2: More Deformable, Better Results</td><td><a href="https://arxiv.org/pdf/1811.11168" target="_blank" rel="noopener">PDF</a>/video/<a href="">code</a></td></tr><tr><td>Non-local Neural Networks</td><td><a href="https://arxiv.org/pdf/1711.07971" target="_blank" rel="noopener">PDF</a>/video/<a href="">code</a></td></tr><tr><td>PointPillars: Fast Encoders for Object Detection from Point Clouds</td><td><a href="https://arxiv.org/pdf/1812.05784" target="_blank" rel="noopener">PDF</a>/video/<a href="">code</a></td></tr><tr><td>Box2Pix : Single-Shot Instance Segmentation by Assigning Pixels to Object Boxes</td><td><a href="https://lmb.informatik.uni-freiburg.de/Publications/2018/UB18/paper-box2pix.pdf" target="_blank" rel="noopener">PDF</a>/video/<a href="">code</a></td></tr><tr><td>IPOD: Intensive Point-based Object Detector for Point Cloud</td><td><a href="https://arxiv.org/pdf/1812.05276" target="_blank" rel="noopener">PDF</a>/video/<a href="">code</a></td></tr><tr><td>Densely connected convolutional networks</td><td><a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Densely_Connected_Convolutional_CVPR_2017_paper.pdf" target="_blank" rel="noopener">PDF</a>/video/<a href="">code</a></td></tr><tr><td>SqueezeSegV2: Improved Model Structure and Unsupervised Domain Adaptation for Road-Object Segmentation from a LiDAR Point Cloud</td><td><a href="https://arxiv.org/pdf/1809.08495" target="_blank" rel="noopener">PDF</a>/<a href="https://www.youtube.com/watch?v=ZitFO1_YpNM" target="_blank" rel="noopener">video</a>/<a href="">code</a></td></tr><tr><td>SSD: Single Shot MultiBox Detector</td><td><a href="https://www.cs.unc.edu/~wliu/papers/ssd.pdf" target="_blank" rel="noopener">PDF</a>/video/<a href="">code</a></td></tr><tr><td>Residual Networks Behave Like Ensembles of Relatively Shallow Networks</td><td><a href="https://arxiv.org/pdf/1605.06431" target="_blank" rel="noopener">PDF</a>/<a href="https://www.youtube.com/watch?v=5wku9_vMxsA" target="_blank" rel="noopener">video</a>/code</td></tr><tr><td>Single-Shot Refinement Neural Network for Object Detection (RefineDet)</td><td><a href="https://arxiv.org/pdf/1711.06897" target="_blank" rel="noopener">PDF</a>/video/<a href="https://github.com/sfzhang15/RefineDet" target="_blank" rel="noopener">code</a></td></tr><tr><td>MIXED PRECISION TRAINING</td><td><a href="https://arxiv.org/pdf/1710.03740" target="_blank" rel="noopener">PDF</a>/video/<a href="https://github.com/suvojit-0x55aa/mixed-precision-pytorch" target="_blank" rel="noopener">code</a></td></tr><tr><td>Gradient Harmonized Single-stage Detector</td><td><a href="https://arxiv.org/pdf/1811.05181" target="_blank" rel="noopener">PDF</a>/video/<a href="https://github.com/libuyu/GHM_Detection" target="_blank" rel="noopener">code</a></td></tr><tr><td>Gather-Excite: Exploiting Feature Context in Convolutional Neural Networks</td><td><a href="https://arxiv.org/pdf/1810.12348" target="_blank" rel="noopener">PDF</a>/video/<a href="">code</a></td></tr><tr><td>GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks</td><td><a href="https://arxiv.org/pdf/1711.02257.pdf" target="_blank" rel="noopener">PDF</a>/<a href="https://vimeo.com/287812909" target="_blank" rel="noopener">video</a>/<a href="https://github.com/ignaciorlando/pytorch-grad-norm" target="_blank" rel="noopener">code</a></td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This is my 2018 paper lib.
    
    </summary>
    
    
    
      <category term="DL" scheme="http://zengzeyu.com/tags/DL/"/>
    
      <category term="Paper" scheme="http://zengzeyu.com/tags/Paper/"/>
    
  </entry>
  
  <entry>
    <title>Mixed-Precision Training of Deep Neural Networks</title>
    <link href="http://zengzeyu.com/2019/01/16/Mixed-Precision%20Training%20of%20Deep%20Neural%20Networks/"/>
    <id>http://zengzeyu.com/2019/01/16/Mixed-Precision%20Training%20of%20Deep%20Neural%20Networks/</id>
    <published>2019-01-16T10:59:46.000Z</published>
    <updated>2020-07-15T04:41:21.034Z</updated>
    
    <content type="html"><![CDATA[<p>由于FP16的精度损失问题，如果我们在神经网络的训练过程中直接将网络参数以FP16的形式进行计算，可能会出现数值不稳定的情况而导致模型性能下降。 对此，百度和NVIDIA的研究院在<a href="https://arxiv.org/abs/1710.03740" target="_blank" rel="noopener">Mixed Precision Training</a>这一论文中提出混合精度训练的方法，在充分利用FP16加速运算的优点的同时保证了模型的精度。下面介绍其论文中的主要要点。<a id="more"></a></p><p>FP32 master copy of weigths</p><p>FP32 master copy即维护一份网络中FP16精度参数的FP32精度的拷贝。 计算过程如下图所示，在前向传播过程中，使用由master copy类型转换得到的FP16精度参数进行运算； 而在反向传播计算完梯度后，将梯度作用到master copy上以在FP32精度上进行参数更新。</p><p><img data-src="/images/19_1_16/mix_precision_training/b89a595f09deb2caf14d44176f931440.png" alt="FP32 master copy">FP32 master copy</p><p>这么做的主要原因有两个。</p><p>第一是由于若直接在FP16精度下进行参数更新，存在梯度过小而导致更新值为0的情况。 下图是某次模型训练过程汇总网络参数的梯度的分布直方图，有5%的梯度值是分布在小于2-24的区间内的。 若优化器直接将这部分梯度乘上学习率作用到FP16精度参数的更新上，那么更新值将为0。 这将影响到模型的准确率。 但是如果是更新值是作用到FP32精度的master copy上时，则不会出现更新值下溢为0的情况。</p><p>第二是如果参数相较于其更新值过大的话，也可能会由于浮点数加法机制的缘故而导致更新值为0。 在浮点数加法过程中，需要将两数对齐进行运算。 如果参数的大小是其更新值的2048倍或者更大的话，那么更新值的小数位需要右移至少11位才能与前者对齐，这超出了FP16精度的表示范围。 在FP32精度下则一般不会出现这种问题。</p><ol><li><p><img data-src="/images/19_1_16/mix_precision_training/20190628220843.png" alt="gradient histogram">gradient histogram</p></li><li><p>loss scaling</p><p>loss scaling即将loss值放大，以保证反向传播过程当中梯度落在FP16精度能表示的范围之间。</p><p>下图是Multibox SSD网络在训练过程过程中激活单元梯度的分布情况。其中有67%的梯度落在了小于2-24的范围内，在FP16精度下无法表示。 如果不对梯度进行放大，在FP16精度下对该网络进行训练将导致发散。 对激活梯度进行放大，再对参数梯度缩小相应倍数，即可解决该问题。</p><p><img data-src="/images/19_1_16/mix_precision_training/ssd_ag_log_histo_coarse.png" alt="activation gradients">activation gradients</p><p>根据梯度计算的链式法则，对梯度放大的最简单的方法就是放大loss。 放大因子的大小选择没有固定的标准，对于上述Multibox SSD网络，作者尝试了8-32K的放大因子，均训练成功了。 只要保证放大后的梯度不超过FP16精度的表示上限（65504），选择较大的放大因子并无副作用。[1]</p></li></ol><h2 id="2-Related-Materials"><a href="#2-Related-Materials" class="headerlink" title="2. Related Materials"></a>2. Related Materials</h2><ol><li>Nvidia Reources: <a href="https://github.com/NvidiaResources/nvidia_mixed_precision_training" target="_blank" rel="noopener">https://github.com/NvidiaResources/nvidia_mixed_precision_training</a></li><li>Nvidia: Mixed-Precision Training of Deep Neural Networks: <a href="https://devblogs.nvidia.com/mixed-precision-training-deep-neural-networks/" target="_blank" rel="noopener">https://devblogs.nvidia.com/mixed-precision-training-deep-neural-networks/</a></li></ol><h2 id="3-Method"><a href="#3-Method" class="headerlink" title="3. Method"></a>3. Method</h2><h3 id="2-1-直接在python2的pytorch训练框架中修改可实现："><a href="#2-1-直接在python2的pytorch训练框架中修改可实现：" class="headerlink" title="2.1 直接在python2的pytorch训练框架中修改可实现："></a>2.1 直接在python2的pytorch训练框架中修改可实现：</h3><p><a href="https://github.com/suvojit-0x55aa/mixed-precision-pytorch" target="_blank" rel="noopener">https://github.com/suvojit-0x55aa/mixed-precision-pytorch</a></p><h3 id="2-2-使用Nvidia-apex库"><a href="#2-2-使用Nvidia-apex库" class="headerlink" title="2.2 使用Nvidia apex库"></a>2.2 使用Nvidia apex库</h3><p>库地址：<a href="https://github.com/NVIDIA/apex" target="_blank" rel="noopener">https://github.com/NVIDIA/apex</a></p><p>Nvidia 官方apex使用教程：<a href="http://on-demand.gputechconf.com/gtc-cn/2018/pdf/CH8302.pdf" target="_blank" rel="noopener">http://on-demand.gputechconf.com/gtc-cn/2018/pdf/CH8302.pdf</a></p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="http://kevinlt.top/2018/09/14/mixed_precision_training/" target="_blank" rel="noopener">混合精度训练</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;由于FP16的精度损失问题，如果我们在神经网络的训练过程中直接将网络参数以FP16的形式进行计算，可能会出现数值不稳定的情况而导致模型性能下降。 对此，百度和NVIDIA的研究院在&lt;a href=&quot;https://arxiv.org/abs/1710.03740&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Mixed Precision Training&lt;/a&gt;这一论文中提出混合精度训练的方法，在充分利用FP16加速运算的优点的同时保证了模型的精度。下面介绍其论文中的主要要点。
    
    </summary>
    
    
    
      <category term="DL" scheme="http://zengzeyu.com/tags/DL/"/>
    
      <category term="Paper" scheme="http://zengzeyu.com/tags/Paper/"/>
    
  </entry>
  
  <entry>
    <title>PCL 点云索引方法K维树（KD-tree）和八叉树（octree）介绍</title>
    <link href="http://zengzeyu.com/2018/03/30/pcl_kdtree_and_octree/"/>
    <id>http://zengzeyu.com/2018/03/30/pcl_kdtree_and_octree/</id>
    <published>2018-03-30T14:55:54.000Z</published>
    <updated>2020-07-15T05:01:39.843Z</updated>
    
    <content type="html"><![CDATA[<p>通过雷达、激光扫描、立体摄像机等三维测量设备获取的点云数据，具有数据量大、分布不均匀等特点。作为三维领域中一个重要的数据来源，点云数据主要是表征目标表面的海量点集合，并不具备传统网格数据的集合拓扑信息。所以点云数据处理中最为核心的问题就是建立离散点间的拓扑关系，实现基于邻域关系的快速查找。<a id="more"></a><br>建立空间索引在点云数据处理中已被广泛应用，常见空间索引一般是自顶向下逐级划分空间的各种空间索引结构，比较有代表性的包括 BSP树、 KD树、 KDB树、 R树、 R+树、 CELL树、四叉树和八叉树等索引结构，而在这些结构中KD树和八叉树在3D点云数据组织中应用较为广泛，PCL对上述两者进行了实现。</p><h2 id="1-K维树（KD-tree）"><a href="#1-K维树（KD-tree）" class="headerlink" title="1.K维树（KD-tree）"></a>1.K维树（KD-tree）</h2><h3 id="1-1-KD-tree-概念简介"><a href="#1-1-KD-tree-概念简介" class="headerlink" title="1.1 KD-tree 概念简介"></a>1.1 KD-tree 概念简介</h3><p>KD-tree 又称 K 维树是计算机科学中使用的一种数据结构，用来组织表示 K 维空间中点集合。它是一种带有其他约束条件的二分查找树。KD-tree对于区间和近邻搜索十分有用。我们为了达到目的，通常只在三个维度中进行处理，因此所有的 KD-tree 都将是三维 KD-tree。 如下图所示（动图，慢慢看）， KD-tree 的每一级在指定维度上分开所有的子节点。在树的根部所有子节点是以第一个指定的维度上被分开（即如果第一维坐标小于根节点的点它将被分在左边的子树中，如果大于根节点的点它将分在右边的子树中）。</p><p><img data-src="/images/18_3_30/pcl_kdtree_and_octree/10028058-4d7b7c39333666b1.gif" alt="KDTree-animation.gif"></p><p>树的每一级都在下一个维度上分开，所有其他的维度用完之后就回到第一个维度，建立 KD-tree 最高效的方法是像快速分类一样使用分割法，把指定维度的值放在根上，在该维度上包含较小数值的在左子树，较大的在右子树。然后分别在左边和右边的子树上重复这个过程，直到用户准备分类的最后一个树仅仅由一个元素组成。</p><p><img data-src="/images/18_3_30/pcl_kdtree_and_octree/10028058-66cd9ff8af223044.png" alt="Screenshot from 2018-03-30 11:51:38.png"></p><h3 id="1-2-PCL中KD-tree使用"><a href="#1-2-PCL中KD-tree使用" class="headerlink" title="1.2 PCL中KD-tree使用"></a>1.2 PCL中KD-tree使用</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;pcl/point_cloud.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;pcl/kdtree/kdtree_flann.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;ctime&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span></span><br><span class="line">main (<span class="keyword">int</span> argc, <span class="keyword">char</span>** argv)</span><br><span class="line">&#123;</span><br><span class="line">  srand (time (<span class="literal">NULL</span>));</span><br><span class="line"></span><br><span class="line">  pcl::PointCloud&lt;pcl::PointXYZ&gt;::<span class="function">Ptr <span class="title">cloud</span> <span class="params">(<span class="keyword">new</span> pcl::PointCloud&lt;pcl::PointXYZ&gt;)</span></span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Generate pointcloud data</span></span><br><span class="line">  cloud-&gt;<span class="built_in">width</span> = <span class="number">1000</span>;</span><br><span class="line">  cloud-&gt;<span class="built_in">height</span> = <span class="number">1</span>;</span><br><span class="line">  cloud-&gt;points.resize (cloud-&gt;<span class="built_in">width</span> * cloud-&gt;<span class="built_in">height</span>);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; cloud-&gt;points.<span class="built_in">size</span> (); ++i)</span><br><span class="line">  &#123;</span><br><span class="line">    cloud-&gt;points[i].x = <span class="number">1024.0f</span> * rand () / (RAND_MAX + <span class="number">1.0f</span>);</span><br><span class="line">    cloud-&gt;points[i].y = <span class="number">1024.0f</span> * rand () / (RAND_MAX + <span class="number">1.0f</span>);</span><br><span class="line">    cloud-&gt;points[i].z = <span class="number">1024.0f</span> * rand () / (RAND_MAX + <span class="number">1.0f</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  pcl::KdTreeFLANN&lt;pcl::PointXYZ&gt; kdtree;</span><br><span class="line"></span><br><span class="line">  kdtree.setInputCloud (cloud);</span><br><span class="line"></span><br><span class="line">  pcl::PointXYZ searchPoint;</span><br><span class="line"></span><br><span class="line">  searchPoint.x = <span class="number">1024.0f</span> * rand () / (RAND_MAX + <span class="number">1.0f</span>);</span><br><span class="line">  searchPoint.y = <span class="number">1024.0f</span> * rand () / (RAND_MAX + <span class="number">1.0f</span>);</span><br><span class="line">  searchPoint.z = <span class="number">1024.0f</span> * rand () / (RAND_MAX + <span class="number">1.0f</span>);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// K nearest neighbor search</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">int</span> K = <span class="number">10</span>;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">pointIdxNKNSearch</span><span class="params">(K)</span></span>;</span><br><span class="line">  <span class="function"><span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">float</span>&gt; <span class="title">pointNKNSquaredDistance</span><span class="params">(K)</span></span>;</span><br><span class="line"></span><br><span class="line">  <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"K nearest neighbor search at ("</span> &lt;&lt; searchPoint.x </span><br><span class="line">            &lt;&lt; <span class="string">" "</span> &lt;&lt; searchPoint.y </span><br><span class="line">            &lt;&lt; <span class="string">" "</span> &lt;&lt; searchPoint.z</span><br><span class="line">            &lt;&lt; <span class="string">") with K="</span> &lt;&lt; K &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> ( kdtree.nearestKSearch (searchPoint, K, pointIdxNKNSearch, pointNKNSquaredDistance) &gt; <span class="number">0</span> )</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; pointIdxNKNSearch.<span class="built_in">size</span> (); ++i)</span><br><span class="line">      <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"    "</span>  &lt;&lt;   cloud-&gt;points[ pointIdxNKNSearch[i] ].x </span><br><span class="line">                &lt;&lt; <span class="string">" "</span> &lt;&lt; cloud-&gt;points[ pointIdxNKNSearch[i] ].y </span><br><span class="line">                &lt;&lt; <span class="string">" "</span> &lt;&lt; cloud-&gt;points[ pointIdxNKNSearch[i] ].z </span><br><span class="line">                &lt;&lt; <span class="string">" (squared distance: "</span> &lt;&lt; pointNKNSquaredDistance[i] &lt;&lt; <span class="string">")"</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Neighbors within radius search</span></span><br><span class="line"></span><br><span class="line">  <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; pointIdxRadiusSearch;</span><br><span class="line">  <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">float</span>&gt; pointRadiusSquaredDistance;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">float</span> radius = <span class="number">256.0f</span> * rand () / (RAND_MAX + <span class="number">1.0f</span>);</span><br><span class="line"></span><br><span class="line">  <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"Neighbors within radius search at ("</span> &lt;&lt; searchPoint.x </span><br><span class="line">            &lt;&lt; <span class="string">" "</span> &lt;&lt; searchPoint.y </span><br><span class="line">            &lt;&lt; <span class="string">" "</span> &lt;&lt; searchPoint.z</span><br><span class="line">            &lt;&lt; <span class="string">") with radius="</span> &lt;&lt; radius &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> ( kdtree.radiusSearch (searchPoint, radius, pointIdxRadiusSearch, pointRadiusSquaredDistance) &gt; <span class="number">0</span> )</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; pointIdxRadiusSearch.<span class="built_in">size</span> (); ++i)</span><br><span class="line">      <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"    "</span>  &lt;&lt;   cloud-&gt;points[ pointIdxRadiusSearch[i] ].x </span><br><span class="line">                &lt;&lt; <span class="string">" "</span> &lt;&lt; cloud-&gt;points[ pointIdxRadiusSearch[i] ].y </span><br><span class="line">                &lt;&lt; <span class="string">" "</span> &lt;&lt; cloud-&gt;points[ pointIdxRadiusSearch[i] ].z </span><br><span class="line">                &lt;&lt; <span class="string">" (squared distance: "</span> &lt;&lt; pointRadiusSquaredDistance[i] &lt;&lt; <span class="string">")"</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="代码解读"><a href="#代码解读" class="headerlink" title="代码解读"></a>代码解读</h4><ul><li>设置kdtree搜索对象和输入数据，然后使用随机坐标搜索方式 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">pcl::KdTreeFLANN&lt;pcl::PointXYZ&gt; kdtree;</span><br><span class="line"></span><br><span class="line"> kdtree.setInputCloud (cloud);</span><br><span class="line"></span><br><span class="line"> pcl::PointXYZ searchPoint;</span><br><span class="line"></span><br><span class="line"> searchPoint.x &#x3D; 1024.0f * rand () &#x2F; (RAND_MAX + 1.0f);</span><br><span class="line"> searchPoint.y &#x3D; 1024.0f * rand () &#x2F; (RAND_MAX + 1.0f);</span><br><span class="line"> searchPoint.z &#x3D; 1024.0f * rand () &#x2F; (RAND_MAX + 1.0f);</span><br></pre></td></tr></table></figure></li><li>设置临近点个数 （10），两个向量来存储搜索到的 K 近邻，两个向量中一个存储搜索到查询点近邻的索引，另一个存储对应近邻的距离平方<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"> &#x2F;&#x2F; K nearest neighbor search</span><br><span class="line"></span><br><span class="line">int K &#x3D; 10;</span><br><span class="line"></span><br><span class="line">std::vector&lt;int&gt; pointIdxNKNSearch(K);</span><br><span class="line">std::vector&lt;float&gt; pointNKNSquaredDistance(K);</span><br><span class="line"></span><br><span class="line">std::cout &lt;&lt; &quot;K nearest neighbor search at (&quot; &lt;&lt; searchPoint.x </span><br><span class="line">          &lt;&lt; &quot; &quot; &lt;&lt; searchPoint.y </span><br><span class="line">          &lt;&lt; &quot; &quot; &lt;&lt; searchPoint.z</span><br><span class="line">          &lt;&lt; &quot;) with K&#x3D;&quot; &lt;&lt; K &lt;&lt; std::endl;</span><br></pre></td></tr></table></figure></li></ul><h3 id="1-3-KD-tree-算法伪代码"><a href="#1-3-KD-tree-算法伪代码" class="headerlink" title="1.3 KD-tree 算法伪代码"></a>1.3 KD-tree 算法伪代码</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">function kdtree (list of points pointList, int depth)</span><br><span class="line">&#123;</span><br><span class="line">    &#x2F;&#x2F; Select axis based on depth so that axis cycles through all valid values</span><br><span class="line">    var int axis :&#x3D; depth mod k;</span><br><span class="line">        </span><br><span class="line">    &#x2F;&#x2F; Sort point list and choose median as pivot element</span><br><span class="line">    select median by axis from pointList;</span><br><span class="line">        </span><br><span class="line">    &#x2F;&#x2F; Create node and construct subtree</span><br><span class="line">    node.location :&#x3D; median;</span><br><span class="line">    node.leftChild :&#x3D; kdtree(points in pointList before median, depth+1);</span><br><span class="line">    node.rightChild :&#x3D; kdtree(points in pointList after median, depth+1);</span><br><span class="line">    return node;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="2-八叉树（octree）"><a href="#2-八叉树（octree）" class="headerlink" title="2. 八叉树（octree）"></a>2. 八叉树（octree）</h2><h3 id="2-1-octree-概念简介"><a href="#2-1-octree-概念简介" class="headerlink" title="2.1 octree 概念简介"></a>2.1 octree 概念简介</h3><p>八叉树结构是由 Hunter 博士于1978年首次提出的一种数据模型。八叉树结构通过对三维空间的几何实体进行体元剖分，每个体元具有相同的时间和空间复杂度，通过循环递归的划分方法对大小为( 2 <em>n</em> x 2 <em>n</em> x 2 <em>n</em> ) 的三维空间的几何对象进行剖分，从而构成一个具有根节点的方向图。在八叉树结构中如果被划分的体元具有相同的属性，则该体元构成一个叶节点；否则继续对该体元剖分成8个子立方体，依次递剖分，对于( 2 <em>n</em> x 2 <em>n</em> x 2 <em>n</em> ) 大小的空间对象，最多剖分 <em>n</em> 次，如下图所示。</p><p><img data-src="/images/18_3_30/pcl_kdtree_and_octree/10028058-c98fc8b763769c76.png" alt="Octree2.png"></p><h3 id="2-2-PCL中octree-在压缩点云数据方面应用"><a href="#2-2-PCL中octree-在压缩点云数据方面应用" class="headerlink" title="2.2 PCL中octree 在压缩点云数据方面应用"></a>2.2 PCL中octree 在压缩点云数据方面应用</h3><p>点云由海量的数据集组成，这些数据通过距离、颜色、法线等附加信息来描述空间三维点。此外，点云能以非常高的速率被创建出来，因此需要占用相当大的存储资源，一旦点云需要存储或者通过速率受限制的通信信道进行传输，提供针对这种数据的压缩方法就变得十分有用。PCL库提供了点云压缩功能，它允许编码压缩所有类型的点云，包括无序点云，它具有无参考点和变化的点尺寸、分辨率、分布密度和点顺序等结构特征。而且，底层的 octree 数据结构允许从几个输入源高效地合并点云数据。<br>下面解释单个点云和点云数据流是如何高效压缩的，在给出的例子中用PCL点云压缩技术来压缩用 OpenNIGrabber 抓取到的点云。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;pcl&#x2F;point_cloud.h&gt;</span><br><span class="line">#include &lt;pcl&#x2F;point_types.h&gt;</span><br><span class="line">#include &lt;pcl&#x2F;io&#x2F;openni_grabber.h&gt;</span><br><span class="line">#include &lt;pcl&#x2F;visualization&#x2F;cloud_viewer.h&gt;</span><br><span class="line"></span><br><span class="line">#include &lt;pcl&#x2F;compression&#x2F;octree_pointcloud_compression.h&gt;</span><br><span class="line"></span><br><span class="line">#include &lt;stdio.h&gt;</span><br><span class="line">#include &lt;sstream&gt;</span><br><span class="line">#include &lt;stdlib.h&gt;</span><br><span class="line"></span><br><span class="line">#ifdef WIN32</span><br><span class="line"># define sleep(x) Sleep((x)*1000)</span><br><span class="line">#endif</span><br><span class="line"></span><br><span class="line">class SimpleOpenNIViewer</span><br><span class="line">&#123;</span><br><span class="line">public:</span><br><span class="line">  SimpleOpenNIViewer () :</span><br><span class="line">    viewer (&quot; Point Cloud Compression Example&quot;)</span><br><span class="line">  &#123;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  void</span><br><span class="line">  cloud_cb_ (const pcl::PointCloud&lt;pcl::PointXYZRGBA&gt;::ConstPtr &amp;cloud)</span><br><span class="line">  &#123;</span><br><span class="line">    if (!viewer.wasStopped ())</span><br><span class="line">    &#123;</span><br><span class="line">      &#x2F;&#x2F; stringstream to store compressed point cloud</span><br><span class="line">      std::stringstream compressedData;</span><br><span class="line">      &#x2F;&#x2F; output pointcloud</span><br><span class="line">      pcl::PointCloud&lt;pcl::PointXYZRGBA&gt;::Ptr cloudOut (new pcl::PointCloud&lt;pcl::PointXYZRGBA&gt; ());</span><br><span class="line">    </span><br><span class="line">      &#x2F;&#x2F; compress point cloud</span><br><span class="line">      PointCloudEncoder-&gt;encodePointCloud (cloud, compressedData);</span><br><span class="line">    </span><br><span class="line">      &#x2F;&#x2F; decompress point cloud</span><br><span class="line">      PointCloudDecoder-&gt;decodePointCloud (compressedData, cloudOut);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">      &#x2F;&#x2F; show decompressed point cloud</span><br><span class="line">      viewer.showCloud (cloudOut);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  void</span><br><span class="line">  run ()</span><br><span class="line">  &#123;</span><br><span class="line"></span><br><span class="line">    bool showStatistics &#x3D; true;</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F; for a full list of profiles see: &#x2F;io&#x2F;include&#x2F;pcl&#x2F;compression&#x2F;compression_profiles.h</span><br><span class="line">    pcl::io::compression_Profiles_e compressionProfile &#x3D; pcl::io::MED_RES_ONLINE_COMPRESSION_WITH_COLOR;</span><br><span class="line">    </span><br><span class="line">    &#x2F;&#x2F; instantiate point cloud compression for encoding and decoding</span><br><span class="line">    PointCloudEncoder &#x3D; new pcl::io::OctreePointCloudCompression&lt;pcl::PointXYZRGBA&gt; (compressionProfile, showStatistics);</span><br><span class="line">    PointCloudDecoder &#x3D; new pcl::io::OctreePointCloudCompression&lt;pcl::PointXYZRGBA&gt; ();</span><br><span class="line">    </span><br><span class="line">    &#x2F;&#x2F; create a new grabber for OpenNI devices</span><br><span class="line">    pcl::Grabber* interface &#x3D; new pcl::OpenNIGrabber ();</span><br><span class="line">    </span><br><span class="line">    &#x2F;&#x2F; make callback function from member function</span><br><span class="line">    boost::function&lt;void</span><br><span class="line">    (const pcl::PointCloud&lt;pcl::PointXYZRGBA&gt;::ConstPtr&amp;)&gt; f &#x3D; boost::bind (&amp;SimpleOpenNIViewer::cloud_cb_, this, _1);</span><br><span class="line">    </span><br><span class="line">    &#x2F;&#x2F; connect callback function for desired signal. In this case its a point cloud with color values</span><br><span class="line">    boost::signals2::connection c &#x3D; interface-&gt;registerCallback (f);</span><br><span class="line">    </span><br><span class="line">    &#x2F;&#x2F; start receiving point clouds</span><br><span class="line">    interface-&gt;start ();</span><br><span class="line">    </span><br><span class="line">    while (!viewer.wasStopped ())</span><br><span class="line">    &#123;</span><br><span class="line">      sleep (1);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    interface-&gt;stop ();</span><br><span class="line">    </span><br><span class="line">    &#x2F;&#x2F; delete point cloud compression instances</span><br><span class="line">    delete (PointCloudEncoder);</span><br><span class="line">    delete (PointCloudDecoder);</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  pcl::visualization::CloudViewer viewer;</span><br><span class="line"></span><br><span class="line">  pcl::io::OctreePointCloudCompression&lt;pcl::PointXYZRGBA&gt;* PointCloudEncoder;</span><br><span class="line">  pcl::io::OctreePointCloudCompression&lt;pcl::PointXYZRGBA&gt;* PointCloudDecoder;</span><br><span class="line"></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">int</span><br><span class="line">main (int argc, char **argv)</span><br><span class="line">&#123;</span><br><span class="line">  SimpleOpenNIViewer v;</span><br><span class="line">  v.run ();</span><br><span class="line"></span><br><span class="line">  return (0);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-3-octree算法-Matlab-伪代码"><a href="#2-3-octree算法-Matlab-伪代码" class="headerlink" title="2.3 octree算法 Matlab 伪代码"></a>2.3 octree算法 Matlab 伪代码</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">function [binDepths,binParents,binCorners,pointBins] &#x3D; OcTree(points)</span><br><span class="line"></span><br><span class="line">binDepths &#x3D; [0]     % Initialize an array of bin depths with this single base-level bin</span><br><span class="line">binParents &#x3D; [0]    % This base level bin is not a child of other bins</span><br><span class="line">binCorners &#x3D; [min(points) max(points)] % It surrounds all points in XYZ space</span><br><span class="line">pointBins(:) &#x3D; 1    % Initially, all points are assigned to this first bin</span><br><span class="line">divide(1)           % Begin dividing this first bin</span><br><span class="line"></span><br><span class="line">function divide(binNo)</span><br><span class="line">​    </span><br><span class="line">% If this bin meets any exit conditions, do not divide it any further.</span><br><span class="line">binPointCount &#x3D; nnz(pointBins&#x3D;&#x3D;binNo)</span><br><span class="line">binEdgeLengths &#x3D; binCorners(binNo,1:3) - binCorners(binNo,4:6)</span><br><span class="line">binDepth &#x3D; binDepths(binNo)</span><br><span class="line">exitConditionsMet &#x3D; binPointCount&lt;value || min(binEdgeLengths)&lt;value || binDepth&gt;value</span><br><span class="line">if exitConditionsMet</span><br><span class="line">    return; % Exit recursive function</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">% Otherwise, split this bin into 8 new sub-bins with a new division point</span><br><span class="line">newDiv &#x3D; (binCorners(binNo,1:3) + binCorners(binNo,4:6)) &#x2F; 2</span><br><span class="line">for i &#x3D; 1:8</span><br><span class="line">    newBinNo &#x3D; length(binDepths) + 1</span><br><span class="line">    binDepths(newBinNo) &#x3D; binDepths(binNo) + 1</span><br><span class="line">    binParents(newBinNo) &#x3D; binNo</span><br><span class="line">    binCorners(newBinNo) &#x3D; [one of the 8 pairs of the newDiv with minCorner or maxCorner]</span><br><span class="line">    oldBinMask &#x3D; pointBins&#x3D;&#x3D;binNo</span><br><span class="line">    % Calculate which points in pointBins&#x3D;&#x3D;binNo now belong in newBinNo </span><br><span class="line">    pointBins(newBinMask) &#x3D; newBinNo</span><br><span class="line">    % Recursively divide this newly created bin</span><br><span class="line">    divide(newBinNo)</span><br><span class="line">end</span><br></pre></td></tr></table></figure><p>以上。</p><hr><p>参考文献：</p><ol><li><a href="https://book.douban.com/subject/20283456/" target="_blank" rel="noopener">点云库PCL学习教程</a></li><li><a href="https://en.wikipedia.org/wiki/K-d_tree" target="_blank" rel="noopener">k-d tree wiki</a></li><li><a href="http://pointclouds.org/documentation/tutorials/kdtree_search.php#kdtree-search" target="_blank" rel="noopener">PCL官方KD-tree使用教程</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;通过雷达、激光扫描、立体摄像机等三维测量设备获取的点云数据，具有数据量大、分布不均匀等特点。作为三维领域中一个重要的数据来源，点云数据主要是表征目标表面的海量点集合，并不具备传统网格数据的集合拓扑信息。所以点云数据处理中最为核心的问题就是建立离散点间的拓扑关系，实现基于邻域关系的快速查找。
    
    </summary>
    
    
      <category term="PCL" scheme="http://zengzeyu.com/categories/PCL/"/>
    
    
      <category term="PCL" scheme="http://zengzeyu.com/tags/PCL/"/>
    
  </entry>
  
  <entry>
    <title>激光雷达点云数据内部空点补全</title>
    <link href="http://zengzeyu.com/2018/03/28/fix_nan_point_in_point_cloud/"/>
    <id>http://zengzeyu.com/2018/03/28/fix_nan_point_in_point_cloud/</id>
    <published>2018-03-28T14:55:54.000Z</published>
    <updated>2020-07-15T04:46:57.279Z</updated>
    
    <content type="html"><![CDATA[<p>点云数据区别于图像数据，不管是二维图像还是三维图像，图像数据都充满整个区域，二维图像中每个像素点都有值，灰度值、RGB值等；三维图像中有体数据（Voxel），根据光线投影算法等，可计算出每个体数据对应值，从而显示于显示器中。点云数据由于其扫描生成数据过程的特性，就决定了其在数据方面与图像数据不同，以机械式激光雷达为例，当出现以下情况时，该位置扫描生成的点云数据不存在（即为NAN点）<a id="more"></a>：</p><ul><li>激光发射器发射出去的激光未收到返回光束</li><li>激光接收器接收到的返回激光强度超出阈值范围</li></ul><p>另外，在数据处理阶段，可根据需要对部分数据进行滤波处理，赋值为NAN点，也能造成该出点云缺失情况。</p><p>PCL库中有自带判断点云数据是否含有NAN点的函数： <code>pcl::PointCloud&lt;pcl::PointXYZ&gt;::is_dense()</code>， 以及过滤NAN点函数：<code>pcl::removeNaNFromPointCloud()</code>。</p><p><strong>本文旨在补全内部空洞点，而不是去掉空洞点。</strong></p><h2 id="点云NAN点补全"><a href="#点云NAN点补全" class="headerlink" title="点云NAN点补全"></a>点云NAN点补全</h2><hr><p>本文补全原则基于有序点云（organised）进行处理，非有序点云无法进行处理（unorganised）。<br>以自动驾驶中使用的机械式激光雷达<a href="http://www.robosense.cn/rslidar/RS-LiDAR-16" target="_blank" rel="noopener">速腾聚创16线激光雷达RS-LiDAR-16</a>为例，其生成的有序点云（organised）点云尺寸为 16 x 2016： 16为激光线数，2016为每一线激光绕中心一周旋转储存的点个数，因此有 16 x 2016 = 32256 个点，而实际得到的点数据基本不可能是32256，必有缺失。</p><h3 id="补全规则"><a href="#补全规则" class="headerlink" title="补全规则"></a>补全规则</h3><p><strong>在每一线激光扫描得到一行点数据中，查找与NAN点最近的点进行补全，如果本行数据全部为NAN（虽然不可能发生），则此行可删除，调整点云尺寸。</strong><br><strong>该规则基于的原则：在同一线激光扫描得到的点中，由于水平方向数据分辨率很高，所以一行数据中每个点与其邻域内点相似。</strong></p><h3 id="算法设计"><a href="#算法设计" class="headerlink" title="算法设计"></a>算法设计</h3><h5 id="算法思路"><a href="#算法思路" class="headerlink" title="算法思路"></a>算法思路</h5><p>为了简化叙述，本文将一线激光扫描得到数据缩小为 360 个点，即一帧点云尺寸变为 16x360。<br>以一线激光扫描数据为例，默认激光旋转方向为顺时针方向，采用线性差值方法进行补全，由于一线激光扫描一圈得到的数据在360°内任意位置都是对偶的，所以在空点附近查找两边非空点，用其值进行补全，具体参考下图示意。<br>图上半部分为一线激光雷达扫描得到数据鸟瞰图，其中黑色方块代表非空点，白色方块代表空点；下半部分为点距离图，根据线性插值方法可以补全非空点。非空点距离计算采用极坐标方式，首先得到线性插值得到的range，再使用当前转角转换到笛卡尔坐标系下，可得到其 x， y 坐标值，z 坐标值也采用同样的插值方法计算。</p><p><img data-src="/images/18_3_30/fix_nan_point_in_point_cloud/10028058-3941ede1118e84a7.png" alt="Screenshot from 2018-03-29 09:42:08.png"></p><p>其中一个特殊情况：转角为 0° 与转角为 360° 是等效的，在查找过程中，当转角顺时针查找到 360 °时则置为 0°，当转角逆时针查找到 0° 时则置为 360°。</p><h5 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h5><p>根据以上思路，设计算法如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line">def fix_nan_point(self, in_cloud):</span><br><span class="line">    #fix edeg nan point 1st</span><br><span class="line">    in_cloud &#x3D; self.fix_left_edge_nan_point( in_cloud )</span><br><span class="line">    in_cloud &#x3D; self.fix_right_edge_nan_point( in_cloud )</span><br><span class="line">    #fix centrol nan point</span><br><span class="line">    for i in range(in_cloud.shape[0]):</span><br><span class="line">        for j in range(1, in_cloud.shape[1]):</span><br><span class="line">            if in_cloud[i, j, -1] &#x3D;&#x3D; -1:</span><br><span class="line">                nan_size &#x3D; 1</span><br><span class="line">                left &#x3D; j - 1</span><br><span class="line">                right &#x3D; j + 1</span><br><span class="line">                while in_cloud[i, left, -1] &#x3D;&#x3D; -1:</span><br><span class="line">                    left -&#x3D; 1</span><br><span class="line">                    nan_size +&#x3D; 1</span><br><span class="line">                while in_cloud[i, right, -1] &#x3D;&#x3D; -1:</span><br><span class="line">                    right +&#x3D; 1</span><br><span class="line">                    nan_size +&#x3D; 1</span><br><span class="line"></span><br><span class="line">                height_diff_cell &#x3D; (in_cloud[i, right, 2] - in_cloud[i, left, 2]) &#x2F; nan_size</span><br><span class="line">                range_diff_cell &#x3D; (in_cloud[i, right, 3] - in_cloud[i, left, 3]) &#x2F; nan_size</span><br><span class="line">                in_cloud[i, j, 2] &#x3D; in_cloud[i, left, 2] + (j - left) * height_diff_cell</span><br><span class="line">                in_cloud[i, j, 3] &#x3D; in_cloud[i, left, 3] + (j - left) * range_diff_cell</span><br><span class="line">                if abs(j - left) &lt; abs(right-j):</span><br><span class="line">                    in_cloud[i, j, -1] &#x3D; in_cloud[i, left, -1]</span><br><span class="line">                else:</span><br><span class="line">                    in_cloud[i, j, -1] &#x3D; in_cloud[i, right, -1]</span><br><span class="line">    return in_cloud</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def fix_left_edge_nan_point(self, in_cloud):</span><br><span class="line">    for i in range(in_cloud.shape[0]):</span><br><span class="line">        if in_cloud[i, 0, -1] &#x3D;&#x3D; -1:</span><br><span class="line">            nan_size &#x3D; 1</span><br><span class="line">            left &#x3D; 359</span><br><span class="line">            right &#x3D; 1</span><br><span class="line">            while in_cloud[i,left,-1] &#x3D;&#x3D; -1:</span><br><span class="line">                left -&#x3D; 1</span><br><span class="line">                nan_size +&#x3D; 1</span><br><span class="line">            while in_cloud[i,right,-1] &#x3D;&#x3D; -1:</span><br><span class="line">                right +&#x3D; 1</span><br><span class="line">                nan_size +&#x3D;1</span><br><span class="line"></span><br><span class="line">            height_diff_cell &#x3D; (in_cloud[i, right, 2] - in_cloud[i, left, 2]) &#x2F; nan_size</span><br><span class="line">            range_diff_cell &#x3D; (in_cloud[i, right, 3] - in_cloud[i, left, 3]) &#x2F; nan_size</span><br><span class="line">            in_cloud[i, 0, 2] &#x3D; in_cloud[i, left, 2] + (360 - left) * height_diff_cell</span><br><span class="line">            in_cloud[i, 0, 3] &#x3D; in_cloud[i, left, 3] + (360 - left) * range_diff_cell</span><br><span class="line">            if abs(360 - left) &lt; right:</span><br><span class="line">                in_cloud[i, 0, -1] &#x3D; in_cloud[i, left, -1]</span><br><span class="line">            else:</span><br><span class="line">                in_cloud[i, 0, -1] &#x3D; in_cloud[i, right, -1]</span><br><span class="line">    return in_cloud</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def fix_right_edge_nan_point(self, in_cloud):</span><br><span class="line">    for i in range(in_cloud.shape[0]):</span><br><span class="line">        if in_cloud[i, in_cloud.shape[1]-1, -1] &#x3D;&#x3D; -1:</span><br><span class="line">            nan_size &#x3D; 1</span><br><span class="line">            left &#x3D; in_cloud.shape[1]-2</span><br><span class="line">            right &#x3D; 0</span><br><span class="line">            while in_cloud[i,left,-1] &#x3D;&#x3D; -1:</span><br><span class="line">                left -&#x3D; 1</span><br><span class="line">                nan_size +&#x3D; 1</span><br><span class="line">            while in_cloud[i,right,-1] &#x3D;&#x3D; -1:</span><br><span class="line">                right +&#x3D; 1</span><br><span class="line">                nan_size +&#x3D;1</span><br><span class="line"></span><br><span class="line">            height_diff_cell &#x3D; (in_cloud[i, right, 2] - in_cloud[i, left, 2]) &#x2F; nan_size</span><br><span class="line">            range_diff_cell &#x3D; (in_cloud[i, right, 3] - in_cloud[i, left, 3]) &#x2F; nan_size</span><br><span class="line">            in_cloud[i, in_cloud.shape[1]-1, 2] &#x3D; in_cloud[i, left, 2] + (in_cloud.shape[1]-1 - left) * height_diff_cell</span><br><span class="line">            in_cloud[i, in_cloud.shape[1]-1, 3] &#x3D; in_cloud[i, left, 3] + (in_cloud.shape[1]-1 - left) * range_diff_cell</span><br><span class="line">            if abs(in_cloud.shape[1]-1 - left) &lt; right + 1:</span><br><span class="line">                in_cloud[i, in_cloud.shape[1]-1, -1] &#x3D; in_cloud[i, left, -1]</span><br><span class="line">            else:</span><br><span class="line">                in_cloud[i, in_cloud.shape[1]-1, -1] &#x3D; in_cloud[i, right, -1]</span><br><span class="line">    return in_cloud</span><br></pre></td></tr></table></figure><h5 id="算法结果"><a href="#算法结果" class="headerlink" title="算法结果"></a>算法结果</h5><p>下图结果为源数据先经过降采样之后，再进行补全NAN点操作。源数据中的 <strong><em>label</em></strong> 有三个值 [-1, 0, 1]， 经过降采样然后补全操作只剩下 <strong><em>label</em></strong> 为[0， 1] 的点。</p><p><img data-src="/images/18_3_30/fix_nan_point_in_point_cloud/20180329160618551.png" alt="这里写图片描述"></p><p>以上。</p><hr><p>参考文献：</p><ol><li><a href="https://www.cnblogs.com/li-yao7758258/p/6519830.html" target="_blank" rel="noopener">PCL点云变换与移除NaN</a></li><li><a href="http://www.robosense.cn/" target="_blank" rel="noopener">速腾聚创自动驾驶激光雷达</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;点云数据区别于图像数据，不管是二维图像还是三维图像，图像数据都充满整个区域，二维图像中每个像素点都有值，灰度值、RGB值等；三维图像中有体数据（Voxel），根据光线投影算法等，可计算出每个体数据对应值，从而显示于显示器中。点云数据由于其扫描生成数据过程的特性，就决定了其在数据方面与图像数据不同，以机械式激光雷达为例，当出现以下情况时，该位置扫描生成的点云数据不存在（即为NAN点）
    
    </summary>
    
    
      <category term="ROS" scheme="http://zengzeyu.com/categories/ROS/"/>
    
    
      <category term="ROS" scheme="http://zengzeyu.com/tags/ROS/"/>
    
      <category term="PCL" scheme="http://zengzeyu.com/tags/PCL/"/>
    
  </entry>
  
  <entry>
    <title>KITTI 原始点云数据（PCL）地面点分割</title>
    <link href="http://zengzeyu.com/2018/03/28/kitti_ground_point_seg/"/>
    <id>http://zengzeyu.com/2018/03/28/kitti_ground_point_seg/</id>
    <published>2018-03-28T14:55:54.000Z</published>
    <updated>2020-07-15T05:03:05.753Z</updated>
    
    <content type="html"><![CDATA[<p>自动驾驶系统中，对激光雷达获取的点云数据进行地面点分割是第一步，地面点分割结果的好坏直接影响聚类，识别和追踪效果。在对地面点分割过程中，前人尝试了许多方法，部分方法结果请参考本人博文<strong>《基于几何特征的地面点云分割》</strong>。所以，地面点分割是自动驾驶激光雷达点云处理永恒的话题。目前，基于几何特征的地面点分割都基于各自的前提假设，大多数的原理是根据地面点与非地面点的特征不同而进行区分，如法向量、高度、高度差等。<a id="more"></a></p><p><img data-src="/images/18_3_30/kitti_ground_point_seg/10028058-f14bf66059d90048.gif" alt="点运数据"></p><p>本文利用现有的<a href="http://www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_completion" target="_blank" rel="noopener">KITTI点云数据</a>进行地面点分割，通过不同方法组合，试验出结果相对较好的方法，并予以结果显示。</p><h2 id="地面点分割"><a href="#地面点分割" class="headerlink" title="地面点分割"></a>地面点分割</h2><h3 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h3><p>本文相关的一些工作，请参考本博客前博文：</p><pre><code>1.  《KITTI 原始bin数据转pcd数据》2.  《KITTI 无序点云数据转有序点云数据》3.  《基于几何特征的地面点云分割》</code></pre><h3 id="地面点分割流程"><a href="#地面点分割流程" class="headerlink" title="地面点分割流程"></a>地面点分割流程</h3><h4 id="算法流程："><a href="#算法流程：" class="headerlink" title="算法流程："></a>算法流程：</h4><pre><code>1. 法向量分割2. 平均高度分割3. 校准平面4. 栅格内高度差分割5. 平均高度分割</code></pre><h5 id="部分代码："><a href="#部分代码：" class="headerlink" title="部分代码："></a>部分代码：</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">void KittiPCL::generateGroundCloud(PointCloudXYZI::Ptr &amp;out_cloud, visualization_msgs::MarkerPtr &amp;plane_marker)</span><br><span class="line">&#123;</span><br><span class="line">    PointCloudXYZI::Ptr temp_cloud_1 ( new PointCloudXYZI );</span><br><span class="line">    PointCloudXYZI::Ptr temp_cloud_2 ( new PointCloudXYZI );</span><br><span class="line">    *temp_cloud_1 &#x3D; *kitti_organised_cloud_ptr_;</span><br><span class="line">    &#x2F;&#x2F; calibration</span><br><span class="line">    this-&gt;filtCloudWithNormalZ( temp_cloud_1, temp_cloud_2 );</span><br><span class="line">    this-&gt;filtWithAverageHeight( temp_cloud_2, kitti_organised_cloud_ptr_, temp_cloud_1 );</span><br><span class="line">    this-&gt;estimateGroundPlane( temp_cloud_1, transform_cloud_ptr_, plane_marker );</span><br><span class="line">    &#x2F;&#x2F; grid</span><br><span class="line">    this-&gt;generateGridMap( transform_cloud_ptr_, temp_cloud_2, 75.0f, 0.2f );</span><br><span class="line">    this-&gt;computePtsCloudFeature(transform_cloud_ptr_);</span><br><span class="line">    *out_cloud &#x3D; *temp_cloud_2;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="试验结果"><a href="#试验结果" class="headerlink" title="试验结果"></a>试验结果</h2><hr><p>为了客观合理展示试验结果，S选取了直道路和十字路口的点云进行分割结果展示。<br>同时，统计时间，在 <em>Release</em> 模式下，处理一帧平均耗时 <strong>130ms</strong>。 </p><blockquote><p>平台配置：<br>CPU: Intel® Xeon(R) CPU E3-1230 v3 @ 3.30GHz × 8<br>GPU: GeForce GT 730/PCIe/SSE2<br>System: 64-bit</p></blockquote><p><img data-src="/images/18_3_30/kitti_ground_point_seg/10028058-1616a85fc81e6b83.png" alt="Screenshot from 2018-03-28 16:02:53.png"></p><p><img data-src="/images/18_3_30/kitti_ground_point_seg/10028058-c02548c179e73e41.png" alt="Screenshot from 2018-03-28 16:04:27.png"></p><p><img data-src="/images/18_3_30/kitti_ground_point_seg/10028058-5388648e2db90935.png" alt="Screenshot from 2018-03-28 16:05:02.png"></p><p><img data-src="/images/18_3_30/kitti_ground_point_seg/10028058-a7d779ef26a1b105.png" alt="Screenshot from 2018-03-28 16:05:52.png"></p><p><img data-src="/images/18_3_30/kitti_ground_point_seg/10028058-53efb931098d3835.png" alt="Screenshot from 2018-03-28 16:08:50.png"></p><p><strong>最后一张图片右上方，可发现有大块地面点未被分割出来，原因是该十字路口地面不平，本文算法还无法适用于这种情况，后续需要改进。</strong></p><p>以上。</p><hr>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;自动驾驶系统中，对激光雷达获取的点云数据进行地面点分割是第一步，地面点分割结果的好坏直接影响聚类，识别和追踪效果。在对地面点分割过程中，前人尝试了许多方法，部分方法结果请参考本人博文&lt;strong&gt;《基于几何特征的地面点云分割》&lt;/strong&gt;。所以，地面点分割是自动驾驶激光雷达点云处理永恒的话题。目前，基于几何特征的地面点分割都基于各自的前提假设，大多数的原理是根据地面点与非地面点的特征不同而进行区分，如法向量、高度、高度差等。
    
    </summary>
    
    
      <category term="ROS" scheme="http://zengzeyu.com/categories/ROS/"/>
    
    
      <category term="ROS" scheme="http://zengzeyu.com/tags/ROS/"/>
    
      <category term="PCL" scheme="http://zengzeyu.com/tags/PCL/"/>
    
  </entry>
  
  <entry>
    <title>CNN for Very Fast Ground Segmentation in Velodyne LiDAR Data</title>
    <link href="http://zengzeyu.com/2018/03/24/CNN%20for%20Very%20Fast%20Ground%20Segmentation%20in%20Velodyne%20LiDAR%20Data/"/>
    <id>http://zengzeyu.com/2018/03/24/CNN%20for%20Very%20Fast%20Ground%20Segmentation%20in%20Velodyne%20LiDAR%20Data/</id>
    <published>2018-03-24T14:55:54.000Z</published>
    <updated>2020-07-15T04:48:04.056Z</updated>
    
    <content type="html"><![CDATA[<p>本文提出了一种新型的去地面点云方法。一种对3D点云数据编码来给CNN进行训练，最后来分割地面点云的方法。<a id="more"></a></p><h1 id="地面点分割方法"><a href="#地面点分割方法" class="headerlink" title="地面点分割方法"></a>地面点分割方法</h1><hr><h2 id="训练数据说明"><a href="#训练数据说明" class="headerlink" title="训练数据说明"></a>训练数据说明</h2><hr><p>首先说明，根据Velodyne HDL-64E 生成的KITTI原始点云数据分析得知，每一帧点云尺寸大概为 <strong>64x4500</strong>，本文每一帧数据为 <strong>64x360</strong> ，所以要对原始数据进行降采样。在每一帧点云中，每一线激光绕中心旋转一圈得到的点云按照 <strong>1°</strong> 的归类分为 <strong>360</strong> 份，每一份点云的信息提取某一个点或者平均信息作为点代表，代表点的特征和 <strong><em>label</em></strong> 填入格子中生成CNN所需训练数据。每个点 <strong><em>label</em></strong> 进行二分类，分为地面点和分地面点。点特征包括 <strong><em>P = [Px, Py, Pz, Pi, Pr]</em></strong> (<strong><em>[ 坐标x， 坐标y， 坐标z， 反射强度intensity， 距离range ]</em></strong>)。</p><h2 id="A-数据准备（Encoding-Sparse-3D-Data-Into-a-Dense-2D-Matrix）"><a href="#A-数据准备（Encoding-Sparse-3D-Data-Into-a-Dense-2D-Matrix）" class="headerlink" title="A. 数据准备（Encoding Sparse 3D Data Into a Dense 2D Matrix）"></a>A. 数据准备（Encoding Sparse 3D Data Into a Dense 2D Matrix）</h2><hr><p>为了将稀疏的3D点云数据应用的2D的CNN中，本文将其编码为2D的多信号通道数据储存在矩阵 <strong><em>M</em></strong> 中，如下图所示。</p><p>![image.png](/images/18_3_24/CNN for Very Fast Ground Segmentation in Velodyne LiDAR Data/10028058-9f36bdbffa079ed2.png)</p><p>矩阵M尺寸为 <strong><em>64x360</em></strong> ，降采样过程中，对一个格子内多个点进行平均取值作为代表。同时为了简化数据，*<strong>[x,z]*** 计算得到的值代表距离，因为本文默认 *</strong>Y*** 轴为高度方向，所以 <strong><em>x， z</em></strong> 值为对偶，可以采取此种方式进行简化数据。对于空格子，则从临近格子进行线性插值来生成该格子内值。</p><p>![image.png](/images/18_3_24/CNN for Very Fast Ground Segmentation in Velodyne LiDAR Data/10028058-2733dbe408f56512.png)</p><h2 id="B-训练数据集（Training-Dataset）"><a href="#B-训练数据集（Training-Dataset）" class="headerlink" title="B. 训练数据集（Training Dataset）"></a>B. 训练数据集（Training Dataset）</h2><hr><p>训练数据集的重要性不容多说，本文自行开发了基于人工种子点选取的点云分割工具（semiautomatic tool for ground annotation，原理参考图像中的<a href="https://blog.csdn.net/shenziheng1/article/details/50878911" target="_blank" rel="noopener">区域增长算法</a>，只不过此处将点之间距离作为判断条件代替灰度值，同时发现当上下限为*<strong>[0.03, 0.07]米**<em>时分割效果最好。选取了KITTI不同场景下共252帧点云作为人工分割数据，将分割好的数据按照</em></strong>7:3*<strong>比例分为**<em>[训练集，评价集]</em></strong>。<br>由于上面得到的数据量太少，所以本文又通过其他一些方法对剩下的19k帧数据，生成了训练所需数据集，基与点云特征有：最低高度，高度变化值，两线激光点云之间的距离和高度差。本文也尝试过自动生成数据（artificial 3D LiDAR data），但是效果较差。</p><h2 id="C-网络结构以及训练方法（Topology-and-Training-of-the-Proposed-Networks）"><a href="#C-网络结构以及训练方法（Topology-and-Training-of-the-Proposed-Networks）" class="headerlink" title="C. 网络结构以及训练方法（Topology and Training of the Proposed Networks）"></a>C. 网络结构以及训练方法（Topology and Training of the Proposed Networks）</h2><hr><p>因为生成的训练数据较少，所以只采用浅层的CNN网络结构（shallow CNN architectures），类型为全卷积（fully convolutional）。卷积层和反卷积层都包含非线性的ReLU神经元（ReLU non-linearities），采用梯度下降方法进行训练。网络结构如下图所示：</p><p>![image.png](/images/18_3_24/CNN for Very Fast Ground Segmentation in Velodyne LiDAR Data/10028058-816d4607cb5d3875.png)</p><p>上文 <strong>A.</strong> 中得到的矩阵 <strong><em>M</em></strong> 作为网络输入，因为是逐点（pixel）进行分类，所以网络的输出尺寸与输入尺寸相同，根据分类： <strong><em>ground = 1</em></strong>，其余点根据softmax函数概率映射进行输出。反卷积层（Deconvolutional<br>layers，广泛应用于语义分割（semantic segmentation）领域）在本文提出的4个网络结构中的中3个都有应用，其中包括效果最好的 <strong><em>L05+deconv</em></strong> （上图中第一个）。</p><p>CNN的输入数据先要进行归一化（normalize）和剪裁（rescale），高度方面KITTI数据集将 <strong>3m</strong> 以上的数据进行了滤波处理，深度 <strong><em>d</em></strong> 通道方面则使用 <strong><em>log</em></strong> 进行归一化处理。</p><p>![image.png](/images/18_3_24/CNN for Very Fast Ground Segmentation in Velodyne LiDAR Data/10028058-5119dbf286402ffb.png)</p><p>![image.png](/images/18_3_24/CNN for Very Fast Ground Segmentation in Velodyne LiDAR Data/10028058-6bba27cb72e18258.png)</p><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><hr><p>![image.png](/images/18_3_24/CNN for Very Fast Ground Segmentation in Velodyne LiDAR Data/10028058-3875bce36b9f2a46.png)</p><p><strong>以上。</strong></p><hr><p>参考文献：<a href="https://arxiv.org/pdf/1709.02128v1.pdf" target="_blank" rel="noopener">CNN for Very Fast Ground Segmentation in Velodyne LiDAR Data.PDF</a></p><p><strong>欢迎访问我的个人博客： <a href="http://zengzeyu.com/">zengzeyu.com</a></strong></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文提出了一种新型的去地面点云方法。一种对3D点云数据编码来给CNN进行训练，最后来分割地面点云的方法。
    
    </summary>
    
    
      <category term="Paper" scheme="http://zengzeyu.com/categories/Paper/"/>
    
    
      <category term="PCL" scheme="http://zengzeyu.com/tags/PCL/"/>
    
      <category term="CNN" scheme="http://zengzeyu.com/tags/CNN/"/>
    
  </entry>
  
  <entry>
    <title>点云降采样输入到caffe数据输入层</title>
    <link href="http://zengzeyu.com/2018/03/21/down_sample_for_caffe_data_layer/"/>
    <id>http://zengzeyu.com/2018/03/21/down_sample_for_caffe_data_layer/</id>
    <published>2018-03-21T14:55:54.000Z</published>
    <updated>2020-07-15T04:47:27.337Z</updated>
    
    <content type="html"><![CDATA[<p>KITTI 数据根据上篇博文 <strong><em>KITTI unorganised cloud to organised cloud</em></strong> 输出尺寸为 <strong><em>HxW = 64x4500</em></strong>， 本文准备复现论文 <a href="https://arxiv.org/pdf/1709.02128.pdf" target="_blank" rel="noopener">CNN for Very Fast Ground Segmentation in Velodyne LiDAR Data</a>， 根据论文内CNN网络结构需要对数据尺寸进行调整，调整尺寸为 <strong><em>HxW = 64x360</em></strong>， 相当于读源数据进行了降采样（downsampling），由于无法使用 <strong>pooling</strong> 池化操作来达到此目的，所以进行手工调整。<a id="more"></a></p><h1 id="点云数据调整"><a href="#点云数据调整" class="headerlink" title="点云数据调整"></a>点云数据调整</h1><hr><h2 id="调整规则"><a href="#调整规则" class="headerlink" title="调整规则"></a>调整规则</h2><hr><h3 id="尺寸调整规则"><a href="#尺寸调整规则" class="headerlink" title="尺寸调整规则"></a>尺寸调整规则</h3><p>将点云数据由 <strong><em>HxW = 64x4500</em></strong> 调整为 <strong><em>HxW = 64x360</em></strong>，可见数据在 <strong><em>H</em></strong> 高度方向上不发生改变， <strong><em>W</em></strong> 水平方向上由 <strong>4500</strong> 调整为 <strong>360</strong>，本文依据以下规则进行调整：</p><blockquote><p>调整前尺寸：  |- - - - - - - - - - - - - - - - - - - - . . .  4500 . . . - - - - - - - - - - - - - - - - - - - - - - |<br>调整后尺寸：  | - 12 - | - 13 - | - 12 - | - 13 - | . . . . . . . . . . . . .| - 12 - | - 13 - | - 12 - | - 13 - | </p></blockquote><p>将 <strong>12</strong> 与 <strong>13</strong> 个与卷积 <strong><em>kernel</em></strong> 类似的结构定义为卷积容器，上述两个尺寸的卷积容器交替进行采样，得到最终结构输出尺寸为 <strong>4500 / 25 x 2 = 360</strong>。</p><h3 id="数据调整"><a href="#数据调整" class="headerlink" title="数据调整"></a>数据调整</h3><hr><p><strong>对每一个卷积容器内的数据进行提取操作，依据不同的数据使用不同的数据提取规则。以一个卷积容器为例，假设已经得到了容器内的数据，下面将对卷积容器内不同情况进行讨论。</strong></p><h5 id="1-明确提取代表点的原则，目标"><a href="#1-明确提取代表点的原则，目标" class="headerlink" title="1. 明确提取代表点的原则，目标"></a>1. 明确提取代表点的原则，目标</h5><p>假设要提取的点为 <strong>A = [ r, c, h, w, l ]</strong> 点提取代表点不是为了正确反应卷积容器内的所有点的特征分布，而是，<strong>A</strong>点自身的特征能和容器内最相近的数据点label能一一对应，这样才能保证训练网络时的数据准确性。<br>因为我们的目标是，最后训练得到网络之后，通过网络来进行预测，那么，对于测试数据，也要进行尺寸 rescale 为 <strong>4500 / 25 x 2 = 360</strong> 操作，最后，根据对代表点 <strong><em>label</em></strong> 的预测，将 <strong><em>label</em></strong> 赋值到同卷积容器内的其他点。</p><h5 id="2-当容器内数据不为空，选取代表数据规则可进行下面三个方向的思考："><a href="#2-当容器内数据不为空，选取代表数据规则可进行下面三个方向的思考：" class="headerlink" title="2. 当容器内数据不为空，选取代表数据规则可进行下面三个方向的思考："></a>2. 当容器内数据不为空，选取代表数据规则可进行下面三个方向的思考：</h5><ul><li>采用平均值方法： 依据容器内数据个数取 <strong><em>[range, height]</em></strong> 平均值，并根据二者数据的权重方向思考寻找容器内与计算得出的平均值特征最近的点，用作代替，但是，用哪个做代表值作为寻找代替点的依据，需要仔细考量！</li><li>采用最近距离方法： 一次迭代就可查找出结果，直接用最近距离点作为代表点</li><li>采用高度方法： 考虑到高度方向的作为障碍物与地面区分的重要特征，可用此方法</li></ul><h3 id="数据调整输出"><a href="#数据调整输出" class="headerlink" title="数据调整输出"></a>数据调整输出</h3><hr><p><strong>以 label data 为例输出显示，使用matplot进行绘图，python代码如下：</strong><br>​```<br>import matplotlib.pyplot as plt</p><p>in_label = in_file[:,:,0]<br>fig = plt.figure()<br>origin_label = fig.add_subplot(121)<br>origin_label.imshow(in_label)<br>rescale_label =<br>plt.show(fig)<br>​```</p><h3 id="调整前后数据（源数据）"><a href="#调整前后数据（源数据）" class="headerlink" title="调整前后数据（源数据）"></a>调整前后数据（源数据）</h3><p>由下图可看出，点颜色有三种，分别是代表三个 <strong><em>label</em></strong> 值： [ -1, 0, 1 ]</p><p>调整尺寸后点颜色有二种，分别代表两个 <strong><em>label</em></strong> 值： [ 0, 1 ]</p><p><img data-src="/images/18_3_30/down_sample_for_caffe_data_layer/10028058-5caa1c5c8af2546d.png" alt="Screenshot from 2018-03-29 16:04:32.png"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;KITTI 数据根据上篇博文 &lt;strong&gt;&lt;em&gt;KITTI unorganised cloud to organised cloud&lt;/em&gt;&lt;/strong&gt; 输出尺寸为 &lt;strong&gt;&lt;em&gt;HxW = 64x4500&lt;/em&gt;&lt;/strong&gt;， 本文准备复现论文 &lt;a href=&quot;https://arxiv.org/pdf/1709.02128.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CNN for Very Fast Ground Segmentation in Velodyne LiDAR Data&lt;/a&gt;， 根据论文内CNN网络结构需要对数据尺寸进行调整，调整尺寸为 &lt;strong&gt;&lt;em&gt;HxW = 64x360&lt;/em&gt;&lt;/strong&gt;， 相当于读源数据进行了降采样（downsampling），由于无法使用 &lt;strong&gt;pooling&lt;/strong&gt; 池化操作来达到此目的，所以进行手工调整。
    
    </summary>
    
    
      <category term="ROS" scheme="http://zengzeyu.com/categories/ROS/"/>
    
    
      <category term="ROS" scheme="http://zengzeyu.com/tags/ROS/"/>
    
      <category term="PCL" scheme="http://zengzeyu.com/tags/PCL/"/>
    
  </entry>
  
  <entry>
    <title>Effective c++ 1.0</title>
    <link href="http://zengzeyu.com/2018/03/21/Effective-c++-1.0/"/>
    <id>http://zengzeyu.com/2018/03/21/Effective-c++-1.0/</id>
    <published>2018-03-21T14:55:54.000Z</published>
    <updated>2020-07-15T04:54:31.638Z</updated>
    
    <content type="html"><![CDATA[<p><strong>欢迎访问我的个人博客： <a href="http://zengzeyu.com/">zengzeyu.com</a></strong> <a id="more"></a></p><h1 id="Tip"><a href="#Tip" class="headerlink" title="Tip"></a>Tip</h1><p>在针对类中非 <strong><code>public</code></strong> 成员函数编写函数接口时，不应该像 <strong><code>public</code></strong> 成员函数一样不写传参变量。非 <strong><code>public</code></strong> 成员函数传参变量在函数内部被调用时，有利于及时输出数据进行可视化，在调用该非 <strong><code>public</code></strong> 成员函数的函数内部进行调试时，可只对数据输入输出进行观察，而不用关心非 <strong><code>public</code></strong> 成员函数内部实现细节，内部实现细节应该和调试阶段分开。<br>同时，对每一个输出非 <strong><code>public</code></strong> 成员函数的形参变量，在调用该非 <strong><code>public</code></strong> 成员函数内部的开始应进行数据清楚，以达到在调用该函数的内部相同类型临时变量容器的重复利用，这样做的目的是节省内存运行空间。</p><h1 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">bool KittiPCL::filtCloudWithNormalZ(const PointCloudXYZI::ConstPtr &amp;in_cloud,</span><br><span class="line">                                    PointCloudXYZI::Ptr &amp;out_cloud, const float &amp;beam_range)</span><br><span class="line">&#123;</span><br><span class="line">    if ( beam_range &lt;&#x3D; 0 || beam_range &gt; 1 )</span><br><span class="line">    &#123;</span><br><span class="line">        std::cerr &lt;&lt; &quot;KittiPCL::filtCloudWithNormalZ(): Beam range NOT correct!&quot; &lt;&lt; std::endl;</span><br><span class="line">        return false;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    out_cloud-&gt;clear();</span><br><span class="line">    &#x2F;&#x2F;classify point with normal z</span><br><span class="line">    PointCloudXYZINormal::Ptr cloud_normal ( new PointCloudXYZINormal );</span><br><span class="line">    this-&gt;computeNormal( in_cloud, cloud_normal );</span><br><span class="line">    int beam_size;</span><br><span class="line">    beam_size &#x3D; static_cast&lt;int &gt;( 1 &#x2F; beam_range );</span><br><span class="line">    std::vector&lt;PointCloudXYZI&gt; classify_cloud_vec;</span><br><span class="line">    classify_cloud_vec.resize( beam_size );</span><br><span class="line">    int beam_num &#x3D; 0;</span><br><span class="line">    for (int i &#x3D; 0; i &lt; cloud_normal-&gt;size(); ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        if ( isnan( cloud_normal-&gt;at(i).x ) )</span><br><span class="line">            continue;</span><br><span class="line"></span><br><span class="line">        if ( cloud_normal-&gt;at(i).normal_z &lt; 0 )</span><br><span class="line">            beam_num &#x3D; - static_cast&lt; int &gt; ( cloud_normal-&gt;at(i).normal_z &#x2F; beam_range );</span><br><span class="line">        else</span><br><span class="line">            beam_num &#x3D; static_cast&lt; int &gt; ( cloud_normal-&gt;at(i).normal_z &#x2F; beam_range );</span><br><span class="line"></span><br><span class="line">        if ( beam_num &gt;&#x3D; 0 &amp;&amp; beam_num &lt; beam_size )</span><br><span class="line">            classify_cloud_vec[ beam_num ].push_back( in_cloud-&gt;at(i) );</span><br><span class="line">    &#125;</span><br><span class="line">    &#x2F;&#x2F;find the largest size point cloud</span><br><span class="line">    size_t pts_size &#x3D; 0;</span><br><span class="line">    size_t largest_num &#x3D; 0;</span><br><span class="line">    for (int j &#x3D; 0; j &lt; classify_cloud_vec.size(); ++j)</span><br><span class="line">    &#123;</span><br><span class="line">        if ( pts_size &lt; classify_cloud_vec[j].size() )</span><br><span class="line">        &#123;</span><br><span class="line">            pts_size &#x3D; classify_cloud_vec[j].size();</span><br><span class="line">            largest_num &#x3D; j;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    *out_cloud &#x3D; classify_cloud_vec[largest_num];</span><br><span class="line"></span><br><span class="line">    return true;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">void KittiPCL::generateGroundCloud(PointCloudXYZI::Ptr &amp;out_cloud, visualization_msgs::MarkerPtr &amp;plane_marker)</span><br><span class="line">&#123;</span><br><span class="line">    PointCloudXYZI::Ptr operat_cloud ( new PointCloudXYZI );</span><br><span class="line">    *operat_cloud &#x3D; *kitti_organised_cloud_ptr_;</span><br><span class="line">    &#x2F;&#x2F;1. compute transform matrix</span><br><span class="line">    PointCloudXYZI::Ptr normal_filt_cloud ( new PointCloudXYZI );</span><br><span class="line">    this-&gt;filtCloudWithNormalZ( operat_cloud, normal_filt_cloud );</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;欢迎访问我的个人博客： &lt;a href=&quot;http://zengzeyu.com/&quot;&gt;zengzeyu.com&lt;/a&gt;&lt;/strong&gt;
    
    </summary>
    
    
      <category term="c++" scheme="http://zengzeyu.com/categories/c/"/>
    
    
      <category term="c++" scheme="http://zengzeyu.com/tags/c/"/>
    
  </entry>
  
  <entry>
    <title>基于几何特征的地面点云分割</title>
    <link href="http://zengzeyu.com/2018/03/21/geometry_feature_segmentation_for_ground_point/"/>
    <id>http://zengzeyu.com/2018/03/21/geometry_feature_segmentation_for_ground_point/</id>
    <published>2018-03-21T14:55:54.000Z</published>
    <updated>2020-07-15T04:53:59.329Z</updated>
    
    <content type="html"><![CDATA[<p>激光雷达扫描得到的点云含有大部分地面点，这对后续障碍物点云的分类、识别和跟踪带来麻烦，所以需要首先滤波滤掉。传统的基于几何特征的滤波是最基本最简单的方法，目前本文尝试的有如下几种：<a id="more"></a></p><ul><li>水平面校准</li><li>法向量</li><li>栅格高度差</li><li>栅格最低高度以上0.2米</li><li>绝对高度</li><li>平均高度</li></ul><p>以上方法基于假设是地面点云所构成的地面为平面，而不是弧面，当然对于有倾斜角度的地面也是可以先通过水平面校准然后再进行后处理来达到目标。下面将针对以上几种方法，通过实验结果比对各自方法优劣。</p><h2 id="1-激光雷达地面点云分割方法"><a href="#1-激光雷达地面点云分割方法" class="headerlink" title="1. 激光雷达地面点云分割方法"></a>1. 激光雷达地面点云分割方法</h2><hr><h3 id="1-1-水平面校准"><a href="#1-1-水平面校准" class="headerlink" title="1.1 水平面校准"></a>1.1 水平面校准</h3><hr><p>水平面校准顾名思义就是通过找到地面点所在平面，然后进行校准点云的方法。通过此步可将数据采集阶段，采集道德地面点云相对于激光雷达 <strong><em>z</em></strong> 轴不平行校准为与之平行。目的是为后续处理提供更易于处理的点云。</p><h4 id="方法过程："><a href="#方法过程：" class="headerlink" title="方法过程："></a>方法过程：</h4><ol><li>分割出大致地面点</li><li>找到地点所在平面</li><li>通过变换矩阵校准平面</li></ol><p><strong>1. 分割出大致地面点</strong><br>这一步可使用栅格高度差、绝对高度、法向量等方法来进行分割，目的只需要找到大部分地面点即可，不用进行精确的分割。本文通过法向量进行分割找到地面点。</p><p><strong>2. 找到地点所在平面</strong><br>通过PCL自带函数进行处理。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">bool estimateGroundPlane(PointCloudXYZI::Ptr &amp;in_cloud, PointCloudXYZI::Ptr &amp;out_cloud,</span><br><span class="line">                                   visualization_msgs::MarkerPtr &amp;plane_marker, const float in_distance_thre)</span><br><span class="line">&#123;</span><br><span class="line">    &#x2F;&#x2F;plane segmentation</span><br><span class="line">    pcl::SACSegmentation&lt;pcl::PointXYZI&gt; plane_seg;</span><br><span class="line">    pcl::PointIndices::Ptr plane_inliers ( new pcl::PointIndices );</span><br><span class="line">    pcl::ModelCoefficients::Ptr plane_coefficients ( new pcl::ModelCoefficients );</span><br><span class="line">    plane_seg.setOptimizeCoefficients (true);</span><br><span class="line">    plane_seg.setModelType ( pcl::SACMODEL_PLANE );</span><br><span class="line">    plane_seg.setMethodType ( pcl::SAC_RANSAC );</span><br><span class="line">    plane_seg.setDistanceThreshold ( in_distance_thre );</span><br><span class="line">    plane_seg.setInputCloud ( in_cloud );</span><br><span class="line">    plane_seg.segment ( *plane_inliers, *plane_coefficients );</span><br><span class="line">    return true;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>通过上述代码找到平面之后，确定平面法向量就可以找到其到 <strong><em>z</em></strong> 轴的变换矩阵 <strong><em>T</em></strong> 。</p><p><strong>3. 通过变换矩阵校准平面</strong><br>将原始点云与变换矩阵 <strong><em>T</em></strong> 作点积就可得到校准后点云。<br><img data-src="/images/18_3_22/geometry_feature_segmentation_for_ground_point/10028058-a89ad85720742b69.png" alt="校准点云(白色点为校准前点云，绿色点为校准后点云)"><br>从图左下方可观察到校准后点云与原始点云有一定距离，说明激光雷达在采集数据时，其 <strong><em>z</em></strong> 轴与地面法向量不平行，而且这种情况随时都在发生。自动驾驶车辆行驶过程中，路面随时都有小的颠簸，偶尔还会有较大颠簸，如通过城市道路中的减速带，转弯时速度过大等等情况。所以校准点云是很有必要的。</p><h3 id="1-2-栅格高度差方法"><a href="#1-2-栅格高度差方法" class="headerlink" title="1.2 栅格高度差方法"></a>1.2 栅格高度差方法</h3><hr><h4 id="方法过程：-1"><a href="#方法过程：-1" class="headerlink" title="方法过程："></a>方法过程：</h4><ol><li>根据栅格尺寸生成栅格</li><li>计算每个栅格最低点与最高点高度差 </li><li>比较 <strong><em>h</em></strong> 与预设高度差阈值 <strong><em>threshold</em></strong> 大小，对栅格进行分类</li><li>根据栅格分类，对栅格内点进行分类</li></ol><h4 id="方法结果"><a href="#方法结果" class="headerlink" title="方法结果"></a>方法结果</h4><p><img data-src="/images/18_3_22/geometry_feature_segmentation_for_ground_point/10028058-a89ad85720742b69.png" alt="鸟瞰图"></p><p><img data-src="/images/18_3_22/geometry_feature_segmentation_for_ground_point/10028058-8fbf0adb43fb73a9.png" alt="右视图"></p><h4 id="结果分析"><a href="#结果分析" class="headerlink" title="结果分析"></a>结果分析</h4><p>依据对每个栅格的高度差的大小进行分类，栅格高度差方法依赖于点云数据。栅格内地面点高度差特征符合栅格高度差方法，但是对于高平台仍然符合该特征，所以对于进行栅格话之后的高平台点仍然被分类为地面点。但是，该方法的分类出的地面点包含真实的地面点。</p><h3 id="1-3-法向量方法"><a href="#1-3-法向量方法" class="headerlink" title="1.3 法向量方法"></a>1.3 法向量方法</h3><hr><p>法向量方法基于假设为计算得到的地面点法向量为竖直向上或向下，即地面点法向量值为 <strong>（0, 0, 1）</strong> 或 <strong>（0, 0, -1）</strong> 。</p><h4 id="方法过程：-2"><a href="#方法过程：-2" class="headerlink" title="方法过程："></a>方法过程：</h4><ol><li>计算点法向量</li><li>设定法向量阈值 <strong><em>threshold</em></strong> 进行点分类</li></ol><h4 id="方法结果-1"><a href="#方法结果-1" class="headerlink" title="方法结果"></a>方法结果</h4><p><img data-src="/images/18_3_22/geometry_feature_segmentation_for_ground_point/10028058-7a000d1ce9475827.png" alt="鸟瞰图"></p><p><img data-src="/images/18_3_22/geometry_feature_segmentation_for_ground_point/10028058-f045d011c812ae05.png" alt="右视图"></p><h4 id="结果分析-1"><a href="#结果分析-1" class="headerlink" title="结果分析"></a>结果分析</h4><p>根据法向量方法的假设，一定要先对点云进行校正，如果不进行校正，那么很可能出现某一帧没有地面点被分割出来的极端情况（激光雷达倾斜角度过大）。法向量方法与高度差方法结果类似，对于平台类型障碍物生成的点无法有效区分。所以可以看到右视图中有部分店漂浮与真实地面点上方。</p><h3 id="1-4-栅格最低高度以上0-2米方法"><a href="#1-4-栅格最低高度以上0-2米方法" class="headerlink" title="1.4 栅格最低高度以上0.2米方法"></a>1.4 栅格最低高度以上0.2米方法</h3><hr><p>栅格最低高度以上 <strong>0.2</strong> 米方法中的数值 <strong>0.2</strong> 可在 <strong>0.2</strong> 附近进行选取，有论文设置为 <strong>0.15</strong> 。此方法与栅格高度差方法类似，也是基于栅格内点的高度信息来进行点分类。不过该方法并没有对栅格进行地面栅格或障碍物栅格分类，而在每个栅格内进行点的分类，最后将所有栅格内的点汇总得到地面点。</p><h4 id="方法过程：-3"><a href="#方法过程：-3" class="headerlink" title="方法过程："></a>方法过程：</h4><ol><li>生成栅格地图</li><li>找到栅格内最低点，并储存其高度 <strong><em>h</em></strong></li><li>找到栅格内点高度小于 <strong><em>h + 0.2</em></strong>，分类为地面点</li></ol><h4 id="方法结果-2"><a href="#方法结果-2" class="headerlink" title="方法结果"></a>方法结果</h4><p><img data-src="/images/18_3_22/geometry_feature_segmentation_for_ground_point/10028058-d1c1dc002a216fae.png" alt="鸟瞰图"></p><p><img data-src="/images/18_3_22/geometry_feature_segmentation_for_ground_point/10028058-b1bd69d9d7e04181.png" alt="右视图"></p><h4 id="结果分析-2"><a href="#结果分析-2" class="headerlink" title="结果分析"></a>结果分析</h4><p>栅格最低高度以上0.2米方法依赖于栅格内的最低点选取，当最地点正好是真实地面点时，结果较为正确，反之则不然。与栅格高度差方法类似，该方法对悬浮物无法处理。</p><h3 id="1-5-绝对高度方法"><a href="#1-5-绝对高度方法" class="headerlink" title="1.5 绝对高度方法"></a>1.5 绝对高度方法</h3><hr><p>绝对高度方法根据校准后点云高度进行分割，通过设定阈值将点云分为地面点和障碍物点。</p><h4 id="方法过程：-4"><a href="#方法过程：-4" class="headerlink" title="方法过程："></a>方法过程：</h4><ol><li>校准点云</li><li>根据高度阈值 <strong><em>threshold</em></strong> 对点进行分类</li></ol><h4 id="方法结果-3"><a href="#方法结果-3" class="headerlink" title="方法结果"></a>方法结果</h4><p><img data-src="/images/18_3_22/geometry_feature_segmentation_for_ground_point/10028058-3f9f58ddc8d145b9.png" alt="鸟瞰图"></p><p><img data-src="/images/18_3_22/geometry_feature_segmentation_for_ground_point/10028058-350e221b7d7b3bcd.png" alt="右视图"></p><h4 id="结果分析-3"><a href="#结果分析-3" class="headerlink" title="结果分析"></a>结果分析</h4><p>绝对高度方法必须对校准点云进行操作，根据校准后点云通过设定高度阈值进行分类。从鸟瞰图可看出，远处有一部分地面点被分割为障碍物点，从结果右视图可看出，该方法对悬浮物可很好处理。</p><h3 id="1-6-平均高度方法"><a href="#1-6-平均高度方法" class="headerlink" title="1.6 平均高度方法"></a>1.6 平均高度方法</h3><hr><p>平均高度方法是对预处理后已经包含大部分地面点进行的处理，而不能单独进行使用。本文采用<strong>栅格最低点高度以上 <em>0.2</em> 米</strong>方法作为预处理，其他地面点预处理也可。该方法基于假设为，预处理分割后得到点中地面点为绝大部分点，从而可根据平均高度作为进一步滤波。</p><h4 id="方法过程：-5"><a href="#方法过程：-5" class="headerlink" title="方法过程："></a>方法过程：</h4><ol><li>栅格最低点高度以上 <em>0.2</em> 米方法分割出地面点</li><li>计算 <strong>过程 1.</strong> 得到地面点的平均高度 <strong><em>h</em></strong></li><li>以 <strong><em>h</em></strong> 为阈值再进行分割得到地面点</li></ol><h4 id="方法结果-4"><a href="#方法结果-4" class="headerlink" title="方法结果"></a>方法结果</h4><p><img data-src="/images/18_3_22/geometry_feature_segmentation_for_ground_point/10028058-b59b735d90d94aa3.png" alt="鸟瞰图"></p><p><img data-src="/images/18_3_22/geometry_feature_segmentation_for_ground_point/10028058-2d8f34015aa60324.png" alt="右视图"></p><h4 id="结果分析-4"><a href="#结果分析-4" class="headerlink" title="结果分析"></a>结果分析</h4><p>平均高度方法作为其他方法的一个小的补充，可对分割出的点悬浮物点进行进一步滤波。但对当大平台场景无法处理。</p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><hr><p>本文对基于几何特征方法 <strong>水平面校准方法</strong>、<strong>法向量方法</strong>、<strong>栅格高度差方法</strong>、<strong>栅格最低高度以上0.2米方法</strong>、<strong>绝对高度方法</strong>、<strong>平均高度方法</strong> 等方法进行了分别分析，对各自方法优缺点进行了探讨。各个方法可以进行合理组合来达到地面点分割效果。<br>这些方法可作为机器学习方法的数据集生成，为机器学习方法做好数据准备。</p><hr><p><strong>欢迎访问我的个人博客： <a href="http://zengzeyu.com/">zengzeyu.com</a></strong></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;激光雷达扫描得到的点云含有大部分地面点，这对后续障碍物点云的分类、识别和跟踪带来麻烦，所以需要首先滤波滤掉。传统的基于几何特征的滤波是最基本最简单的方法，目前本文尝试的有如下几种：
    
    </summary>
    
    
      <category term="ROS" scheme="http://zengzeyu.com/categories/ROS/"/>
    
    
      <category term="ROS" scheme="http://zengzeyu.com/tags/ROS/"/>
    
      <category term="PCL" scheme="http://zengzeyu.com/tags/PCL/"/>
    
  </entry>
  
  <entry>
    <title>KITTI unorganised cloud to organised cloud</title>
    <link href="http://zengzeyu.com/2018/03/21/kitti_unorganised_point_to_organised_cloud/"/>
    <id>http://zengzeyu.com/2018/03/21/kitti_unorganised_point_to_organised_cloud/</id>
    <published>2018-03-21T14:55:54.000Z</published>
    <updated>2020-07-15T04:51:54.038Z</updated>
    
    <content type="html"><![CDATA[<p>KITTI 点云数据集 <strong>bin</strong> 格式转 <strong>pcd</strong> 格式请参照本人博客文章： 《KITTI - 二进制点云数据集》。<br>KITTI下载点云数据集为 <strong>unorganised</strong> ， 这为计算带来了麻烦，本文将无序点云进行排序生成有序点（<strong>organiesd</strong>）。<a id="more"></a></p><h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>KITTI 点云数据集是原始激光雷达点云经过了预处理之后的点云，预处理包括：</p><ul><li>将高度为2米以上点过滤（2米为估计，没有考证）</li><li>噪点过滤</li></ul><h3 id="思路-1："><a href="#思路-1：" class="headerlink" title="思路 1："></a>思路 1：</h3><ol><li>将垂直方向上的激光束按照64个水平高度格子进行分类</li><li>在每一个水平高度上，按照水平角度分辨率计算此排激光束排序</li><li>根据水平面上的 <strong><em>x</em></strong>、**<em>y**</em> 坐标值进行排序</li></ol><h4 id="预期问题："><a href="#预期问题：" class="headerlink" title="预期问题："></a>预期问题：</h4><ul><li>每一束激光的角度对应格子，不一定能正好对上，也就是说，可能存在数据偏差，结果导致某一个格子没有点，某个格子有多个点情况。当然，这种情况在激光雷达生成原始数据中就存在。</li></ul><h4 id="重要问题："><a href="#重要问题：" class="headerlink" title="重要问题："></a>重要问题：</h4><ul><li><p><strong>1. 怎样判断哪一堆点云属于一束激光扫出来的？或者说，怎样判断在 **unorganised</strong> 点云中哪里是下一帧点云的分隔标识？通过哪些信息来定义这个标识，从而能保证分隔正确？**<br><strong>思路：</strong><br>KITTI数据集存储是按照一束激光的所有扫描数据存储完之后再存储下一束激光的数据，所以标识可以通过计算点在水平面上的转角值之差来得到。<br><strong>解决方法：</strong><br>因为每一束激光扫描起始点转角为0°附近值，结束点转角为360°附近值，所以在<strong>unorganised</strong>点云中，当相邻两个点转角值差大于100°（或者更大，值可任意取，但建议不要超过300°）时，可认定为两束激光扫描点云储存在<strong>unorganised</strong>点云中的分界标识。</p></li><li><p><strong>2. 如何确定每一排格子数目？怎样将每一束激光点云储存在对应的此排格子中？</strong><br><strong>思路：</strong><br>a. 由<a href="http://www.velodynelidar.com/hdl-64e.html" target="_blank" rel="noopener">官方提供水平角分辨率<strong>0.08°</strong></a>进行格子数确定<br>b. 统计每排最小转角 <strong><em>alpha</em></strong> ，以此作为水平角分辨率<br><strong>解决方法：</strong><br>根据方法b计算得出**<em>alpha**</em>值在<strong>0.08</strong>附近：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">0: 0.0766167</span><br><span class="line">1: 0.0766167</span><br><span class="line">2: 0.0766167</span><br><span class="line">3: 0.0791294</span><br><span class="line">4: 0.0815647</span><br><span class="line">5: 0.0740187</span><br><span class="line">6: 0.0815647</span><br><span class="line">7: 0.0766167</span><br><span class="line">8: 0.0791294</span><br><span class="line">9: 0.0766167</span><br><span class="line">10: 0.0791294</span><br><span class="line">11: 0.0815647</span><br><span class="line">12: 0.0815647</span><br><span class="line">13: 0.0815647</span><br><span class="line">14: 0.0791294</span><br><span class="line">15: 0.0766167</span><br><span class="line">16: 0.0815647</span><br><span class="line">17: 0.0791294</span><br><span class="line">18: 0.0791294</span><br><span class="line">19: 0.0791294</span><br><span class="line">20: 0.0815647</span><br><span class="line">21: 0.0815647</span><br><span class="line">22: 0.0815647</span><br><span class="line">23: 0.0791294</span><br><span class="line">24: 0.0791294</span><br><span class="line">25: 0.0740187</span><br><span class="line">26: 0.0766167</span><br><span class="line">27: 0.0766167</span><br><span class="line">28: 0.0766167</span><br><span class="line">29: 0.0766167</span><br><span class="line">30: 0.0766167</span><br><span class="line">31: 0.0766167</span><br><span class="line">32: 0.068528</span><br><span class="line">33: 0.0656106</span><br><span class="line">34: 0.068528</span><br><span class="line">35: 0.0740187</span><br><span class="line">36: 0.0740187</span><br><span class="line">37: 0.0740187</span><br><span class="line">38: 0.0740187</span><br><span class="line">39: 0.0713263</span><br><span class="line">40: 0.0766167</span><br><span class="line">41: 0.0713263</span><br><span class="line">42: 0.0713263</span><br><span class="line">43: 0.0740187</span><br><span class="line">44: 0.0656106</span><br><span class="line">45: 0.0656106</span><br><span class="line">46: 0.0713263</span><br><span class="line">47: 0.0740187</span><br><span class="line">48: 0.0740187</span><br><span class="line">49: 0.0713263</span><br><span class="line">50: 0.0656106</span><br><span class="line">51: 0.0656106</span><br><span class="line">52: 0.0713263</span><br><span class="line">53: 0.0713263</span><br><span class="line">54: 0.0713263</span><br><span class="line">55: 0.068528</span><br><span class="line">56: 0.068528</span><br><span class="line">57: 0.068528</span><br><span class="line">58: 0.068528</span><br><span class="line">59: 0.0656106</span><br><span class="line">60: 0.0559529</span><br><span class="line">61: 0.068528</span><br><span class="line">62: 0.0625573</span><br><span class="line">63: 0.0656106</span><br></pre></td></tr></table></figure><p>同时，根据官方给定数据水平角分辨率 <strong>0.08°</strong> 进行计算，得到最大点格子数为：**<code>max col: 4499</code>**<br>因此，本文决定依据官方给定数据水平角分辨率 <strong>0.08°</strong> 进行计算，结果得到每一束激光扫描得到点个数为：</p></li></ul><p><strong>360° / 0.08° = 4500 （个）</strong></p><ul><li><p><strong>3. PCL中的 **Organiesd cloud</strong> 属性设置**<br>PCL中通过如下代码，设置初始点云为 <strong>organised</strong> ：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">bool initialKittiOrganiseCloud(const int &amp;row_size, const int &amp;col_size,pcl::PointCloud&lt;pcl::PointXYZI&gt;::Ptr&amp; in_cloud)</span><br><span class="line">&#123;</span><br><span class="line">    in_cloud-&gt;resize( row_size * col_size );</span><br><span class="line">    in_cloud-&gt;height &#x3D; row_size;</span><br><span class="line">    in_cloud-&gt;width &#x3D; col_size;</span><br><span class="line">    return true;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p><strong>4. 关于C++：为何在构造函数内进行的对变量的初始化值会在后续函数中发现该函数并未达到初始化的效果？而当将初始化变量函数放到其他函数内，后续函数并不会报错？<br>具体如下：</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 初始化变量函数</span><br><span class="line">bool GroundRemove::initialKittiOrganiseCloud(const int &amp;row_size, const int &amp;col_size)</span><br><span class="line">&#123;</span><br><span class="line">    kitti_organised_cloud_ptr_-&gt;height &#x3D; row_size;</span><br><span class="line">    kitti_organised_cloud_ptr_-&gt;width &#x3D; col_size;</span><br><span class="line">    kitti_organised_cloud_ptr_-&gt;resize( row_size * col_size );</span><br><span class="line">    for (int i &#x3D; 0; i &lt; kitti_organised_cloud_ptr_-&gt;height; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        for (int j &#x3D; 0; j &lt; kitti_organised_cloud_ptr_-&gt;width; ++j)</span><br><span class="line">        &#123;</span><br><span class="line">            kitti_organised_cloud_ptr_-&gt;at( j,i ).x &#x3D; NAN;</span><br><span class="line">            kitti_organised_cloud_ptr_-&gt;at( j,i ).y &#x3D; NAN;</span><br><span class="line">            kitti_organised_cloud_ptr_-&gt;at( j,i ).z &#x3D; NAN;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    return true;</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;构造函数</span><br><span class="line">GroundRemove::GroundRemove()</span><br><span class="line">&#123;</span><br><span class="line">    kitti_organised_cloud_ptr_.reset( new PointCloudXYZI );</span><br><span class="line">    this-&gt;initialKittiOrganiseCloud(64, 4500)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;后续函数</span><br><span class="line">bool GroundRemove::arrangePointInOrganise(std::vector&lt;PointCloudXYZI&gt; &amp;in_cloud,</span><br><span class="line">                                                 PointCloudXYZI::Ptr &amp;out_cloud)</span><br><span class="line">&#123;</span><br><span class="line">    if ( in_cloud.empty() )</span><br><span class="line">    &#123;</span><br><span class="line">        std::cerr &lt;&lt; &quot;Input cloud vector is EMPTY!&quot; &lt;&lt; std::endl;</span><br><span class="line">        return false;</span><br><span class="line">    &#125;</span><br><span class="line">    else if ( !out_cloud-&gt;isOrganized() )</span><br><span class="line">    &#123;</span><br><span class="line">        std::cerr &lt;&lt; &quot;Input point cloud is UNORGANISED!&quot; &lt;&lt; std::endl;</span><br><span class="line">        return false;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    float angle &#x3D; 0.0;</span><br><span class="line">    float distance &#x3D; 0.0;</span><br><span class="line">    int col_num &#x3D; 0;</span><br><span class="line">    int tmp_vec_point &#x3D; 0;</span><br><span class="line">    for (int i &#x3D; 0; i &lt; in_cloud.size(); ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        for (int j &#x3D; 0; j &lt; in_cloud[i].size(); ++j)</span><br><span class="line">        &#123;</span><br><span class="line">             angle &#x3D; this-&gt;computeHorResoluteAngle( in_cloud[i].at(j) ) &#x2F; M_PI * 180.0f;</span><br><span class="line">             col_num &#x3D; static_cast&lt;int &gt; ( angle &#x2F; velodyne_angle_res );</span><br><span class="line">             out_cloud-&gt;at(col_num,i) &#x3D; in_cloud[i].at(j)</span><br><span class="line">         &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    return true;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>当把 <strong><code>initialKittiOrganiseCloud()</code>**函数放在构造函数中时，</strong><code>arrangePointInOrganise()</code><strong>内在检测会发生</strong><code>Input point cloud is UNORGANISED!</code><strong>输出，而当把</strong><code>initialKittiOrganiseCloud()</code>**函数放在其他函数中时，该检测会通过。</p></li></ul><p><strong>思路：</strong><br>猜测与构造函数的机制有关，也有可能与PCL有关。<br><strong>解决方法：</strong><br>如果非要在构造函数中使用该函数，尚未找到解决办法。<strong>记录于此，待解决。</strong></p><ul><li><strong>5. 为何每次储存点云相对于原数据会少300个点？具体如下：</strong><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">bool GroundRemove::arrangePointInOrganise(std::vector&lt;PointCloudXYZI&gt; &amp;in_cloud,</span><br><span class="line">                                         PointCloudXYZI::Ptr &amp;out_cloud)</span><br><span class="line">&#123;</span><br><span class="line">    if ( in_cloud.empty() )</span><br><span class="line">    &#123;</span><br><span class="line">        std::cerr &lt;&lt; &quot;Input cloud vector is EMPTY!&quot; &lt;&lt; std::endl;</span><br><span class="line">        return false;</span><br><span class="line">    &#125;</span><br><span class="line">    else if ( !out_cloud-&gt;isOrganized() )</span><br><span class="line">    &#123;</span><br><span class="line">        std::cerr &lt;&lt; &quot;Input point cloud is UNORGANISED!&quot; &lt;&lt; std::endl;</span><br><span class="line">        return false;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    float angle &#x3D; 0.0f;</span><br><span class="line">    float distance &#x3D; 0.0f;</span><br><span class="line">    int col_num &#x3D; 0;</span><br><span class="line">    int tmp_vec_point &#x3D; 0;</span><br><span class="line">    for (int i &#x3D; 0; i &lt; in_cloud.size(); ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        for (int j &#x3D; 0; j &lt; in_cloud[i].size(); ++j)</span><br><span class="line">        &#123;</span><br><span class="line">            angle &#x3D; this-&gt;computeHorResoluteAngle( in_cloud[i].at(j) ) &#x2F; M_PI * 180.0f;</span><br><span class="line">            col_num &#x3D; static_cast&lt;int &gt; ( angle &#x2F; velodyne_angle_res );</span><br><span class="line">            out_cloud-&gt;at(col_num,i) &#x3D; in_cloud[i].at(j);</span><br><span class="line">        &#125;</span><br><span class="line">        tmp_vec_point +&#x3D; in_cloud[i].size();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    int tmp_pt_point &#x3D; 0;</span><br><span class="line">    int tmp_nan_point &#x3D; 0;</span><br><span class="line">    for (int k &#x3D; 0; k &lt; out_cloud-&gt;size(); ++k)</span><br><span class="line">    &#123;</span><br><span class="line">        if (isnanf(out_cloud-&gt;at(k).x))</span><br><span class="line">            tmp_nan_point ++;</span><br><span class="line">        else</span><br><span class="line">            tmp_pt_point ++;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    std::cout &lt;&lt; &quot;origin point size: &quot; &lt;&lt; origin_cloud_ptr_-&gt;size() &lt;&lt; std::endl;</span><br><span class="line">    std::cout &lt;&lt; &quot;tmp_vec_point: &quot; &lt;&lt; tmp_vec_point &lt;&lt; std::endl;</span><br><span class="line">    std::cout &lt;&lt; &quot;tmp_pt_point: &quot; &lt;&lt; tmp_pt_point &lt;&lt; std::endl;</span><br><span class="line">    std::cout &lt;&lt; &quot;num diff: &quot; &lt;&lt; tmp_vec_point - tmp_pt_point &lt;&lt; std::endl;</span><br><span class="line">    std::cout &lt;&lt; &quot;nan point: &quot; &lt;&lt; tmp_nan_point + tmp_pt_point &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    return true;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 某一帧输出</span><br><span class="line">origin point size: 120805</span><br><span class="line">tmp_vec_point: 120804</span><br><span class="line">tmp_pt_point: 120482</span><br><span class="line">num diff: 322</span><br><span class="line">nan point: 288000</span><br></pre></td></tr></table></figure></li></ul><p><strong>思路：</strong><br>在<code>out_cloud-&gt;at(col_num,i) = in_cloud[i].at(j);</code>这行代码中，有这样的逻辑：<br>只管填入点，不管<code>out_cloud-&gt;at(col_num,i)</code>此前是否已经有点，这就会导致点的损失。<br><strong>解决方法：</strong><br>这种情况无法避免，当同一个格子中时，只能选取更优的点，选取原则：格子中距离雷达最近距离的点。</p><p>以上。</p><p><strong>欢迎访问我的个人博客： <a href="http://zengzeyu.com/">zengzeyu.com</a></strong></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;KITTI 点云数据集 &lt;strong&gt;bin&lt;/strong&gt; 格式转 &lt;strong&gt;pcd&lt;/strong&gt; 格式请参照本人博客文章： 《KITTI - 二进制点云数据集》。&lt;br&gt;KITTI下载点云数据集为 &lt;strong&gt;unorganised&lt;/strong&gt; ， 这为计算带来了麻烦，本文将无序点云进行排序生成有序点（&lt;strong&gt;organiesd&lt;/strong&gt;）。
    
    </summary>
    
    
      <category term="ROS" scheme="http://zengzeyu.com/categories/ROS/"/>
    
    
      <category term="ROS" scheme="http://zengzeyu.com/tags/ROS/"/>
    
      <category term="PCL" scheme="http://zengzeyu.com/tags/PCL/"/>
    
  </entry>
  
  <entry>
    <title>cnpy 库使用笔记</title>
    <link href="http://zengzeyu.com/2018/03/15/cnpy_note/"/>
    <id>http://zengzeyu.com/2018/03/15/cnpy_note/</id>
    <published>2018-03-15T15:47:44.000Z</published>
    <updated>2020-07-15T04:57:50.270Z</updated>
    
    <content type="html"><![CDATA[<p>在进行网络训练过程中，在生成训练数据时，一般会使用比较底层的传感器来生成数据，如摄像头或雷达，所以大部分使用C++进行开发。为了将数据转为Numpy Array格式供Python调用，cnpy库就是供C++生成这种格式数据的开源库，由国外一名小哥开发。<a id="more"></a><br>cnpy地址：<a href="https://github.com/rogersce/cnpy" target="_blank" rel="noopener">https://github.com/rogersce/cnpy</a><br>官方例子：<a href="https://github.com/rogersce/cnpy/blob/master/example1.cpp" target="_blank" rel="noopener">https://github.com/rogersce/cnpy/blob/master/example1.cpp</a></p><h3 id="应用方法"><a href="#应用方法" class="headerlink" title="应用方法"></a>应用方法</h3><hr><h4 id="cnpy有两种应用方法："><a href="#cnpy有两种应用方法：" class="headerlink" title="cnpy有两种应用方法："></a>cnpy有两种应用方法：</h4><ol><li>官网方法：将cnpy库加入到ubuntu系统环境中，当做系统库进行调用，类似于安装好的OpenCV库；</li><li>ROS package 方法：将cnpy库包含到ROS工作空间下，当做独立的package供调用。</li></ol><h4 id="方法1："><a href="#方法1：" class="headerlink" title="方法1："></a>方法1：</h4><hr><p>按照官网安装教程安装即可：</p><h5 id="Installation"><a href="#Installation" class="headerlink" title="Installation:"></a>Installation:</h5><p>Default installation directory is /usr/local. To specify a different directory, add <code>-DCMAKE_INSTALL_PREFIX=/path/to/install/dir</code> to the cmake invocation in step 4.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1.  get [cmake](https:&#x2F;&#x2F;github.com&#x2F;rogersce&#x2F;cnpy&#x2F;blob&#x2F;master&#x2F;www.cmake.org)</span><br><span class="line">2.  create a build directory, say $HOME&#x2F;build</span><br><span class="line">3.  cd $HOME&#x2F;build</span><br><span class="line">4.  cmake &#x2F;path&#x2F;to&#x2F;cnpy</span><br><span class="line">5.  make</span><br><span class="line">6.  make install</span><br></pre></td></tr></table></figure><h5 id="Using"><a href="#Using" class="headerlink" title="Using:"></a>Using:</h5><p>To use, <code>#include&quot;cnpy.h&quot;</code> in your source code. Compile the source code mycode.cpp as</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">g++ -o mycode mycode.cpp -L&#x2F;path&#x2F;to&#x2F;install&#x2F;dir -lcnpy -lz --std&#x3D;c++11</span><br></pre></td></tr></table></figure><h4 id="方法2："><a href="#方法2：" class="headerlink" title="方法2："></a>方法2：</h4><hr><ol><li>将下载好的<code>cnpy</code>文件夹放到与调用<code>cnpy</code>库的 package A 同级目录下，并将以下内容添加到A的<code>package.xml</code>中：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;build_depend&gt;cnpy&lt;&#x2F;build_depend&gt;</span><br><span class="line">    &lt;run_depend&gt;cnpy&lt;&#x2F;run_depend&gt;</span><br></pre></td></tr></table></figure></li><li>然后以下内容添加到 A 的<code>Cmakelist.txt</code>中：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">find_package(</span><br><span class="line">        cnpy )</span><br><span class="line"></span><br><span class="line">catkin_package(</span><br><span class="line">        CATKIN_DEPENDS cnpy )</span><br></pre></td></tr></table></figure></li><li>在 A 中调用的<code>xx.h</code>中按照路径包含即可：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">#include &quot;..&#x2F;..&#x2F;..&#x2F;cnpy&#x2F;include&#x2F;cnpy&#x2F;cnpy.h&quot;</span><br></pre></td></tr></table></figure>此方法也适用于任何其他想要调用的package，按照上书步骤操作即可。</li></ol><p>以上。</p><hr>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在进行网络训练过程中，在生成训练数据时，一般会使用比较底层的传感器来生成数据，如摄像头或雷达，所以大部分使用C++进行开发。为了将数据转为Numpy Array格式供Python调用，cnpy库就是供C++生成这种格式数据的开源库，由国外一名小哥开发。
    
    </summary>
    
    
      <category term="Python" scheme="http://zengzeyu.com/categories/Python/"/>
    
    
      <category term="NumPy" scheme="http://zengzeyu.com/tags/NumPy/"/>
    
  </entry>
  
  <entry>
    <title>c++ string 操作汇总</title>
    <link href="http://zengzeyu.com/2018/03/15/cpp_string_operation/"/>
    <id>http://zengzeyu.com/2018/03/15/cpp_string_operation/</id>
    <published>2018-03-15T15:47:44.000Z</published>
    <updated>2020-07-15T04:57:20.618Z</updated>
    
    <content type="html"><![CDATA[<p>作为传递信息的载体<code>string</code>数据类型广泛应用于各种编程语言中，尤其在轻量化语言 <strong>Python</strong> 中发挥的淋漓尽致，通过<code>string</code>传递信息 <strong>Python</strong> 使各个模块组装在一起完成指定的工作任务，这也是 <strong>Python</strong> 被称为“胶水语言”的原因。<a id="more"></a><br>本文着眼于 <strong>c++</strong> 中<code>string</code>的应用。首先建议先浏览<code>string</code><a href="http://www.cplusplus.com/reference/string/string/" target="_blank" rel="noopener">在线文档</a>，这其中包含了大多数日常所需函数。</p><h2 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h2><h3 id="1-string-内查找字符串str"><a href="#1-string-内查找字符串str" class="headerlink" title="1. string 内查找字符串str"></a>1. <code>string</code> 内查找字符串<code>str</code></h3><p><code>std::string::find(str)</code>函数：<a href="http://www.cplusplus.com/reference/string/string/find/" target="_blank" rel="noopener">http://www.cplusplus.com/reference/string/string/find/</a><br><strong>Example</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; string::find</span><br><span class="line">#include &lt;iostream&gt;       &#x2F;&#x2F; std::cout</span><br><span class="line">#include &lt;string&gt;         &#x2F;&#x2F; std::string</span><br><span class="line"></span><br><span class="line">int main ()</span><br><span class="line">&#123;</span><br><span class="line">  std::string str (&quot;There are two needles in this haystack with needles.&quot;);</span><br><span class="line">  std::string str2 (&quot;needle&quot;);</span><br><span class="line"></span><br><span class="line">  &#x2F;&#x2F; different member versions of find in the same order as above:</span><br><span class="line">  std::size_t found &#x3D; str.find(str2);</span><br><span class="line">  if (found!&#x3D;std::string::npos)</span><br><span class="line">    std::cout &lt;&lt; &quot;first &#39;needle&#39; found at: &quot; &lt;&lt; found &lt;&lt; &#39;\n&#39;;</span><br><span class="line"></span><br><span class="line">  found&#x3D;str.find(&quot;needles are small&quot;,found+1,6);</span><br><span class="line">  if (found!&#x3D;std::string::npos)</span><br><span class="line">    std::cout &lt;&lt; &quot;second &#39;needle&#39; found at: &quot; &lt;&lt; found &lt;&lt; &#39;\n&#39;;</span><br><span class="line"></span><br><span class="line">  found&#x3D;str.find(&quot;haystack&quot;);</span><br><span class="line">  if (found!&#x3D;std::string::npos)</span><br><span class="line">    std::cout &lt;&lt; &quot;&#39;haystack&#39; also found at: &quot; &lt;&lt; found &lt;&lt; &#39;\n&#39;;</span><br><span class="line"></span><br><span class="line">  found&#x3D;str.find(&#39;.&#39;);</span><br><span class="line">  if (found!&#x3D;std::string::npos)</span><br><span class="line">    std::cout &lt;&lt; &quot;Period found at: &quot; &lt;&lt; found &lt;&lt; &#39;\n&#39;;</span><br><span class="line"></span><br><span class="line">  &#x2F;&#x2F; let&#39;s replace the first needle:</span><br><span class="line">  str.replace(str.find(str2),str2.length(),&quot;preposition&quot;);</span><br><span class="line">  std::cout &lt;&lt; str &lt;&lt; &#39;\n&#39;;</span><br><span class="line"></span><br><span class="line">  return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其中 <code>npos</code> 值为 <code>-1</code>，用于判断。<br>其他查找功能类函数：</p><ul><li><a href="http://www.cplusplus.com/reference/string/string/rfind/" target="_blank" rel="noopener"><strong>rfind</strong></a>： 找到目标<code>str</code>最后一次出现位置</li><li><a href="http://www.cplusplus.com/reference/string/string/find_first_of/" target="_blank" rel="noopener"><strong>find_first_of</strong></a>： 从起始位置查找目标<code>str</code></li><li><a href="http://www.cplusplus.com/reference/string/string/find_last_of/" target="_blank" rel="noopener"><strong>find_last_of</strong></a>： 从结束位置往回查找目标<code>str</code></li></ul><h3 id="2-string内删除目标str"><a href="#2-string内删除目标str" class="headerlink" title="2. string内删除目标str"></a>2. <code>string</code>内删除目标<code>str</code></h3><p> <a href="http://www.cplusplus.com/reference/string/string/substr/" target="_blank" rel="noopener"><strong>substr</strong></a>： 本质还是查找目标<code>str</code><br><strong>Example</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; string::substr</span><br><span class="line">#include &lt;iostream&gt;</span><br><span class="line">#include &lt;string&gt;</span><br><span class="line"></span><br><span class="line">int main ()</span><br><span class="line">&#123;</span><br><span class="line">  std::string str&#x3D;&quot;We think in generalities, but we live in details.&quot;;</span><br><span class="line">                                           &#x2F;&#x2F; (quoting Alfred N. Whitehead)</span><br><span class="line"></span><br><span class="line">  std::string str2 &#x3D; str.substr (3,5);     &#x2F;&#x2F; &quot;think&quot;</span><br><span class="line"></span><br><span class="line">  std::size_t pos &#x3D; str.find(&quot;live&quot;);      &#x2F;&#x2F; position of &quot;live&quot; in str</span><br><span class="line"></span><br><span class="line">  std::string str3 &#x3D; str.substr (pos);     &#x2F;&#x2F; get from &quot;live&quot; to the end</span><br><span class="line"></span><br><span class="line">  std::cout &lt;&lt; str2 &lt;&lt; &#39; &#39; &lt;&lt; str3 &lt;&lt; &#39;\n&#39;;</span><br><span class="line"></span><br><span class="line">  return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3-字符串分割"><a href="#3-字符串分割" class="headerlink" title="3. 字符串分割"></a>3. 字符串分割</h3><p>字符串分割推荐使用 <strong>std::stringstream</strong> 作为中间载体和 <a href="http://www.cplusplus.com/reference/istream/istream/getline/" target="_blank" rel="noopener"><strong>getline()</strong></a> 函数组合来完成。<br>以分割字符 “ <strong>;</strong> ”为例：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">void splitPathStr(std::string&amp; path_str)</span><br><span class="line">&#123;</span><br><span class="line">    std::vector&lt;std::string&gt; filelists;</span><br><span class="line">    std::stringstream sstr( path_str );</span><br><span class="line">    std::string token;</span><br><span class="line">    while(getline(sstr, token, &#39;;&#39;))</span><br><span class="line">    &#123;</span><br><span class="line">        filelists.push_back(token);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="4-int型与string型互相转换"><a href="#4-int型与string型互相转换" class="headerlink" title="4. int型与string型互相转换"></a>4. int型与string型互相转换</h3><p><strong>int</strong>型转<strong>string</strong>型</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">void int2str(const int &amp;int_temp,string &amp;string_temp)  </span><br><span class="line">&#123;  </span><br><span class="line">        stringstream stream;  </span><br><span class="line">        stream&lt;&lt;int_temp;  </span><br><span class="line">        string_temp&#x3D;stream.str();   &#x2F;&#x2F;此处也可以用 stream&gt;&gt;string_temp  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>string</strong>型转<strong>int</strong>型</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">void str2int(int &amp;int_temp,const string &amp;string_temp)  </span><br><span class="line">&#123;  </span><br><span class="line">    stringstream stream(string_temp);  </span><br><span class="line">    stream&gt;&gt;int_temp;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>未完待续…</p><p>以上。</p><hr><p>参考文献：</p><ol><li><a href="http://www.cplusplus.com/reference/" target="_blank" rel="noopener">c++ reference</a></li><li><a href="http://www.cnblogs.com/gaobw/p/7070622.html" target="_blank" rel="noopener">C++中int、string等常见类型转换</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;作为传递信息的载体&lt;code&gt;string&lt;/code&gt;数据类型广泛应用于各种编程语言中，尤其在轻量化语言 &lt;strong&gt;Python&lt;/strong&gt; 中发挥的淋漓尽致，通过&lt;code&gt;string&lt;/code&gt;传递信息 &lt;strong&gt;Python&lt;/strong&gt; 使各个模块组装在一起完成指定的工作任务，这也是 &lt;strong&gt;Python&lt;/strong&gt; 被称为“胶水语言”的原因。
    
    </summary>
    
    
      <category term="C++" scheme="http://zengzeyu.com/categories/C/"/>
    
    
      <category term="c++" scheme="http://zengzeyu.com/tags/c/"/>
    
  </entry>
  
  <entry>
    <title>KITTI 原始bin格式数据集转PCD格式</title>
    <link href="http://zengzeyu.com/2018/03/15/kitti_bin_to_pcd/"/>
    <id>http://zengzeyu.com/2018/03/15/kitti_bin_to_pcd/</id>
    <published>2018-03-15T15:47:44.000Z</published>
    <updated>2020-07-15T04:55:40.685Z</updated>
    
    <content type="html"><![CDATA[<p>官网数据集说明：<a href="http://www.cvlibs.net/datasets/kitti/raw_data.php" target="_blank" rel="noopener">http://www.cvlibs.net/datasets/kitti/raw_data.php</a><br>数据集详细说明论文：<a href="http://www.cvlibs.net/publications/Geiger2013IJRR.pdf" target="_blank" rel="noopener">http://www.cvlibs.net/publications/Geiger2013IJRR.pdf</a><br>KITTI的激光雷达型号为 <strong>Velodyne HDL-64E</strong> ，<a id="more"></a>具体信息如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Velodyne HDL-64E rotating 3D laser scanner</span><br><span class="line">- 10 Hz</span><br><span class="line">- 64 beams</span><br><span class="line">- 0.09 degree angular resolution</span><br><span class="line">- 2 cm distanceaccuracy</span><br><span class="line">- collecting∼1.3 million points&#x2F;second</span><br><span class="line">- field of view: 360°</span><br><span class="line">- horizontal, 26.8°</span><br><span class="line">- vertical, range: 120 m</span><br></pre></td></tr></table></figure><p>针对激光雷达点云数据集使用的信息在 <strong><em>KITTI_README.TXT</em></strong> 中有详细说明，文件下载地址：<a href="https://github.com/yanii/kitti-pcl" target="_blank" rel="noopener">Code to use the KITTI data set with PCL</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">Velodyne 3D laser scan data</span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line"></span><br><span class="line">The velodyne point clouds are stored in the folder &#39;velodyne_points&#39;. To</span><br><span class="line">save space, all scans have been stored as Nx4 float matrix into a binary</span><br><span class="line">file using the following code:</span><br><span class="line"></span><br><span class="line">  stream &#x3D; fopen (dst_file.c_str(),&quot;wb&quot;);</span><br><span class="line">  fwrite(data,sizeof(float),4*num,stream);</span><br><span class="line">  fclose(stream);</span><br><span class="line"></span><br><span class="line">Here, data contains 4*num values, where the first 3 values correspond to</span><br><span class="line">x,y and z, and the last value is the reflectance information. All scans</span><br><span class="line">are stored row-aligned, meaning that the first 4 values correspond to the</span><br><span class="line">first measurement. Since each scan might potentially have a different</span><br><span class="line">number of points, this must be determined from the file size when reading</span><br><span class="line">the file, where 1e6 is a good enough upper bound on the number of values:</span><br><span class="line"></span><br><span class="line">  &#x2F;&#x2F; allocate 4 MB buffer (only ~130*4*4 KB are needed)</span><br><span class="line">  int32_t num &#x3D; 1000000;</span><br><span class="line">  float *data &#x3D; (float*)malloc(num*sizeof(float));</span><br><span class="line"></span><br><span class="line">  &#x2F;&#x2F; pointers</span><br><span class="line">  float *px &#x3D; data+0;</span><br><span class="line">  float *py &#x3D; data+1;</span><br><span class="line">  float *pz &#x3D; data+2;</span><br><span class="line">  float *pr &#x3D; data+3;</span><br><span class="line"></span><br><span class="line">  &#x2F;&#x2F; load point cloud</span><br><span class="line">  FILE *stream;</span><br><span class="line">  stream &#x3D; fopen (currFilenameBinary.c_str(),&quot;rb&quot;);</span><br><span class="line">  num &#x3D; fread(data,sizeof(float),num,stream)&#x2F;4;</span><br><span class="line">  for (int32_t i&#x3D;0; i&lt;num; i++) &#123;</span><br><span class="line">    point_cloud.points.push_back(tPoint(*px,*py,*pz,*pr));</span><br><span class="line">    px+&#x3D;4; py+&#x3D;4; pz+&#x3D;4; pr+&#x3D;4;</span><br><span class="line">  &#125;</span><br><span class="line">  fclose(stream);</span><br><span class="line"></span><br><span class="line">x,y and y are stored in metric (m) Velodyne coordinates.</span><br></pre></td></tr></table></figure><h2 id="KITTI点云数据集读取与转换"><a href="#KITTI点云数据集读取与转换" class="headerlink" title="KITTI点云数据集读取与转换"></a>KITTI点云数据集读取与转换</h2><h3 id="官方源代码解读"><a href="#官方源代码解读" class="headerlink" title="官方源代码解读"></a>官方源代码解读</h3><p><a href="https://github.com/yanii/kitti-pcl" target="_blank" rel="noopener">Code to use the KITTI data set with PCL</a>下载的源代码文件夹中的**<em>src/kitti2pcd.cpp**</em> 中这个函数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">void readKittiPclBinData(std::string &amp;in_file, std::string&amp; out_file)</span><br><span class="line">&#123;</span><br><span class="line">    &#x2F;&#x2F; load point cloud</span><br><span class="line">    std::fstream input(in_file.c_str(), std::ios::in | std::ios::binary);</span><br><span class="line">    if(!input.good())&#123;</span><br><span class="line">        std::cerr &lt;&lt; &quot;Could not read file: &quot; &lt;&lt; in_file &lt;&lt; std::endl;</span><br><span class="line">        exit(EXIT_FAILURE);</span><br><span class="line">    &#125;</span><br><span class="line">    input.seekg(0, std::ios::beg);</span><br><span class="line"></span><br><span class="line">    pcl::PointCloud&lt;pcl::PointXYZI&gt;::Ptr points (new pcl::PointCloud&lt;pcl::PointXYZI&gt;);</span><br><span class="line"></span><br><span class="line">    int i;</span><br><span class="line">    for (i&#x3D;0; input.good() &amp;&amp; !input.eof(); i++) &#123;</span><br><span class="line">        pcl::PointXYZI point;</span><br><span class="line">        input.read((char *) &amp;point.x, 3*sizeof(float));</span><br><span class="line">        input.read((char *) &amp;point.intensity, sizeof(float));</span><br><span class="line">        points-&gt;push_back(point);</span><br><span class="line">    &#125;</span><br><span class="line">    input.close();</span><br><span class="line">&#x2F;&#x2F;    g_cloud_pub.publish( points );</span><br><span class="line"></span><br><span class="line">    std::cout &lt;&lt; &quot;Read KTTI point cloud with &quot; &lt;&lt; i &lt;&lt; &quot; points, writing to &quot; &lt;&lt; out_file &lt;&lt; std::endl;</span><br><span class="line">    pcl::PCDWriter writer;</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F; Save DoN features</span><br><span class="line">    writer.write&lt; pcl::PointXYZI &gt; (out_file, *points, false);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这个函数是最重要的从 <strong>KITTI</strong> 中读取 <strong><em>.bin</em></strong> 文件转 <strong><em>.pcd</em></strong> 文件。</p><h3 id="可运行完整代码"><a href="#可运行完整代码" class="headerlink" title="可运行完整代码"></a>可运行完整代码</h3><p>下面贴本人完整代码，代码功能：</p><ul><li>读取文件夹下**<em>.bin**</em> 文件</li><li>按照文件名进行排序（虽然默认已经排好序）</li><li>转为**<em>.pcd**</em> 文件，并保存</li><li>发送到 <strong>rviz</strong> 进行显示（可选）<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;</span><br><span class="line">&#x2F;&#x2F; Created by zzy on 3&#x2F;14&#x2F;18.</span><br><span class="line">&#x2F;&#x2F;</span><br><span class="line"></span><br><span class="line">#include &lt;ctime&gt;</span><br><span class="line">#include &quot;ros&#x2F;ros.h&quot;</span><br><span class="line">#include &quot;fcn_data_gen&#x2F;ground_remove.h&quot;</span><br><span class="line"></span><br><span class="line">static ros::Publisher g_cloud_pub;</span><br><span class="line">static std::vector&lt;std::string&gt; file_lists;</span><br><span class="line"></span><br><span class="line">void read_filelists(const std::string&amp; dir_path,std::vector&lt;std::string&gt;&amp; out_filelsits,std::string type)</span><br><span class="line">&#123;</span><br><span class="line">    struct dirent *ptr;</span><br><span class="line">    DIR *dir;</span><br><span class="line">    dir &#x3D; opendir(dir_path.c_str());</span><br><span class="line">    out_filelsits.clear();</span><br><span class="line">    while ((ptr &#x3D; readdir(dir)) !&#x3D; NULL)&#123;</span><br><span class="line">        std::string tmp_file &#x3D; ptr-&gt;d_name;</span><br><span class="line">        if (tmp_file[0] &#x3D;&#x3D; &#39;.&#39;)continue;</span><br><span class="line">        if (type.size() &lt;&#x3D; 0)&#123;</span><br><span class="line">            out_filelsits.push_back(ptr-&gt;d_name);</span><br><span class="line">        &#125;else&#123;</span><br><span class="line">            if (tmp_file.size() &lt; type.size())continue;</span><br><span class="line">            std::string tmp_cut_type &#x3D; tmp_file.substr(tmp_file.size() - type.size(),type.size());</span><br><span class="line">            if (tmp_cut_type &#x3D;&#x3D; type)&#123;</span><br><span class="line">                out_filelsits.push_back(ptr-&gt;d_name);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">bool computePairNum(std::string pair1,std::string pair2)</span><br><span class="line">&#123;</span><br><span class="line">    return pair1 &lt; pair2;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void sort_filelists(std::vector&lt;std::string&gt;&amp; filists,std::string type)</span><br><span class="line">&#123;</span><br><span class="line">    if (filists.empty())return;</span><br><span class="line"></span><br><span class="line">    std::sort(filists.begin(),filists.end(),computePairNum);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void readKittiPclBinData(std::string &amp;in_file, std::string&amp; out_file)</span><br><span class="line">&#123;</span><br><span class="line">    &#x2F;&#x2F; load point cloud</span><br><span class="line">    std::fstream input(in_file.c_str(), std::ios::in | std::ios::binary);</span><br><span class="line">    if(!input.good())&#123;</span><br><span class="line">        std::cerr &lt;&lt; &quot;Could not read file: &quot; &lt;&lt; in_file &lt;&lt; std::endl;</span><br><span class="line">        exit(EXIT_FAILURE);</span><br><span class="line">    &#125;</span><br><span class="line">    input.seekg(0, std::ios::beg);</span><br><span class="line"></span><br><span class="line">    pcl::PointCloud&lt;pcl::PointXYZI&gt;::Ptr points (new pcl::PointCloud&lt;pcl::PointXYZI&gt;);</span><br><span class="line"></span><br><span class="line">    int i;</span><br><span class="line">    for (i&#x3D;0; input.good() &amp;&amp; !input.eof(); i++) &#123;</span><br><span class="line">        pcl::PointXYZI point;</span><br><span class="line">        input.read((char *) &amp;point.x, 3*sizeof(float));</span><br><span class="line">        input.read((char *) &amp;point.intensity, sizeof(float));</span><br><span class="line">        points-&gt;push_back(point);</span><br><span class="line">    &#125;</span><br><span class="line">    input.close();</span><br><span class="line">&#x2F;&#x2F;    g_cloud_pub.publish( points );</span><br><span class="line"></span><br><span class="line">    std::cout &lt;&lt; &quot;Read KTTI point cloud with &quot; &lt;&lt; i &lt;&lt; &quot; points, writing to &quot; &lt;&lt; out_file &lt;&lt; std::endl;</span><br><span class="line">    pcl::PCDWriter writer;</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F; Save DoN features</span><br><span class="line">    writer.write&lt; pcl::PointXYZI &gt; (out_file, *points, false);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">int main(int argc, char **argv)</span><br><span class="line">&#123;</span><br><span class="line">&#x2F;&#x2F;    ros::init(argc, argv, &quot;ground_remove_test&quot;);</span><br><span class="line">&#x2F;&#x2F;    ros::NodeHandle n;</span><br><span class="line">&#x2F;&#x2F;    g_cloud_pub &#x3D; n.advertise&lt; pcl::PointCloud&lt; pcl::PointXYZI &gt; &gt; (&quot;point_chatter&quot;, 1);</span><br><span class="line"></span><br><span class="line">    std::string bin_path &#x3D; &quot;..&#x2F;velodyne&#x2F;binary&#x2F;&quot;;</span><br><span class="line">    std::string pcd_path &#x3D; &quot;..&#x2F;velodyne&#x2F;pcd&#x2F;&quot;;</span><br><span class="line">    read_filelists( bin_path, file_lists, &quot;bin&quot; );</span><br><span class="line">    sort_filelists( file_lists, &quot;bin&quot; );</span><br><span class="line">    for (int i &#x3D; 0; i &lt; file_lists.size(); ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        std::string bin_file &#x3D; bin_path + file_lists[i];</span><br><span class="line">        std::string tmp_str &#x3D; file_lists[i].substr(0, file_lists[i].length() - 4) + &quot;.pcd&quot;;</span><br><span class="line">        std::string pcd_file &#x3D; pcd_path + tmp_str;</span><br><span class="line">        readKittiPclBinData( bin_file, pcd_file );</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><p>以上。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;官网数据集说明：&lt;a href=&quot;http://www.cvlibs.net/datasets/kitti/raw_data.php&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://www.cvlibs.net/datasets/kitti/raw_data.php&lt;/a&gt;&lt;br&gt;数据集详细说明论文：&lt;a href=&quot;http://www.cvlibs.net/publications/Geiger2013IJRR.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://www.cvlibs.net/publications/Geiger2013IJRR.pdf&lt;/a&gt;&lt;br&gt;KITTI的激光雷达型号为 &lt;strong&gt;Velodyne HDL-64E&lt;/strong&gt; ，
    
    </summary>
    
    
      <category term="ROS" scheme="http://zengzeyu.com/categories/ROS/"/>
    
    
      <category term="ROS" scheme="http://zengzeyu.com/tags/ROS/"/>
    
      <category term="PCL" scheme="http://zengzeyu.com/tags/PCL/"/>
    
  </entry>
  
</feed>
