<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[cnpy 库使用笔记]]></title>
    <url>%2F2018%2F03%2F15%2F18_3_15%2Fcnpy_note%2F</url>
    <content type="text"><![CDATA[前言 在进行网络训练过程中，在生成训练数据时，一般会使用比较底层的传感器来生成数据，如摄像头或雷达，所以大部分使用C++进行开发。为了将数据转为Numpy Array格式供Python调用，cnpy库就是供C++生成这种格式数据的开源库，由国外一名小哥开发。cnpy地址：https://github.com/rogersce/cnpy官方例子：https://github.com/rogersce/cnpy/blob/master/example1.cpp 应用方法 cnpy有两种应用方法： 官网方法：将cnpy库加入到ubuntu系统环境中，当做系统库进行调用，类似于安装好的OpenCV库； ROS package 方法：将cnpy库包含到ROS工作空间下，当做独立的package供调用。 方法1： 按照官网安装教程安装即可： Installation:Default installation directory is /usr/local. To specify a different directory, add -DCMAKE_INSTALL_PREFIX=/path/to/install/dir to the cmake invocation in step 4.1234561. get [cmake](https://github.com/rogersce/cnpy/blob/master/www.cmake.org)2. create a build directory, say $HOME/build3. cd $HOME/build4. cmake /path/to/cnpy5. make6. make install Using:To use, #include&quot;cnpy.h&quot; in your source code. Compile the source code mycode.cpp as1g++ -o mycode mycode.cpp -L/path/to/install/dir -lcnpy -lz --std=c++11 方法2： 将下载好的cnpy文件夹放到与调用cnpy库的 package A 同级目录下，并将以下内容添加到A的package.xml中： 12&lt;build_depend&gt;cnpy&lt;/build_depend&gt; &lt;run_depend&gt;cnpy&lt;/run_depend&gt; 然后以下内容添加到 A 的Cmakelist.txt中： 12345find_package( cnpy )catkin_package( CATKIN_DEPENDS cnpy ) 在 A 中调用的xx.h中按照路径包含即可： 1#include &quot;../../../cnpy/include/cnpy/cnpy.h&quot; 此方法也适用于任何其他想要调用的package，按照上书步骤操作即可。 以上。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>NumPy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[c++ string 操作汇总]]></title>
    <url>%2F2018%2F03%2F15%2F18_3_15%2Fcpp_string_operation%2F</url>
    <content type="text"><![CDATA[前言 作为传递信息的载体string数据类型广泛应用于各种编程语言中，尤其在轻量化语言 Python 中发挥的淋漓尽致，通过string传递信息 Python 使各个模块组装在一起完成指定的工作任务，这也是 Python 被称为“胶水语言”的原因。本文着眼于 c++ 中string的应用。首先建议先浏览string在线文档，这其中包含了大多数日常所需函数。 正文 1. string 内查找字符串strstd::string::find(str)函数：http://www.cplusplus.com/reference/string/string/find/Example1234567891011121314151617181920212223242526272829303132// string::find#include &lt;iostream&gt; // std::cout#include &lt;string&gt; // std::stringint main ()&#123; std::string str (&quot;There are two needles in this haystack with needles.&quot;); std::string str2 (&quot;needle&quot;); // different member versions of find in the same order as above: std::size_t found = str.find(str2); if (found!=std::string::npos) std::cout &lt;&lt; &quot;first &apos;needle&apos; found at: &quot; &lt;&lt; found &lt;&lt; &apos;\n&apos;; found=str.find(&quot;needles are small&quot;,found+1,6); if (found!=std::string::npos) std::cout &lt;&lt; &quot;second &apos;needle&apos; found at: &quot; &lt;&lt; found &lt;&lt; &apos;\n&apos;; found=str.find(&quot;haystack&quot;); if (found!=std::string::npos) std::cout &lt;&lt; &quot;&apos;haystack&apos; also found at: &quot; &lt;&lt; found &lt;&lt; &apos;\n&apos;; found=str.find(&apos;.&apos;); if (found!=std::string::npos) std::cout &lt;&lt; &quot;Period found at: &quot; &lt;&lt; found &lt;&lt; &apos;\n&apos;; // let&apos;s replace the first needle: str.replace(str.find(str2),str2.length(),&quot;preposition&quot;); std::cout &lt;&lt; str &lt;&lt; &apos;\n&apos;; return 0;&#125; 其中 npos 值为 -1，用于判断。其他查找功能类函数： rfind： 找到目标str最后一次出现位置 find_first_of： 从起始位置查找目标str find_last_of： 从结束位置往回查找目标str 2. string内删除目标str substr： 本质还是查找目标strExample12345678910111213141516171819// string::substr#include &lt;iostream&gt;#include &lt;string&gt;int main ()&#123; std::string str=&quot;We think in generalities, but we live in details.&quot;; // (quoting Alfred N. Whitehead) std::string str2 = str.substr (3,5); // &quot;think&quot; std::size_t pos = str.find(&quot;live&quot;); // position of &quot;live&quot; in str std::string str3 = str.substr (pos); // get from &quot;live&quot; to the end std::cout &lt;&lt; str2 &lt;&lt; &apos; &apos; &lt;&lt; str3 &lt;&lt; &apos;\n&apos;; return 0;&#125; 3. 字符串分割字符串分割推荐使用 std::stringstream 作为中间载体和 getline() 函数组合来完成。以分割字符 “ ; ”为例：12345678910void splitPathStr(std::string&amp; path_str)&#123; std::vector&lt;std::string&gt; filelists; std::stringstream sstr( path_str ); std::string token; while(getline(sstr, token, &apos;;&apos;)) &#123; filelists.push_back(token); &#125;&#125; 4. int型与string型互相转换int型转string型123456void int2str(const int &amp;int_temp,string &amp;string_temp) &#123; stringstream stream; stream&lt;&lt;int_temp; string_temp=stream.str(); //此处也可以用 stream&gt;&gt;string_temp &#125; string型转int型12345void str2int(int &amp;int_temp,const string &amp;string_temp) &#123; stringstream stream(string_temp); stream&gt;&gt;int_temp; &#125; 未完待续… 以上。 参考文献： c++ reference C++中int、string等常见类型转换]]></content>
      <categories>
        <category>C++</category>
      </categories>
      <tags>
        <tag>c++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[KITTI 原始bin格式数据集转PCD格式]]></title>
    <url>%2F2018%2F03%2F15%2F18_3_15%2Fkitti_bin_to_pcd%2F</url>
    <content type="text"><![CDATA[前言 官网数据集说明：http://www.cvlibs.net/datasets/kitti/raw_data.php数据集详细说明论文：http://www.cvlibs.net/publications/Geiger2013IJRR.pdfKITTI的激光雷达型号为 Velodyne HDL-64E ，具体信息如下：123456789Velodyne HDL-64E rotating 3D laser scanner- 10 Hz- 64 beams- 0.09 degree angular resolution- 2 cm distanceaccuracy- collecting∼1.3 million points/second- field of view: 360°- horizontal, 26.8°- vertical, range: 120 m 针对激光雷达点云数据集使用的信息在 KITTI_README.TXT 中有详细说明，文件下载地址：Code to use the KITTI data set with PCL123456789101112131415161718192021222324252627282930313233343536373839Velodyne 3D laser scan data===========================The velodyne point clouds are stored in the folder &apos;velodyne_points&apos;. Tosave space, all scans have been stored as Nx4 float matrix into a binaryfile using the following code: stream = fopen (dst_file.c_str(),&quot;wb&quot;); fwrite(data,sizeof(float),4*num,stream); fclose(stream);Here, data contains 4*num values, where the first 3 values correspond tox,y and z, and the last value is the reflectance information. All scansare stored row-aligned, meaning that the first 4 values correspond to thefirst measurement. Since each scan might potentially have a differentnumber of points, this must be determined from the file size when readingthe file, where 1e6 is a good enough upper bound on the number of values: // allocate 4 MB buffer (only ~130*4*4 KB are needed) int32_t num = 1000000; float *data = (float*)malloc(num*sizeof(float)); // pointers float *px = data+0; float *py = data+1; float *pz = data+2; float *pr = data+3; // load point cloud FILE *stream; stream = fopen (currFilenameBinary.c_str(),&quot;rb&quot;); num = fread(data,sizeof(float),num,stream)/4; for (int32_t i=0; i&lt;num; i++) &#123; point_cloud.points.push_back(tPoint(*px,*py,*pz,*pr)); px+=4; py+=4; pz+=4; pr+=4; &#125; fclose(stream);x,y and y are stored in metric (m) Velodyne coordinates. KITTI点云数据集读取与转换 官方源代码解读Code to use the KITTI data set with PCL下载的源代码文件夹中的src/kitti2pcd.cpp 中这个函数：12345678910111213141516171819202122232425262728void readKittiPclBinData(std::string &amp;in_file, std::string&amp; out_file)&#123; // load point cloud std::fstream input(in_file.c_str(), std::ios::in | std::ios::binary); if(!input.good())&#123; std::cerr &lt;&lt; &quot;Could not read file: &quot; &lt;&lt; in_file &lt;&lt; std::endl; exit(EXIT_FAILURE); &#125; input.seekg(0, std::ios::beg); pcl::PointCloud&lt;pcl::PointXYZI&gt;::Ptr points (new pcl::PointCloud&lt;pcl::PointXYZI&gt;); int i; for (i=0; input.good() &amp;&amp; !input.eof(); i++) &#123; pcl::PointXYZI point; input.read((char *) &amp;point.x, 3*sizeof(float)); input.read((char *) &amp;point.intensity, sizeof(float)); points-&gt;push_back(point); &#125; input.close();// g_cloud_pub.publish( points ); std::cout &lt;&lt; &quot;Read KTTI point cloud with &quot; &lt;&lt; i &lt;&lt; &quot; points, writing to &quot; &lt;&lt; out_file &lt;&lt; std::endl; pcl::PCDWriter writer; // Save DoN features writer.write&lt; pcl::PointXYZI &gt; (out_file, *points, false);&#125; 这个函数是最重要的从 KITTI 中读取 .bin 文件转 .pcd 文件。 可运行完整代码下面贴本人完整代码，代码功能： 读取文件夹下.bin 文件 按照文件名进行排序（虽然默认已经排好序） 转为.pcd 文件，并保存 发送到 rviz 进行显示（可选）12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394//// Created by zzy on 3/14/18.//#include &lt;ctime&gt;#include &quot;ros/ros.h&quot;#include &quot;fcn_data_gen/ground_remove.h&quot;static ros::Publisher g_cloud_pub;static std::vector&lt;std::string&gt; file_lists;void read_filelists(const std::string&amp; dir_path,std::vector&lt;std::string&gt;&amp; out_filelsits,std::string type)&#123; struct dirent *ptr; DIR *dir; dir = opendir(dir_path.c_str()); out_filelsits.clear(); while ((ptr = readdir(dir)) != NULL)&#123; std::string tmp_file = ptr-&gt;d_name; if (tmp_file[0] == &apos;.&apos;)continue; if (type.size() &lt;= 0)&#123; out_filelsits.push_back(ptr-&gt;d_name); &#125;else&#123; if (tmp_file.size() &lt; type.size())continue; std::string tmp_cut_type = tmp_file.substr(tmp_file.size() - type.size(),type.size()); if (tmp_cut_type == type)&#123; out_filelsits.push_back(ptr-&gt;d_name); &#125; &#125; &#125;&#125;bool computePairNum(std::string pair1,std::string pair2)&#123; return pair1 &lt; pair2;&#125;void sort_filelists(std::vector&lt;std::string&gt;&amp; filists,std::string type)&#123; if (filists.empty())return; std::sort(filists.begin(),filists.end(),computePairNum);&#125;void readKittiPclBinData(std::string &amp;in_file, std::string&amp; out_file)&#123; // load point cloud std::fstream input(in_file.c_str(), std::ios::in | std::ios::binary); if(!input.good())&#123; std::cerr &lt;&lt; &quot;Could not read file: &quot; &lt;&lt; in_file &lt;&lt; std::endl; exit(EXIT_FAILURE); &#125; input.seekg(0, std::ios::beg); pcl::PointCloud&lt;pcl::PointXYZI&gt;::Ptr points (new pcl::PointCloud&lt;pcl::PointXYZI&gt;); int i; for (i=0; input.good() &amp;&amp; !input.eof(); i++) &#123; pcl::PointXYZI point; input.read((char *) &amp;point.x, 3*sizeof(float)); input.read((char *) &amp;point.intensity, sizeof(float)); points-&gt;push_back(point); &#125; input.close();// g_cloud_pub.publish( points ); std::cout &lt;&lt; &quot;Read KTTI point cloud with &quot; &lt;&lt; i &lt;&lt; &quot; points, writing to &quot; &lt;&lt; out_file &lt;&lt; std::endl; pcl::PCDWriter writer; // Save DoN features writer.write&lt; pcl::PointXYZI &gt; (out_file, *points, false);&#125;int main(int argc, char **argv)&#123;// ros::init(argc, argv, &quot;ground_remove_test&quot;);// ros::NodeHandle n;// g_cloud_pub = n.advertise&lt; pcl::PointCloud&lt; pcl::PointXYZI &gt; &gt; (&quot;point_chatter&quot;, 1); std::string bin_path = &quot;../velodyne/binary/&quot;; std::string pcd_path = &quot;../velodyne/pcd/&quot;; read_filelists( bin_path, file_lists, &quot;bin&quot; ); sort_filelists( file_lists, &quot;bin&quot; ); for (int i = 0; i &lt; file_lists.size(); ++i) &#123; std::string bin_file = bin_path + file_lists[i]; std::string tmp_str = file_lists[i].substr(0, file_lists[i].length() - 4) + &quot;.pcd&quot;; std::string pcd_file = pcd_path + tmp_str; readKittiPclBinData( bin_file, pcd_file ); &#125; return 0;&#125; 以上。]]></content>
      <categories>
        <category>ROS</category>
      </categories>
      <tags>
        <tag>ROS</tag>
        <tag>PCL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[FCN-PCL 应用解析]]></title>
    <url>%2F2018%2F03%2F15%2F18_3_15%2Ffcn_alexnet_pcl%2F</url>
    <content type="text"><![CDATA[前言 FCN(fully convolutional networks， 全卷积神经网络)的图片语义分割（semantic segmentation）论文：Fully Convolutional Networks for Semantic Segmentation。全卷积网络首现于这篇文章。这篇文章是将CNN结构应用到图像语义分割领域并取得突出结果的开山之作，因而拿到了CVPR 2015年的Best paper honorable mention。图像语义分割，简而言之就是对一张图片上的所有像素点进行分类。如下图就是一个语义分割例子，不同颜色像素代表不同类别： UCB的FCN源码Github地址：https://github.com/shelhamer/fcn.berkeleyvision.org源码中一共包含了4种网络结构模型：nyud-fcn、pascalcontext-fcn、siftflow-fcn、voc-fcn。每一种网络结构根据提取卷积层不同，又分了3-4个不等的网络类别。工作中个人的数据类型和格式不一定与voc-fcn-alexnet源代码提供的数据接口相同或类似（图片），如本文接下来要输入网络模型的数据类型为由激光雷达（LiDAR）扫描得到的点云数据（.pcd），那么如何进行实际操作呢？下面一步一步进行。 1. 激光雷达数据转换 1.1 激光雷达点云数据介绍首先介绍机械式旋转激光雷达生成的数据格式，激光雷达内部电机以一定角速度旋转，通过固定于其上的激光发射器和激光接收器测量激光雷达到障碍物的距离。以速腾聚创公司生产的16线激光雷达RS-LiDAR-16为例，每秒进行10次360°旋转(10Hz)，每次旋转扫描得到周围场景的信息，每一线激光旋转一周得到2016个点，储存在 .pcd 格式文件中。以二维彩色图像的方式（如.png）来理解.pcd文件，16线代表图片高度，2016代表图片宽度，一共16x2016=32256个像素点。每个点 point 的数据有[x, y, z, intensity]，与二维图片中的RGB通道（RGB chanel）是同样的道理，每一个数据代表一个通道。 1.2 点云预处理根据点云数据特征属性对其进行预处理，每个 point 的处理后特征有[row, column, height, range, mark]分别代表 point 的：[行序号， 列标号， 高度， 距离， 属性]，其中 height 与 z 值相等，range 由 sqrt(x^2 + y^2 + z^2) 计算得出， mark 为通过决策树（Decision tree）方式对 point 进行分类得到属性：障碍物点（obstacle mark）或地面点（ground mark），与ground true图片道理相同，作为训练预测分类的结果参考标准用于计算loss。这里作用相当于，人工添加了更多的特征通道，方便进行分类和预测。以上预处理得到的数据通过cnpy库转换为 .npy 格式的二进制文件，方便NumPy对数据进行读取，cnpy库使用教程请移步：cnpy库使用笔记以及官方example。每一帧点云数据储存为一个 .npy 格式文件，命名方式越简单越好，方便读取排序，本文直接以序号作为文件名[0.npy, 1.npy, …, n.npy ]。 2. FCN-AlexNet的点云数据分类任务 FCN-AlexNet的点云数据分类任务工程包含： 5个Python文件： pcl_data_layer.py， net.py， solver.py， surgery.py， score.py 3个prototxt文件: train.prototxt， val.prototxt， solver.prototxt 1个caffe_model文件： fcn-alexnet-pascal.caffemodel 2.1 FCN-AlexNet读取数据层（Data layer）文件命名为pcl_data_layer.py，该文件内包含class PCLSegDataLayer()类函数： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758import caffeimport numpy as npimport randomimport osclass PCLSegDataLayer(caffe.Layer): def setup(self, bottom, top): params = eval(self.param_str) self.npy_dir = params[&quot;pcl_dir&quot;] self.list_name = list() # two tops: data and label if len(top) != 2: raise Exception(&quot;Need to define two tops: data and label.&quot;) # data layers have no bottoms if len(bottom) != 0: raise Exception(&quot;Do not define a bottom.&quot;) self.load_file_name( self.npy_dir, self.list_name ) self.idx = 0 def reshape(self, bottom, top): self.data, self.label = self.load_file( self.idx ) # reshape tops to fit (leading 1 is for batch dimension) top[0].reshape(1, *self.data.shape) top[1].reshape(1, *self.label.shape) def forward(self, bottom, top): # assign output top[0].data[...] = self.data top[1].data[...] = self.label # pick next input self.idx += 1 if self.idx == len(self.list_name): self.idx = 0 def backward(self, top, propagate_down, bottom): pass def load_file(self, idx): in_file = np.load(self.list_name[idx]) #[mark, row, col, height, range] in_data = in_file[:,:,1:-1] in_data = in_data.transpose((2, 0, 1)) in_label = in_file[:,:,0] return in_data, in_label def load_file_name(self, path, list_name): for file in os.listdir(path): file_path = os.path.join(path, file) if os.path.isdir(file_path): os.listdir(file_path, list_name) else: list_name.append(file_path) setup()： 建立类时的参数 reshape()： 根据输入调整模型入口大小 forward()： 前向传播，由于是数据输入层，所以输出为原点云数据及其分类label backward()： 后向传播，数据层没有后向传播，所以舍弃 load_file_name()： 读取指定文件夹内 .npy 格式文件并储存如列表list load_file()： 载入单个.npy 文件，并按照储存顺序对属性进行分类，输出data和label 2.2 FCN-AlexNet模型定义函数（net.py）net.py文件用于生成net.prototxt文件，其定义了整个模型的结构和模型每层的各个参数。当然，模型网络结构可以利用官方已经训练好的fcn-alexnet-pascal.caffemodel来导出，也可以使用net.py自己生成，为了简化操作，本文使用fcn-alexnet-pascal.caffemodel来导出模型网络结构。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162import syssys.path.append(&apos;../../python&apos;)import caffefrom caffe import layers as L, params as Pfrom caffe.coord_map import cropdef conv_relu(bottom, ks, nout, stride=1, pad=0, group=1): conv = L.Convolution(bottom, kernel_size=ks, stride=stride, num_output=nout, pad=pad, group=group) return conv, L.ReLU(conv, in_place=True)def max_pool(bottom, ks, stride=1): return L.Pooling(bottom, pool=P.Pooling.MAX, kernel_size=ks, stride=stride)def fcn(split): n = caffe.NetSpec() pydata_params = dict() pydata_params[&apos;pcl_dir&apos;] = &apos;../fcn_data_gen/data/npy&apos; #.npy files path pylayer = &apos;PCLSegDataLayer&apos; n.data, n.label = L.Python(module=&apos;pcl_data_layer&apos;, layer=pylayer, ntop=2, param_str=str(pydata_params)) # the base net n.conv1, n.relu1 = conv_relu(n.data, 11, 96, stride=4, pad=100) n.pool1 = max_pool(n.relu1, 3, stride=2) n.norm1 = L.LRN(n.pool1, local_size=5, alpha=1e-4, beta=0.75) n.conv2, n.relu2 = conv_relu(n.norm1, 5, 256, pad=2, group=2) n.pool2 = max_pool(n.relu2, 3, stride=2) n.norm2 = L.LRN(n.pool2, local_size=5, alpha=1e-4, beta=0.75) n.conv3, n.relu3 = conv_relu(n.norm2, 3, 384, pad=1) n.conv4, n.relu4 = conv_relu(n.relu3, 3, 384, pad=1, group=2) n.conv5, n.relu5 = conv_relu(n.relu4, 3, 256, pad=1, group=2) n.pool5 = max_pool(n.relu5, 3, stride=2) # fully conv n.fc6, n.relu6 = conv_relu(n.pool5, 6, 4096) n.drop6 = L.Dropout(n.relu6, dropout_ratio=0.5, in_place=True) n.fc7, n.relu7 = conv_relu(n.drop6, 1, 4096) n.drop7 = L.Dropout(n.relu7, dropout_ratio=0.5, in_place=True) n.score_fr = L.Convolution(n.drop7, num_output=21, kernel_size=1, pad=0, param=[dict(lr_mult=1, decay_mult=1), dict(lr_mult=2, decay_mult=0)]) n.upscore = L.Deconvolution(n.score_fr, convolution_param=dict(num_output=21, kernel_size=63, stride=32, bias_term=False), param=[dict(lr_mult=0)]) n.score = crop(n.upscore, n.data) n.loss = L.SoftmaxWithLoss(n.score, n.label, loss_param=dict(normalize=True, ignore_label=255)) return n.to_proto()def make_net(): with open(&apos;train.prototxt&apos;, &apos;w&apos;) as f: f.write(str(fcn(&apos;train&apos;))) with open(&apos;val.prototxt&apos;, &apos;w&apos;) as f: f.write(str(fcn(&apos;seg11valid&apos;)))if __name__ == &apos;__main__&apos;: make_net() conv_relu()： 定义卷积层输入参数 max_pool()： 定义池化层输入参数 fcn()： 定义模型网络结构 fcn()模型结构详解这里建议结合AlexNet原论文ImageNet Classification with Deep Convolutional Neural Networks一起看，并参考AlexNet模型结构图例来进行比较好理解每个参数的意义。 (1). 数据输入层123456n = caffe.NetSpec()pydata_params = dict()pydata_params[&apos;pcl_dir&apos;] = &apos;../fcn_data_gen/data/npy&apos; #.npy files pathpylayer = &apos;PCLSegDataLayer&apos;n.data, n.label = L.Python(module=&apos;pcl_data_layer&apos;, layer=pylayer, ntop=2, param_str=str(pydata_params)) 找到pcl_data_layer.py文件中的PCLSegDataLayer函数，使用该类处理数据方式作为模型数据输入层函数。 (2). 第一个卷积层123n.conv1, n.relu1 = conv_relu(n.data, 11, 96, stride=4, pad=100)n.pool1 = max_pool(n.relu1, 3, stride=2)n.norm1 = L.LRN(n.pool1, local_size=5, alpha=1e-4, beta=0.75) 关于为何pad=100，此文中有详细解释：FCN学习:Semantic Segmentation (3). 第二个卷积层123n.conv2, n.relu2 = conv_relu(n.norm1, 5, 256, pad=2, group=2)n.pool2 = max_pool(n.relu2, 3, stride=2)n.norm2 = L.LRN(n.pool2, local_size=5, alpha=1e-4, beta=0.75) (4). 第三个卷积层1n.conv3, n.relu3 = conv_relu(n.norm2, 3, 384, pad=1) (5). 第四个卷积层1n.conv4, n.relu4 = conv_relu(n.relu3, 3, 384, pad=1, group=2) (6). 第五个卷积层12n.conv5, n.relu5 = conv_relu(n.relu4, 3, 256, pad=1, group=2)n.pool5 = max_pool(n.relu5, 3, stride=2) (7). 第六个全连接层12n.fc6, n.relu6 = conv_relu(n.pool5, 6, 4096)n.drop6 = L.Dropout(n.relu6, dropout_ratio=0.5, in_place=True) (8). 第七个全连接层12n.fc7, n.relu7 = conv_relu(n.drop6, 1, 4096)n.drop7 = L.Dropout(n.relu7, dropout_ratio=0.5, in_place=True) (9). 第八个全连接层123456789n.score_fr = L.Convolution(n.drop7, num_output=21, kernel_size=1, pad=0, param=[dict(lr_mult=1, decay_mult=1), dict(lr_mult=2, decay_mult=0)])n.upscore = L.Deconvolution(n.score_fr, convolution_param=dict(num_output=21, kernel_size=63, stride=32, bias_term=False), param=[dict(lr_mult=0)])n.score = crop(n.upscore, n.data)n.loss = L.SoftmaxWithLoss(n.score, n.label, loss_param=dict(normalize=True, ignore_label=255)) 2.3 FCN-AlexNet求解函数（solve.py）solve.py 文件是整个模型的入口，它整合各个文件，输入外部参数，对结果进行求解并输出。由 solve.py 生成的 solver.prototxt 文件定义了求解函数的结构。1234567891011121314151617181920212223242526272829303132import caffeimport surgery, scoreimport numpy as npimport osimport systry: import setproctitle setproctitle.setproctitle(os.path.basename(os.getcwd()))except: passweights = &apos;../ilsvrc-nets/fcn-alexnet-pascal.caffemodel&apos;# init# caffe.set_device(int(sys.argv[0]))# caffe.set_mode_gpu()solver = caffe.SGDSolver(&apos;solver.prototxt&apos;)solver.net.copy_from(weights)# surgeriesinterp_layers = [k for k in solver.net.params.keys() if &apos;up&apos; in k]surgery.interp(solver.net, interp_layers)# scoringval = np.loadtxt(&apos;../data/pascal/seg11valid.txt&apos;, dtype=str)for _ in range(25): solver.step(4000) score.seg_tests(solver, False, val, layer=&apos;score&apos;) weights = &#39;../ilsvrc-nets/fcn-alexnet-pascal.caffemodel&#39;： 导入训练好的模型，可在[Netscope]中输入net.prototxt来进行网络结构可视化 # caffe.set_device(int(sys.argv[0]))# caffe.set_mode_gpu()： 设置gpu来进行训练，本人电脑使用gpu报错，所以没有使用 solver = caffe.SGDSolver(&#39;solver.prototxt&#39;)solver.net.copy_from(weights)：设置求解器模型 # surgeries： （待补充） # scoring ： （待补充） 3. 点云分割试验结果 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798991001011021031041051061071081091101111121131141151161171181191201211221231241251261271281291301311321331341351361371381391401411421431441451461471481491501511521531541551561571581591601611621631641651661671681691701711721731741751761771781791801811821831841851861871881891901911921931941951961971981992002012022032042052062072082092102112122132142152162172182192202212222232242252262272282292302312322332342352362372382392402412422432442452462472482492502512522532542552562572582592602612622632642652662672682692702712722732742752762772782792802812822832842852862872882892902912922932942952962972982993003013023033043053063073083093103113123133143153163173183193203213223233243253263273283293303313323333343353363373383393403413423433443453463473483493503513523533543553563573583593603613623633643653663673683693703713723733743753763773783793803813823833843853863873883893903913923933943953963973983994004014024034044054064074084094104114124134144154164174184194204214224234244254264274284294304314324334344354364374384394404414424434444454464474484494504514524534544554564574584594604614624634644654664674684694704714724734744754764774784794804814824834844854864874884894904914924934944954964974984995005015025035045055065075085095105115125135145155165175185195205215225235245255265275285295305315325335345355365375385395405415425435445455465475485495505515525535545555565575585595605615625635645655665675685695705715725735745755765775785795805815825835845855865875885895905915925935945955965975985996006016026036046056066076086096106116126136146156166176186196206216226236246256266276286296306316326336346356366376386396406416426436446456466476486496506516526536546556566576586596606616626636646656666676686696706716726736746756766776786796806816826836846856866876886896906916926936946956966976986997007017027037047057067077087097107117127137147157167177187197207217227237247257267277287297307317327337347357367377387397407417427437447457467477487497507517527537547557567577587597607617627637647657667677687697707717727737747757767777787797807817827837847857867877887897907917927937947957967977987998008018028038048058068078088098108118128138148158168178188198208218228238248258268278288298308318328338348358368378388398408418428438448458468478488498508518528538548558568578588598608618628638648658668678688698708718728738748758768778788798808818828838848858868878888898908918928938948958968978988999009019029039049059069079089099109119129139149159169179189199209219229239249259269279289299309319329339349359369379389399409419429439449459469479489499509519529539549559569579589599609619629639649659669679689699709719729739749759769779789799809819829839849859869879889899909919929939949959969979989991000100110021003100410051006100710081009101010111012101310141015101610171018101910201021102210231024102510261027102810291030103110321033103410351036103710381039104010411042104310441045104610471048104910501051105210531054105510561057105810591060106110621063106410651066106710681069107010711072107310741075107610771078pydev debugger: process 9249 is connectingConnected to pydev debugger (build 173.4301.16)WARNING: Logging before InitGoogleLogging() is written to STDERRI0313 11:41:39.369604 9249 solver.cpp:45] Initializing solver from parameters: train_net: &quot;train.prototxt&quot;test_net: &quot;val.prototxt&quot;test_iter: 736test_interval: 999999999base_lr: 0.0001display: 20max_iter: 100000lr_policy: &quot;fixed&quot;momentum: 0.9weight_decay: 0.0005snapshot: 4000snapshot_prefix: &quot;snapshot/train&quot;test_initialization: falseaverage_loss: 20iter_size: 20I0313 11:41:39.369671 9249 solver.cpp:92] Creating training net from train_net file: train.prototxtI0313 11:41:39.370101 9249 net.cpp:51] Initializing net from parameters: state &#123; phase: TRAIN&#125;layer &#123; name: &quot;data&quot; type: &quot;Python&quot; top: &quot;data&quot; top: &quot;label&quot; python_param &#123; module: &quot;pcl_data_layer&quot; layer: &quot;PCLSegDataLayer&quot; param_str: &quot;&#123;\&apos;pcl_dir\&apos;: \&apos;/home/zzy/CLionProjects/ROS_Project/ws/src/fcn_data_gen/data/npy\&apos;&#125;&quot; &#125;&#125;layer &#123; name: &quot;conv1&quot; type: &quot;Convolution&quot; bottom: &quot;data&quot; top: &quot;conv1&quot; convolution_param &#123; num_output: 96 pad: 100 kernel_size: 11 group: 1 stride: 4 &#125;&#125;layer &#123; name: &quot;relu1&quot; type: &quot;ReLU&quot; bottom: &quot;conv1&quot; top: &quot;conv1&quot;&#125;layer &#123; name: &quot;pool1&quot; type: &quot;Pooling&quot; bottom: &quot;conv1&quot; top: &quot;pool1&quot; pooling_param &#123; pool: MAX kernel_size: 3 stride: 2 &#125;&#125;layer &#123; name: &quot;norm1&quot; type: &quot;LRN&quot; bottom: &quot;pool1&quot; top: &quot;norm1&quot; lrn_param &#123; local_size: 5 alpha: 0.0001 beta: 0.75 &#125;&#125;layer &#123; name: &quot;conv2&quot; type: &quot;Convolution&quot; bottom: &quot;norm1&quot; top: &quot;conv2&quot; convolution_param &#123; num_output: 256 pad: 2 kernel_size: 5 group: 2 stride: 1 &#125;&#125;layer &#123; name: &quot;relu2&quot; type: &quot;ReLU&quot; bottom: &quot;conv2&quot; top: &quot;conv2&quot;&#125;layer &#123; name: &quot;pool2&quot; type: &quot;Pooling&quot; bottom: &quot;conv2&quot; top: &quot;pool2&quot; pooling_param &#123; pool: MAX kernel_size: 3 stride: 2 &#125;&#125;layer &#123; name: &quot;norm2&quot; type: &quot;LRN&quot; bottom: &quot;pool2&quot; top: &quot;norm2&quot; lrn_param &#123; local_size: 5 alpha: 0.0001 beta: 0.75 &#125;&#125;layer &#123; name: &quot;conv3&quot; type: &quot;Convolution&quot; bottom: &quot;norm2&quot; top: &quot;conv3&quot; convolution_param &#123; num_output: 384 pad: 1 kernel_size: 3 group: 1 stride: 1 &#125;&#125;layer &#123; name: &quot;relu3&quot; type: &quot;ReLU&quot; bottom: &quot;conv3&quot; top: &quot;conv3&quot;&#125;layer &#123; name: &quot;conv4&quot; type: &quot;Convolution&quot; bottom: &quot;conv3&quot; top: &quot;conv4&quot; convolution_param &#123; num_output: 384 pad: 1 kernel_size: 3 group: 2 stride: 1 &#125;&#125;layer &#123; name: &quot;relu4&quot; type: &quot;ReLU&quot; bottom: &quot;conv4&quot; top: &quot;conv4&quot;&#125;layer &#123; name: &quot;conv5&quot; type: &quot;Convolution&quot; bottom: &quot;conv4&quot; top: &quot;conv5&quot; convolution_param &#123; num_output: 256 pad: 1 kernel_size: 3 group: 2 stride: 1 &#125;&#125;layer &#123; name: &quot;relu5&quot; type: &quot;ReLU&quot; bottom: &quot;conv5&quot; top: &quot;conv5&quot;&#125;layer &#123; name: &quot;pool5&quot; type: &quot;Pooling&quot; bottom: &quot;conv5&quot; top: &quot;pool5&quot; pooling_param &#123; pool: MAX kernel_size: 3 stride: 2 &#125;&#125;layer &#123; name: &quot;fc6&quot; type: &quot;Convolution&quot; bottom: &quot;pool5&quot; top: &quot;fc6&quot; convolution_param &#123; num_output: 4096 pad: 0 kernel_size: 6 group: 1 stride: 1 &#125;&#125;layer &#123; name: &quot;relu6&quot; type: &quot;ReLU&quot; bottom: &quot;fc6&quot; top: &quot;fc6&quot;&#125;layer &#123; name: &quot;drop6&quot; type: &quot;Dropout&quot; bottom: &quot;fc6&quot; top: &quot;fc6&quot; dropout_param &#123; dropout_ratio: 0.5 &#125;&#125;layer &#123; name: &quot;fc7&quot; type: &quot;Convolution&quot; bottom: &quot;fc6&quot; top: &quot;fc7&quot; convolution_param &#123; num_output: 4096 pad: 0 kernel_size: 1 group: 1 stride: 1 &#125;&#125;layer &#123; name: &quot;relu7&quot; type: &quot;ReLU&quot; bottom: &quot;fc7&quot; top: &quot;fc7&quot;&#125;layer &#123; name: &quot;drop7&quot; type: &quot;Dropout&quot; bottom: &quot;fc7&quot; top: &quot;fc7&quot; dropout_param &#123; dropout_ratio: 0.5 &#125;&#125;layer &#123; name: &quot;score_fr&quot; type: &quot;Convolution&quot; bottom: &quot;fc7&quot; top: &quot;score_fr&quot; param &#123; lr_mult: 1 decay_mult: 1 &#125; param &#123; lr_mult: 2 decay_mult: 0 &#125; convolution_param &#123; num_output: 21 pad: 0 kernel_size: 1 &#125;&#125;layer &#123; name: &quot;upscore&quot; type: &quot;Deconvolution&quot; bottom: &quot;score_fr&quot; top: &quot;upscore&quot; param &#123; lr_mult: 0 &#125; convolution_param &#123; num_output: 21 bias_term: false kernel_size: 63 stride: 32 &#125;&#125;layer &#123; name: &quot;score&quot; type: &quot;Crop&quot; bottom: &quot;upscore&quot; bottom: &quot;data&quot; top: &quot;score&quot; crop_param &#123; axis: 2 offset: 18 &#125;&#125;layer &#123; name: &quot;loss&quot; type: &quot;SoftmaxWithLoss&quot; bottom: &quot;score&quot; bottom: &quot;label&quot; top: &quot;loss&quot; loss_param &#123; ignore_label: 255 normalize: true &#125;&#125;I0313 11:41:39.370163 9249 layer_factory.hpp:77] Creating layer dataI0313 11:41:39.370743 9249 net.cpp:84] Creating Layer dataI0313 11:41:39.370753 9249 net.cpp:380] data -&gt; dataI0313 11:41:39.370759 9249 net.cpp:380] data -&gt; labelI0313 11:41:39.372340 9249 net.cpp:122] Setting up dataI0313 11:41:39.372354 9249 net.cpp:129] Top shape: 1 3 16 2016 (96768)I0313 11:41:39.372357 9249 net.cpp:129] Top shape: 1 16 2016 (32256)I0313 11:41:39.372360 9249 net.cpp:137] Memory required for data: 516096I0313 11:41:39.372364 9249 layer_factory.hpp:77] Creating layer data_data_0_splitI0313 11:41:39.372370 9249 net.cpp:84] Creating Layer data_data_0_splitI0313 11:41:39.372372 9249 net.cpp:406] data_data_0_split &lt;- dataI0313 11:41:39.372376 9249 net.cpp:380] data_data_0_split -&gt; data_data_0_split_0I0313 11:41:39.372382 9249 net.cpp:380] data_data_0_split -&gt; data_data_0_split_1I0313 11:41:39.372387 9249 net.cpp:122] Setting up data_data_0_splitI0313 11:41:39.372391 9249 net.cpp:129] Top shape: 1 3 16 2016 (96768)I0313 11:41:39.372395 9249 net.cpp:129] Top shape: 1 3 16 2016 (96768)I0313 11:41:39.372397 9249 net.cpp:137] Memory required for data: 1290240I0313 11:41:39.372400 9249 layer_factory.hpp:77] Creating layer conv1I0313 11:41:39.372406 9249 net.cpp:84] Creating Layer conv1I0313 11:41:39.372409 9249 net.cpp:406] conv1 &lt;- data_data_0_split_0I0313 11:41:39.372412 9249 net.cpp:380] conv1 -&gt; conv1I0313 11:41:39.372515 9249 net.cpp:122] Setting up conv1I0313 11:41:39.372521 9249 net.cpp:129] Top shape: 1 96 52 552 (2755584)I0313 11:41:39.372524 9249 net.cpp:137] Memory required for data: 12312576I0313 11:41:39.372531 9249 layer_factory.hpp:77] Creating layer relu1I0313 11:41:39.372535 9249 net.cpp:84] Creating Layer relu1I0313 11:41:39.372539 9249 net.cpp:406] relu1 &lt;- conv1I0313 11:41:39.372541 9249 net.cpp:367] relu1 -&gt; conv1 (in-place)I0313 11:41:39.372546 9249 net.cpp:122] Setting up relu1I0313 11:41:39.372550 9249 net.cpp:129] Top shape: 1 96 52 552 (2755584)I0313 11:41:39.372552 9249 net.cpp:137] Memory required for data: 23334912I0313 11:41:39.372555 9249 layer_factory.hpp:77] Creating layer pool1I0313 11:41:39.372558 9249 net.cpp:84] Creating Layer pool1I0313 11:41:39.372560 9249 net.cpp:406] pool1 &lt;- conv1I0313 11:41:39.372565 9249 net.cpp:380] pool1 -&gt; pool1I0313 11:41:39.372573 9249 net.cpp:122] Setting up pool1I0313 11:41:39.372576 9249 net.cpp:129] Top shape: 1 96 26 276 (688896)I0313 11:41:39.372579 9249 net.cpp:137] Memory required for data: 26090496I0313 11:41:39.372581 9249 layer_factory.hpp:77] Creating layer norm1I0313 11:41:39.372586 9249 net.cpp:84] Creating Layer norm1I0313 11:41:39.372588 9249 net.cpp:406] norm1 &lt;- pool1I0313 11:41:39.372593 9249 net.cpp:380] norm1 -&gt; norm1I0313 11:41:39.372599 9249 net.cpp:122] Setting up norm1I0313 11:41:39.372602 9249 net.cpp:129] Top shape: 1 96 26 276 (688896)I0313 11:41:39.372604 9249 net.cpp:137] Memory required for data: 28846080I0313 11:41:39.372607 9249 layer_factory.hpp:77] Creating layer conv2I0313 11:41:39.372611 9249 net.cpp:84] Creating Layer conv2I0313 11:41:39.372613 9249 net.cpp:406] conv2 &lt;- norm1I0313 11:41:39.372617 9249 net.cpp:380] conv2 -&gt; conv2I0313 11:41:39.373008 9249 net.cpp:122] Setting up conv2I0313 11:41:39.373013 9249 net.cpp:129] Top shape: 1 256 26 276 (1837056)I0313 11:41:39.373015 9249 net.cpp:137] Memory required for data: 36194304I0313 11:41:39.373021 9249 layer_factory.hpp:77] Creating layer relu2I0313 11:41:39.373025 9249 net.cpp:84] Creating Layer relu2I0313 11:41:39.373028 9249 net.cpp:406] relu2 &lt;- conv2I0313 11:41:39.373030 9249 net.cpp:367] relu2 -&gt; conv2 (in-place)I0313 11:41:39.373034 9249 net.cpp:122] Setting up relu2I0313 11:41:39.373039 9249 net.cpp:129] Top shape: 1 256 26 276 (1837056)I0313 11:41:39.373040 9249 net.cpp:137] Memory required for data: 43542528I0313 11:41:39.373042 9249 layer_factory.hpp:77] Creating layer pool2I0313 11:41:39.373046 9249 net.cpp:84] Creating Layer pool2I0313 11:41:39.373049 9249 net.cpp:406] pool2 &lt;- conv2I0313 11:41:39.373052 9249 net.cpp:380] pool2 -&gt; pool2I0313 11:41:39.373057 9249 net.cpp:122] Setting up pool2I0313 11:41:39.373061 9249 net.cpp:129] Top shape: 1 256 13 138 (459264)I0313 11:41:39.373064 9249 net.cpp:137] Memory required for data: 45379584I0313 11:41:39.373065 9249 layer_factory.hpp:77] Creating layer norm2I0313 11:41:39.373070 9249 net.cpp:84] Creating Layer norm2I0313 11:41:39.373071 9249 net.cpp:406] norm2 &lt;- pool2I0313 11:41:39.373075 9249 net.cpp:380] norm2 -&gt; norm2I0313 11:41:39.373080 9249 net.cpp:122] Setting up norm2I0313 11:41:39.373082 9249 net.cpp:129] Top shape: 1 256 13 138 (459264)I0313 11:41:39.373085 9249 net.cpp:137] Memory required for data: 47216640I0313 11:41:39.373087 9249 layer_factory.hpp:77] Creating layer conv3I0313 11:41:39.373091 9249 net.cpp:84] Creating Layer conv3I0313 11:41:39.373093 9249 net.cpp:406] conv3 &lt;- norm2I0313 11:41:39.373096 9249 net.cpp:380] conv3 -&gt; conv3I0313 11:41:39.373900 9249 net.cpp:122] Setting up conv3I0313 11:41:39.373906 9249 net.cpp:129] Top shape: 1 384 13 138 (688896)I0313 11:41:39.373909 9249 net.cpp:137] Memory required for data: 49972224I0313 11:41:39.373914 9249 layer_factory.hpp:77] Creating layer relu3I0313 11:41:39.373919 9249 net.cpp:84] Creating Layer relu3I0313 11:41:39.373921 9249 net.cpp:406] relu3 &lt;- conv3I0313 11:41:39.373924 9249 net.cpp:367] relu3 -&gt; conv3 (in-place)I0313 11:41:39.373929 9249 net.cpp:122] Setting up relu3I0313 11:41:39.373931 9249 net.cpp:129] Top shape: 1 384 13 138 (688896)I0313 11:41:39.373934 9249 net.cpp:137] Memory required for data: 52727808I0313 11:41:39.373936 9249 layer_factory.hpp:77] Creating layer conv4I0313 11:41:39.373941 9249 net.cpp:84] Creating Layer conv4I0313 11:41:39.373944 9249 net.cpp:406] conv4 &lt;- conv3I0313 11:41:39.373947 9249 net.cpp:380] conv4 -&gt; conv4I0313 11:41:39.374778 9249 net.cpp:122] Setting up conv4I0313 11:41:39.374783 9249 net.cpp:129] Top shape: 1 384 13 138 (688896)I0313 11:41:39.374785 9249 net.cpp:137] Memory required for data: 55483392I0313 11:41:39.374789 9249 layer_factory.hpp:77] Creating layer relu4I0313 11:41:39.374794 9249 net.cpp:84] Creating Layer relu4I0313 11:41:39.374795 9249 net.cpp:406] relu4 &lt;- conv4I0313 11:41:39.374800 9249 net.cpp:367] relu4 -&gt; conv4 (in-place)I0313 11:41:39.374804 9249 net.cpp:122] Setting up relu4I0313 11:41:39.374807 9249 net.cpp:129] Top shape: 1 384 13 138 (688896)I0313 11:41:39.374809 9249 net.cpp:137] Memory required for data: 58238976I0313 11:41:39.374811 9249 layer_factory.hpp:77] Creating layer conv5I0313 11:41:39.374816 9249 net.cpp:84] Creating Layer conv5I0313 11:41:39.374819 9249 net.cpp:406] conv5 &lt;- conv4I0313 11:41:39.374824 9249 net.cpp:380] conv5 -&gt; conv5I0313 11:41:39.375376 9249 net.cpp:122] Setting up conv5I0313 11:41:39.375382 9249 net.cpp:129] Top shape: 1 256 13 138 (459264)I0313 11:41:39.375385 9249 net.cpp:137] Memory required for data: 60076032I0313 11:41:39.375392 9249 layer_factory.hpp:77] Creating layer relu5I0313 11:41:39.375397 9249 net.cpp:84] Creating Layer relu5I0313 11:41:39.375399 9249 net.cpp:406] relu5 &lt;- conv5I0313 11:41:39.375402 9249 net.cpp:367] relu5 -&gt; conv5 (in-place)I0313 11:41:39.375406 9249 net.cpp:122] Setting up relu5I0313 11:41:39.375409 9249 net.cpp:129] Top shape: 1 256 13 138 (459264)I0313 11:41:39.375412 9249 net.cpp:137] Memory required for data: 61913088I0313 11:41:39.375414 9249 layer_factory.hpp:77] Creating layer pool5I0313 11:41:39.375421 9249 net.cpp:84] Creating Layer pool5I0313 11:41:39.375422 9249 net.cpp:406] pool5 &lt;- conv5I0313 11:41:39.375425 9249 net.cpp:380] pool5 -&gt; pool5I0313 11:41:39.375432 9249 net.cpp:122] Setting up pool5I0313 11:41:39.375434 9249 net.cpp:129] Top shape: 1 256 6 69 (105984)I0313 11:41:39.375437 9249 net.cpp:137] Memory required for data: 62337024I0313 11:41:39.375439 9249 layer_factory.hpp:77] Creating layer fc6I0313 11:41:39.375444 9249 net.cpp:84] Creating Layer fc6I0313 11:41:39.375447 9249 net.cpp:406] fc6 &lt;- pool5I0313 11:41:39.375452 9249 net.cpp:380] fc6 -&gt; fc6I0313 11:41:39.404399 9249 net.cpp:122] Setting up fc6I0313 11:41:39.404426 9249 net.cpp:129] Top shape: 1 4096 1 64 (262144)I0313 11:41:39.404430 9249 net.cpp:137] Memory required for data: 63385600I0313 11:41:39.404438 9249 layer_factory.hpp:77] Creating layer relu6I0313 11:41:39.404445 9249 net.cpp:84] Creating Layer relu6I0313 11:41:39.404449 9249 net.cpp:406] relu6 &lt;- fc6I0313 11:41:39.404453 9249 net.cpp:367] relu6 -&gt; fc6 (in-place)I0313 11:41:39.404460 9249 net.cpp:122] Setting up relu6I0313 11:41:39.404464 9249 net.cpp:129] Top shape: 1 4096 1 64 (262144)I0313 11:41:39.404466 9249 net.cpp:137] Memory required for data: 64434176I0313 11:41:39.404469 9249 layer_factory.hpp:77] Creating layer drop6I0313 11:41:39.404474 9249 net.cpp:84] Creating Layer drop6I0313 11:41:39.404476 9249 net.cpp:406] drop6 &lt;- fc6I0313 11:41:39.404481 9249 net.cpp:367] drop6 -&gt; fc6 (in-place)I0313 11:41:39.404486 9249 net.cpp:122] Setting up drop6I0313 11:41:39.404489 9249 net.cpp:129] Top shape: 1 4096 1 64 (262144)I0313 11:41:39.404492 9249 net.cpp:137] Memory required for data: 65482752I0313 11:41:39.404495 9249 layer_factory.hpp:77] Creating layer fc7I0313 11:41:39.404500 9249 net.cpp:84] Creating Layer fc7I0313 11:41:39.404503 9249 net.cpp:406] fc7 &lt;- fc6I0313 11:41:39.404506 9249 net.cpp:380] fc7 -&gt; fc7I0313 11:41:39.417629 9249 net.cpp:122] Setting up fc7I0313 11:41:39.417654 9249 net.cpp:129] Top shape: 1 4096 1 64 (262144)I0313 11:41:39.417657 9249 net.cpp:137] Memory required for data: 66531328I0313 11:41:39.417665 9249 layer_factory.hpp:77] Creating layer relu7I0313 11:41:39.417672 9249 net.cpp:84] Creating Layer relu7I0313 11:41:39.417676 9249 net.cpp:406] relu7 &lt;- fc7I0313 11:41:39.417680 9249 net.cpp:367] relu7 -&gt; fc7 (in-place)I0313 11:41:39.417687 9249 net.cpp:122] Setting up relu7I0313 11:41:39.417690 9249 net.cpp:129] Top shape: 1 4096 1 64 (262144)I0313 11:41:39.417693 9249 net.cpp:137] Memory required for data: 67579904I0313 11:41:39.417696 9249 layer_factory.hpp:77] Creating layer drop7I0313 11:41:39.417703 9249 net.cpp:84] Creating Layer drop7I0313 11:41:39.417706 9249 net.cpp:406] drop7 &lt;- fc7I0313 11:41:39.417709 9249 net.cpp:367] drop7 -&gt; fc7 (in-place)I0313 11:41:39.417713 9249 net.cpp:122] Setting up drop7I0313 11:41:39.417716 9249 net.cpp:129] Top shape: 1 4096 1 64 (262144)I0313 11:41:39.417719 9249 net.cpp:137] Memory required for data: 68628480I0313 11:41:39.417721 9249 layer_factory.hpp:77] Creating layer score_frI0313 11:41:39.417727 9249 net.cpp:84] Creating Layer score_frI0313 11:41:39.417729 9249 net.cpp:406] score_fr &lt;- fc7I0313 11:41:39.417734 9249 net.cpp:380] score_fr -&gt; score_frI0313 11:41:39.417858 9249 net.cpp:122] Setting up score_frI0313 11:41:39.417865 9249 net.cpp:129] Top shape: 1 21 1 64 (1344)I0313 11:41:39.417867 9249 net.cpp:137] Memory required for data: 68633856I0313 11:41:39.417871 9249 layer_factory.hpp:77] Creating layer upscoreI0313 11:41:39.417877 9249 net.cpp:84] Creating Layer upscoreI0313 11:41:39.417881 9249 net.cpp:406] upscore &lt;- score_frI0313 11:41:39.417884 9249 net.cpp:380] upscore -&gt; upscoreI0313 11:41:39.419461 9249 net.cpp:122] Setting up upscoreI0313 11:41:39.419472 9249 net.cpp:129] Top shape: 1 21 63 2079 (2750517)I0313 11:41:39.419476 9249 net.cpp:137] Memory required for data: 79635924I0313 11:41:39.419484 9249 layer_factory.hpp:77] Creating layer scoreI0313 11:41:39.419497 9249 net.cpp:84] Creating Layer scoreI0313 11:41:39.419499 9249 net.cpp:406] score &lt;- upscoreI0313 11:41:39.419503 9249 net.cpp:406] score &lt;- data_data_0_split_1I0313 11:41:39.419507 9249 net.cpp:380] score -&gt; scoreI0313 11:41:39.419517 9249 net.cpp:122] Setting up scoreI0313 11:41:39.419539 9249 net.cpp:129] Top shape: 1 21 16 2016 (677376)I0313 11:41:39.419543 9249 net.cpp:137] Memory required for data: 82345428I0313 11:41:39.419544 9249 layer_factory.hpp:77] Creating layer lossI0313 11:41:39.419554 9249 net.cpp:84] Creating Layer lossI0313 11:41:39.419558 9249 net.cpp:406] loss &lt;- scoreI0313 11:41:39.419560 9249 net.cpp:406] loss &lt;- labelI0313 11:41:39.419564 9249 net.cpp:380] loss -&gt; lossI0313 11:41:39.419572 9249 layer_factory.hpp:77] Creating layer lossI0313 11:41:39.420116 9249 net.cpp:122] Setting up lossI0313 11:41:39.420122 9249 net.cpp:129] Top shape: (1)I0313 11:41:39.420125 9249 net.cpp:132] with loss weight 1I0313 11:41:39.420135 9249 net.cpp:137] Memory required for data: 82345432I0313 11:41:39.420137 9249 net.cpp:198] loss needs backward computation.I0313 11:41:39.420140 9249 net.cpp:198] score needs backward computation.I0313 11:41:39.420143 9249 net.cpp:198] upscore needs backward computation.I0313 11:41:39.420145 9249 net.cpp:198] score_fr needs backward computation.I0313 11:41:39.420148 9249 net.cpp:198] drop7 needs backward computation.I0313 11:41:39.420151 9249 net.cpp:198] relu7 needs backward computation.I0313 11:41:39.420155 9249 net.cpp:198] fc7 needs backward computation.I0313 11:41:39.420156 9249 net.cpp:198] drop6 needs backward computation.I0313 11:41:39.420159 9249 net.cpp:198] relu6 needs backward computation.I0313 11:41:39.420161 9249 net.cpp:198] fc6 needs backward computation.I0313 11:41:39.420164 9249 net.cpp:198] pool5 needs backward computation.I0313 11:41:39.420167 9249 net.cpp:198] relu5 needs backward computation.I0313 11:41:39.420169 9249 net.cpp:198] conv5 needs backward computation.I0313 11:41:39.420172 9249 net.cpp:198] relu4 needs backward computation.I0313 11:41:39.420176 9249 net.cpp:198] conv4 needs backward computation.I0313 11:41:39.420177 9249 net.cpp:198] relu3 needs backward computation.I0313 11:41:39.420181 9249 net.cpp:198] conv3 needs backward computation.I0313 11:41:39.420183 9249 net.cpp:198] norm2 needs backward computation.I0313 11:41:39.420186 9249 net.cpp:198] pool2 needs backward computation.I0313 11:41:39.420189 9249 net.cpp:198] relu2 needs backward computation.I0313 11:41:39.420192 9249 net.cpp:198] conv2 needs backward computation.I0313 11:41:39.420194 9249 net.cpp:198] norm1 needs backward computation.I0313 11:41:39.420197 9249 net.cpp:198] pool1 needs backward computation.I0313 11:41:39.420200 9249 net.cpp:198] relu1 needs backward computation.I0313 11:41:39.420203 9249 net.cpp:198] conv1 needs backward computation.I0313 11:41:39.420207 9249 net.cpp:200] data_data_0_split does not need backward computation.I0313 11:41:39.420210 9249 net.cpp:200] data does not need backward computation.I0313 11:41:39.420212 9249 net.cpp:242] This network produces output lossI0313 11:41:39.420224 9249 net.cpp:255] Network initialization done.I0313 11:41:39.420586 9249 solver.cpp:190] Creating test net (#0) specified by test_net file: val.prototxtI0313 11:41:39.420764 9249 net.cpp:51] Initializing net from parameters: state &#123; phase: TEST&#125;layer &#123; name: &quot;data&quot; type: &quot;Python&quot; top: &quot;data&quot; top: &quot;label&quot; python_param &#123; module: &quot;pcl_data_layer&quot; layer: &quot;PCLSegDataLayer&quot; param_str: &quot;&#123;\&apos;pcl_dir\&apos;: \&apos;/home/zzy/CLionProjects/ROS_Project/ws/src/fcn_data_gen/data/npy\&apos;&#125;&quot; &#125;&#125;layer &#123; name: &quot;conv1&quot; type: &quot;Convolution&quot; bottom: &quot;data&quot; top: &quot;conv1&quot; convolution_param &#123; num_output: 96 pad: 100 kernel_size: 11 group: 1 stride: 4 &#125;&#125;layer &#123; name: &quot;relu1&quot; type: &quot;ReLU&quot; bottom: &quot;conv1&quot; top: &quot;conv1&quot;&#125;layer &#123; name: &quot;pool1&quot; type: &quot;Pooling&quot; bottom: &quot;conv1&quot; top: &quot;pool1&quot; pooling_param &#123; pool: MAX kernel_size: 3 stride: 2 &#125;&#125;layer &#123; name: &quot;norm1&quot; type: &quot;LRN&quot; bottom: &quot;pool1&quot; top: &quot;norm1&quot; lrn_param &#123; local_size: 5 alpha: 0.0001 beta: 0.75 &#125;&#125;layer &#123; name: &quot;conv2&quot; type: &quot;Convolution&quot; bottom: &quot;norm1&quot; top: &quot;conv2&quot; convolution_param &#123; num_output: 256 pad: 2 kernel_size: 5 group: 2 stride: 1 &#125;&#125;layer &#123; name: &quot;relu2&quot; type: &quot;ReLU&quot; bottom: &quot;conv2&quot; top: &quot;conv2&quot;&#125;layer &#123; name: &quot;pool2&quot; type: &quot;Pooling&quot; bottom: &quot;conv2&quot; top: &quot;pool2&quot; pooling_param &#123; pool: MAX kernel_size: 3 stride: 2 &#125;&#125;layer &#123; name: &quot;norm2&quot; type: &quot;LRN&quot; bottom: &quot;pool2&quot; top: &quot;norm2&quot; lrn_param &#123; local_size: 5 alpha: 0.0001 beta: 0.75 &#125;&#125;layer &#123; name: &quot;conv3&quot; type: &quot;Convolution&quot; bottom: &quot;norm2&quot; top: &quot;conv3&quot; convolution_param &#123; num_output: 384 pad: 1 kernel_size: 3 group: 1 stride: 1 &#125;&#125;layer &#123; name: &quot;relu3&quot; type: &quot;ReLU&quot; bottom: &quot;conv3&quot; top: &quot;conv3&quot;&#125;layer &#123; name: &quot;conv4&quot; type: &quot;Convolution&quot; bottom: &quot;conv3&quot; top: &quot;conv4&quot; convolution_param &#123; num_output: 384 pad: 1 kernel_size: 3 group: 2 stride: 1 &#125;&#125;layer &#123; name: &quot;relu4&quot; type: &quot;ReLU&quot; bottom: &quot;conv4&quot; top: &quot;conv4&quot;&#125;layer &#123; name: &quot;conv5&quot; type: &quot;Convolution&quot; bottom: &quot;conv4&quot; top: &quot;conv5&quot; convolution_param &#123; num_output: 256 pad: 1 kernel_size: 3 group: 2 stride: 1 &#125;&#125;layer &#123; name: &quot;relu5&quot; type: &quot;ReLU&quot; bottom: &quot;conv5&quot; top: &quot;conv5&quot;&#125;layer &#123; name: &quot;pool5&quot; type: &quot;Pooling&quot; bottom: &quot;conv5&quot; top: &quot;pool5&quot; pooling_param &#123; pool: MAX kernel_size: 3 stride: 2 &#125;&#125;layer &#123; name: &quot;fc6&quot; type: &quot;Convolution&quot; bottom: &quot;pool5&quot; top: &quot;fc6&quot; convolution_param &#123; num_output: 4096 pad: 0 kernel_size: 6 group: 1 stride: 1 &#125;&#125;layer &#123; name: &quot;relu6&quot; type: &quot;ReLU&quot; bottom: &quot;fc6&quot; top: &quot;fc6&quot;&#125;layer &#123; name: &quot;drop6&quot; type: &quot;Dropout&quot; bottom: &quot;fc6&quot; top: &quot;fc6&quot; dropout_param &#123; dropout_ratio: 0.5 &#125;&#125;layer &#123; name: &quot;fc7&quot; type: &quot;Convolution&quot; bottom: &quot;fc6&quot; top: &quot;fc7&quot; convolution_param &#123; num_output: 4096 pad: 0 kernel_size: 1 group: 1 stride: 1 &#125;&#125;layer &#123; name: &quot;relu7&quot; type: &quot;ReLU&quot; bottom: &quot;fc7&quot; top: &quot;fc7&quot;&#125;layer &#123; name: &quot;drop7&quot; type: &quot;Dropout&quot; bottom: &quot;fc7&quot; top: &quot;fc7&quot; dropout_param &#123; dropout_ratio: 0.5 &#125;&#125;layer &#123; name: &quot;score_fr&quot; type: &quot;Convolution&quot; bottom: &quot;fc7&quot; top: &quot;score_fr&quot; param &#123; lr_mult: 1 decay_mult: 1 &#125; param &#123; lr_mult: 2 decay_mult: 0 &#125; convolution_param &#123; num_output: 21 pad: 0 kernel_size: 1 &#125;&#125;layer &#123; name: &quot;upscore&quot; type: &quot;Deconvolution&quot; bottom: &quot;score_fr&quot; top: &quot;upscore&quot; param &#123; lr_mult: 0 &#125; convolution_param &#123; num_output: 21 bias_term: false kernel_size: 63 stride: 32 &#125;&#125;layer &#123; name: &quot;score&quot; type: &quot;Crop&quot; bottom: &quot;upscore&quot; bottom: &quot;data&quot; top: &quot;score&quot; crop_param &#123; axis: 2 offset: 18 &#125;&#125;layer &#123; name: &quot;loss&quot; type: &quot;SoftmaxWithLoss&quot; bottom: &quot;score&quot; bottom: &quot;label&quot; top: &quot;loss&quot; loss_param &#123; ignore_label: 255 normalize: true &#125;&#125;I0313 11:41:39.420830 9249 layer_factory.hpp:77] Creating layer dataI0313 11:41:39.420866 9249 net.cpp:84] Creating Layer dataI0313 11:41:39.420871 9249 net.cpp:380] data -&gt; dataI0313 11:41:39.420877 9249 net.cpp:380] data -&gt; labelI0313 11:41:39.422286 9249 net.cpp:122] Setting up dataI0313 11:41:39.422296 9249 net.cpp:129] Top shape: 1 3 16 2016 (96768)I0313 11:41:39.422299 9249 net.cpp:129] Top shape: 1 16 2016 (32256)I0313 11:41:39.422302 9249 net.cpp:137] Memory required for data: 516096I0313 11:41:39.422305 9249 layer_factory.hpp:77] Creating layer data_data_0_splitI0313 11:41:39.422310 9249 net.cpp:84] Creating Layer data_data_0_splitI0313 11:41:39.422313 9249 net.cpp:406] data_data_0_split &lt;- dataI0313 11:41:39.422317 9249 net.cpp:380] data_data_0_split -&gt; data_data_0_split_0I0313 11:41:39.422322 9249 net.cpp:380] data_data_0_split -&gt; data_data_0_split_1I0313 11:41:39.422327 9249 net.cpp:122] Setting up data_data_0_splitI0313 11:41:39.422332 9249 net.cpp:129] Top shape: 1 3 16 2016 (96768)I0313 11:41:39.422334 9249 net.cpp:129] Top shape: 1 3 16 2016 (96768)I0313 11:41:39.422338 9249 net.cpp:137] Memory required for data: 1290240I0313 11:41:39.422339 9249 layer_factory.hpp:77] Creating layer conv1I0313 11:41:39.422346 9249 net.cpp:84] Creating Layer conv1I0313 11:41:39.422349 9249 net.cpp:406] conv1 &lt;- data_data_0_split_0I0313 11:41:39.422353 9249 net.cpp:380] conv1 -&gt; conv1I0313 11:41:39.422446 9249 net.cpp:122] Setting up conv1I0313 11:41:39.422451 9249 net.cpp:129] Top shape: 1 96 52 552 (2755584)I0313 11:41:39.422454 9249 net.cpp:137] Memory required for data: 12312576I0313 11:41:39.422461 9249 layer_factory.hpp:77] Creating layer relu1I0313 11:41:39.422466 9249 net.cpp:84] Creating Layer relu1I0313 11:41:39.422469 9249 net.cpp:406] relu1 &lt;- conv1I0313 11:41:39.422472 9249 net.cpp:367] relu1 -&gt; conv1 (in-place)I0313 11:41:39.422477 9249 net.cpp:122] Setting up relu1I0313 11:41:39.422479 9249 net.cpp:129] Top shape: 1 96 52 552 (2755584)I0313 11:41:39.422482 9249 net.cpp:137] Memory required for data: 23334912I0313 11:41:39.422484 9249 layer_factory.hpp:77] Creating layer pool1I0313 11:41:39.422488 9249 net.cpp:84] Creating Layer pool1I0313 11:41:39.422490 9249 net.cpp:406] pool1 &lt;- conv1I0313 11:41:39.422495 9249 net.cpp:380] pool1 -&gt; pool1I0313 11:41:39.422502 9249 net.cpp:122] Setting up pool1I0313 11:41:39.422504 9249 net.cpp:129] Top shape: 1 96 26 276 (688896)I0313 11:41:39.422507 9249 net.cpp:137] Memory required for data: 26090496I0313 11:41:39.422508 9249 layer_factory.hpp:77] Creating layer norm1I0313 11:41:39.422513 9249 net.cpp:84] Creating Layer norm1I0313 11:41:39.422516 9249 net.cpp:406] norm1 &lt;- pool1I0313 11:41:39.422519 9249 net.cpp:380] norm1 -&gt; norm1I0313 11:41:39.422524 9249 net.cpp:122] Setting up norm1I0313 11:41:39.422528 9249 net.cpp:129] Top shape: 1 96 26 276 (688896)I0313 11:41:39.422529 9249 net.cpp:137] Memory required for data: 28846080I0313 11:41:39.422531 9249 layer_factory.hpp:77] Creating layer conv2I0313 11:41:39.422536 9249 net.cpp:84] Creating Layer conv2I0313 11:41:39.422539 9249 net.cpp:406] conv2 &lt;- norm1I0313 11:41:39.422543 9249 net.cpp:380] conv2 -&gt; conv2I0313 11:41:39.422933 9249 net.cpp:122] Setting up conv2I0313 11:41:39.422940 9249 net.cpp:129] Top shape: 1 256 26 276 (1837056)I0313 11:41:39.422941 9249 net.cpp:137] Memory required for data: 36194304I0313 11:41:39.422947 9249 layer_factory.hpp:77] Creating layer relu2I0313 11:41:39.422951 9249 net.cpp:84] Creating Layer relu2I0313 11:41:39.422955 9249 net.cpp:406] relu2 &lt;- conv2I0313 11:41:39.422958 9249 net.cpp:367] relu2 -&gt; conv2 (in-place)I0313 11:41:39.422962 9249 net.cpp:122] Setting up relu2I0313 11:41:39.422966 9249 net.cpp:129] Top shape: 1 256 26 276 (1837056)I0313 11:41:39.422967 9249 net.cpp:137] Memory required for data: 43542528I0313 11:41:39.422971 9249 layer_factory.hpp:77] Creating layer pool2I0313 11:41:39.422973 9249 net.cpp:84] Creating Layer pool2I0313 11:41:39.422976 9249 net.cpp:406] pool2 &lt;- conv2I0313 11:41:39.422979 9249 net.cpp:380] pool2 -&gt; pool2I0313 11:41:39.422984 9249 net.cpp:122] Setting up pool2I0313 11:41:39.422988 9249 net.cpp:129] Top shape: 1 256 13 138 (459264)I0313 11:41:39.422991 9249 net.cpp:137] Memory required for data: 45379584I0313 11:41:39.422992 9249 layer_factory.hpp:77] Creating layer norm2I0313 11:41:39.422997 9249 net.cpp:84] Creating Layer norm2I0313 11:41:39.422999 9249 net.cpp:406] norm2 &lt;- pool2I0313 11:41:39.423003 9249 net.cpp:380] norm2 -&gt; norm2I0313 11:41:39.423008 9249 net.cpp:122] Setting up norm2I0313 11:41:39.423012 9249 net.cpp:129] Top shape: 1 256 13 138 (459264)I0313 11:41:39.423013 9249 net.cpp:137] Memory required for data: 47216640I0313 11:41:39.423015 9249 layer_factory.hpp:77] Creating layer conv3I0313 11:41:39.423020 9249 net.cpp:84] Creating Layer conv3I0313 11:41:39.423023 9249 net.cpp:406] conv3 &lt;- norm2I0313 11:41:39.423027 9249 net.cpp:380] conv3 -&gt; conv3I0313 11:41:39.423882 9249 net.cpp:122] Setting up conv3I0313 11:41:39.423888 9249 net.cpp:129] Top shape: 1 384 13 138 (688896)I0313 11:41:39.423892 9249 net.cpp:137] Memory required for data: 49972224I0313 11:41:39.423897 9249 layer_factory.hpp:77] Creating layer relu3I0313 11:41:39.423902 9249 net.cpp:84] Creating Layer relu3I0313 11:41:39.423904 9249 net.cpp:406] relu3 &lt;- conv3I0313 11:41:39.423907 9249 net.cpp:367] relu3 -&gt; conv3 (in-place)I0313 11:41:39.423912 9249 net.cpp:122] Setting up relu3I0313 11:41:39.423914 9249 net.cpp:129] Top shape: 1 384 13 138 (688896)I0313 11:41:39.423918 9249 net.cpp:137] Memory required for data: 52727808I0313 11:41:39.423919 9249 layer_factory.hpp:77] Creating layer conv4I0313 11:41:39.423923 9249 net.cpp:84] Creating Layer conv4I0313 11:41:39.423925 9249 net.cpp:406] conv4 &lt;- conv3I0313 11:41:39.423930 9249 net.cpp:380] conv4 -&gt; conv4I0313 11:41:39.424738 9249 net.cpp:122] Setting up conv4I0313 11:41:39.424744 9249 net.cpp:129] Top shape: 1 384 13 138 (688896)I0313 11:41:39.424747 9249 net.cpp:137] Memory required for data: 55483392I0313 11:41:39.424751 9249 layer_factory.hpp:77] Creating layer relu4I0313 11:41:39.424756 9249 net.cpp:84] Creating Layer relu4I0313 11:41:39.424757 9249 net.cpp:406] relu4 &lt;- conv4I0313 11:41:39.424762 9249 net.cpp:367] relu4 -&gt; conv4 (in-place)I0313 11:41:39.424764 9249 net.cpp:122] Setting up relu4I0313 11:41:39.424767 9249 net.cpp:129] Top shape: 1 384 13 138 (688896)I0313 11:41:39.424770 9249 net.cpp:137] Memory required for data: 58238976I0313 11:41:39.424772 9249 layer_factory.hpp:77] Creating layer conv5I0313 11:41:39.424777 9249 net.cpp:84] Creating Layer conv5I0313 11:41:39.424779 9249 net.cpp:406] conv5 &lt;- conv4I0313 11:41:39.424784 9249 net.cpp:380] conv5 -&gt; conv5I0313 11:41:39.425376 9249 net.cpp:122] Setting up conv5I0313 11:41:39.425384 9249 net.cpp:129] Top shape: 1 256 13 138 (459264)I0313 11:41:39.425385 9249 net.cpp:137] Memory required for data: 60076032I0313 11:41:39.425393 9249 layer_factory.hpp:77] Creating layer relu5I0313 11:41:39.425397 9249 net.cpp:84] Creating Layer relu5I0313 11:41:39.425400 9249 net.cpp:406] relu5 &lt;- conv5I0313 11:41:39.425403 9249 net.cpp:367] relu5 -&gt; conv5 (in-place)I0313 11:41:39.425406 9249 net.cpp:122] Setting up relu5I0313 11:41:39.425410 9249 net.cpp:129] Top shape: 1 256 13 138 (459264)I0313 11:41:39.425412 9249 net.cpp:137] Memory required for data: 61913088I0313 11:41:39.425415 9249 layer_factory.hpp:77] Creating layer pool5I0313 11:41:39.425420 9249 net.cpp:84] Creating Layer pool5I0313 11:41:39.425423 9249 net.cpp:406] pool5 &lt;- conv5I0313 11:41:39.425426 9249 net.cpp:380] pool5 -&gt; pool5I0313 11:41:39.425432 9249 net.cpp:122] Setting up pool5I0313 11:41:39.425436 9249 net.cpp:129] Top shape: 1 256 6 69 (105984)I0313 11:41:39.425437 9249 net.cpp:137] Memory required for data: 62337024I0313 11:41:39.425441 9249 layer_factory.hpp:77] Creating layer fc6I0313 11:41:39.425446 9249 net.cpp:84] Creating Layer fc6I0313 11:41:39.425448 9249 net.cpp:406] fc6 &lt;- pool5I0313 11:41:39.425452 9249 net.cpp:380] fc6 -&gt; fc6I0313 11:41:39.454087 9249 net.cpp:122] Setting up fc6I0313 11:41:39.454115 9249 net.cpp:129] Top shape: 1 4096 1 64 (262144)I0313 11:41:39.454118 9249 net.cpp:137] Memory required for data: 63385600I0313 11:41:39.454126 9249 layer_factory.hpp:77] Creating layer relu6I0313 11:41:39.454134 9249 net.cpp:84] Creating Layer relu6I0313 11:41:39.454138 9249 net.cpp:406] relu6 &lt;- fc6I0313 11:41:39.454143 9249 net.cpp:367] relu6 -&gt; fc6 (in-place)I0313 11:41:39.454149 9249 net.cpp:122] Setting up relu6I0313 11:41:39.454152 9249 net.cpp:129] Top shape: 1 4096 1 64 (262144)I0313 11:41:39.454155 9249 net.cpp:137] Memory required for data: 64434176I0313 11:41:39.454157 9249 layer_factory.hpp:77] Creating layer drop6I0313 11:41:39.454162 9249 net.cpp:84] Creating Layer drop6I0313 11:41:39.454165 9249 net.cpp:406] drop6 &lt;- fc6I0313 11:41:39.454169 9249 net.cpp:367] drop6 -&gt; fc6 (in-place)I0313 11:41:39.454174 9249 net.cpp:122] Setting up drop6I0313 11:41:39.454177 9249 net.cpp:129] Top shape: 1 4096 1 64 (262144)I0313 11:41:39.454180 9249 net.cpp:137] Memory required for data: 65482752I0313 11:41:39.454182 9249 layer_factory.hpp:77] Creating layer fc7I0313 11:41:39.454188 9249 net.cpp:84] Creating Layer fc7I0313 11:41:39.454190 9249 net.cpp:406] fc7 &lt;- fc6I0313 11:41:39.454195 9249 net.cpp:380] fc7 -&gt; fc7I0313 11:41:39.467375 9249 net.cpp:122] Setting up fc7I0313 11:41:39.467401 9249 net.cpp:129] Top shape: 1 4096 1 64 (262144)I0313 11:41:39.467403 9249 net.cpp:137] Memory required for data: 66531328I0313 11:41:39.467411 9249 layer_factory.hpp:77] Creating layer relu7I0313 11:41:39.467418 9249 net.cpp:84] Creating Layer relu7I0313 11:41:39.467422 9249 net.cpp:406] relu7 &lt;- fc7I0313 11:41:39.467427 9249 net.cpp:367] relu7 -&gt; fc7 (in-place)I0313 11:41:39.467433 9249 net.cpp:122] Setting up relu7I0313 11:41:39.467437 9249 net.cpp:129] Top shape: 1 4096 1 64 (262144)I0313 11:41:39.467439 9249 net.cpp:137] Memory required for data: 67579904I0313 11:41:39.467442 9249 layer_factory.hpp:77] Creating layer drop7I0313 11:41:39.467449 9249 net.cpp:84] Creating Layer drop7I0313 11:41:39.467452 9249 net.cpp:406] drop7 &lt;- fc7I0313 11:41:39.467455 9249 net.cpp:367] drop7 -&gt; fc7 (in-place)I0313 11:41:39.467460 9249 net.cpp:122] Setting up drop7I0313 11:41:39.467463 9249 net.cpp:129] Top shape: 1 4096 1 64 (262144)I0313 11:41:39.467465 9249 net.cpp:137] Memory required for data: 68628480I0313 11:41:39.467468 9249 layer_factory.hpp:77] Creating layer score_frI0313 11:41:39.467474 9249 net.cpp:84] Creating Layer score_frI0313 11:41:39.467476 9249 net.cpp:406] score_fr &lt;- fc7I0313 11:41:39.467481 9249 net.cpp:380] score_fr -&gt; score_frI0313 11:41:39.467617 9249 net.cpp:122] Setting up score_frI0313 11:41:39.467622 9249 net.cpp:129] Top shape: 1 21 1 64 (1344)I0313 11:41:39.467624 9249 net.cpp:137] Memory required for data: 68633856I0313 11:41:39.467629 9249 layer_factory.hpp:77] Creating layer upscoreI0313 11:41:39.467635 9249 net.cpp:84] Creating Layer upscoreI0313 11:41:39.467638 9249 net.cpp:406] upscore &lt;- score_frI0313 11:41:39.467643 9249 net.cpp:380] upscore -&gt; upscoreI0313 11:41:39.469235 9249 net.cpp:122] Setting up upscoreI0313 11:41:39.469246 9249 net.cpp:129] Top shape: 1 21 63 2079 (2750517)I0313 11:41:39.469249 9249 net.cpp:137] Memory required for data: 79635924I0313 11:41:39.469259 9249 layer_factory.hpp:77] Creating layer scoreI0313 11:41:39.469266 9249 net.cpp:84] Creating Layer scoreI0313 11:41:39.469269 9249 net.cpp:406] score &lt;- upscoreI0313 11:41:39.469272 9249 net.cpp:406] score &lt;- data_data_0_split_1I0313 11:41:39.469276 9249 net.cpp:380] score -&gt; scoreI0313 11:41:39.469285 9249 net.cpp:122] Setting up scoreI0313 11:41:39.469288 9249 net.cpp:129] Top shape: 1 21 16 2016 (677376)I0313 11:41:39.469290 9249 net.cpp:137] Memory required for data: 82345428I0313 11:41:39.469293 9249 layer_factory.hpp:77] Creating layer lossI0313 11:41:39.469300 9249 net.cpp:84] Creating Layer lossI0313 11:41:39.469301 9249 net.cpp:406] loss &lt;- scoreI0313 11:41:39.469305 9249 net.cpp:406] loss &lt;- labelI0313 11:41:39.469308 9249 net.cpp:380] loss -&gt; lossI0313 11:41:39.469314 9249 layer_factory.hpp:77] Creating layer lossI0313 11:41:39.469894 9249 net.cpp:122] Setting up lossI0313 11:41:39.469900 9249 net.cpp:129] Top shape: (1)I0313 11:41:39.469903 9249 net.cpp:132] with loss weight 1I0313 11:41:39.469913 9249 net.cpp:137] Memory required for data: 82345432I0313 11:41:39.469915 9249 net.cpp:198] loss needs backward computation.I0313 11:41:39.469918 9249 net.cpp:198] score needs backward computation.I0313 11:41:39.469920 9249 net.cpp:198] upscore needs backward computation.I0313 11:41:39.469923 9249 net.cpp:198] score_fr needs backward computation.I0313 11:41:39.469926 9249 net.cpp:198] drop7 needs backward computation.I0313 11:41:39.469929 9249 net.cpp:198] relu7 needs backward computation.I0313 11:41:39.469931 9249 net.cpp:198] fc7 needs backward computation.I0313 11:41:39.469934 9249 net.cpp:198] drop6 needs backward computation.I0313 11:41:39.469936 9249 net.cpp:198] relu6 needs backward computation.I0313 11:41:39.469939 9249 net.cpp:198] fc6 needs backward computation.I0313 11:41:39.469943 9249 net.cpp:198] pool5 needs backward computation.I0313 11:41:39.469945 9249 net.cpp:198] relu5 needs backward computation.I0313 11:41:39.469947 9249 net.cpp:198] conv5 needs backward computation.I0313 11:41:39.469950 9249 net.cpp:198] relu4 needs backward computation.I0313 11:41:39.469952 9249 net.cpp:198] conv4 needs backward computation.I0313 11:41:39.469955 9249 net.cpp:198] relu3 needs backward computation.I0313 11:41:39.469957 9249 net.cpp:198] conv3 needs backward computation.I0313 11:41:39.469960 9249 net.cpp:198] norm2 needs backward computation.I0313 11:41:39.469964 9249 net.cpp:198] pool2 needs backward computation.I0313 11:41:39.469965 9249 net.cpp:198] relu2 needs backward computation.I0313 11:41:39.469969 9249 net.cpp:198] conv2 needs backward computation.I0313 11:41:39.469971 9249 net.cpp:198] norm1 needs backward computation.I0313 11:41:39.469974 9249 net.cpp:198] pool1 needs backward computation.I0313 11:41:39.469979 9249 net.cpp:198] relu1 needs backward computation.I0313 11:41:39.469981 9249 net.cpp:198] conv1 needs backward computation.I0313 11:41:39.469985 9249 net.cpp:200] data_data_0_split does not need backward computation.I0313 11:41:39.469987 9249 net.cpp:200] data does not need backward computation.I0313 11:41:39.469990 9249 net.cpp:242] This network produces output lossI0313 11:41:39.470001 9249 net.cpp:255] Network initialization done.I0313 11:41:39.470055 9249 solver.cpp:57] Solver scaffolding done.I0313 11:42:40.745103 9249 solver.cpp:239] Iteration 0 (-1.4013e-45 iter/s, 61.136s/20 iters), loss = 4.54161I0313 11:42:40.745129 9249 solver.cpp:258] Train net output #0: loss = 4.00278 (* 1 = 4.00278 loss)I0313 11:42:40.745136 9249 sgd_solver.cpp:112] Iteration 0, lr = 0.0001I0313 12:02:52.273387 9249 solver.cpp:239] Iteration 20 (0.0165081 iter/s, 1211.53s/20 iters), loss = 17.0233I0313 12:02:52.273416 9249 solver.cpp:258] Train net output #0: loss = 19.2508 (* 1 = 19.2508 loss)I0313 12:02:52.273422 9249 sgd_solver.cpp:112] Iteration 20, lr = 0.0001I0313 12:23:09.810516 9249 solver.cpp:239] Iteration 40 (0.0164266 iter/s, 1217.54s/20 iters), loss = 26.7316I0313 12:23:09.810544 9249 solver.cpp:258] Train net output #0: loss = 30.1355 (* 1 = 30.1355 loss)I0313 12:23:09.810550 9249 sgd_solver.cpp:112] Iteration 40, lr = 0.0001I0313 12:43:32.716285 9249 solver.cpp:239] Iteration 60 (0.0163545 iter/s, 1222.91s/20 iters), loss = 30.2106I0313 12:43:32.716313 9249 solver.cpp:258] Train net output #0: loss = 22.8696 (* 1 = 22.8696 loss)I0313 12:43:32.716320 9249 sgd_solver.cpp:112] Iteration 60, lr = 0.0001I0313 13:03:49.434516 9249 solver.cpp:239] Iteration 80 (0.0164377 iter/s, 1216.72s/20 iters), loss = 31.0818I0313 13:03:49.434543 9249 solver.cpp:258] Train net output #0: loss = 23.1428 (* 1 = 23.1428 loss)I0313 13:03:49.434551 9249 sgd_solver.cpp:112] Iteration 80, lr = 0.0001I0313 13:23:51.860294 9249 solver.cpp:239] Iteration 100 (0.0166331 iter/s, 1202.43s/20 iters), loss = 32.5238I0313 13:23:51.860322 9249 solver.cpp:258] Train net output #0: loss = 35.1909 (* 1 = 35.1909 loss)I0313 13:23:51.860328 9249 sgd_solver.cpp:112] Iteration 100, lr = 0.0001I0313 13:43:38.481149 9249 solver.cpp:239] Iteration 120 (0.0168546 iter/s, 1186.62s/20 iters), loss = 33.0024I0313 13:43:38.481176 9249 solver.cpp:258] Train net output #0: loss = 40.9104 (* 1 = 40.9104 loss)I0313 13:43:38.481182 9249 sgd_solver.cpp:112] Iteration 120, lr = 0.0001I0313 14:03:27.667078 9249 solver.cpp:239] Iteration 140 (0.0168182 iter/s, 1189.19s/20 iters), loss = 36.4908I0313 14:03:27.667104 9249 solver.cpp:258] Train net output #0: loss = 53.9975 (* 1 = 53.9975 loss)I0313 14:03:27.667111 9249 sgd_solver.cpp:112] Iteration 140, lr = 0.0001I0313 14:23:25.009404 9249 solver.cpp:239] Iteration 160 (0.0167037 iter/s, 1197.34s/20 iters), loss = 52.2285I0313 14:23:25.009431 9249 solver.cpp:258] Train net output #0: loss = 26.9314 (* 1 = 26.9314 loss)I0313 14:23:25.009438 9249 sgd_solver.cpp:112] Iteration 160, lr = 0.0001I0313 14:43:35.026921 9249 solver.cpp:239] Iteration 180 (0.0165287 iter/s, 1210.02s/20 iters), loss = 33.087I0313 14:43:35.026950 9249 solver.cpp:258] Train net output #0: loss = 44.6887 (* 1 = 44.6887 loss)I0313 14:43:35.026957 9249 sgd_solver.cpp:112] Iteration 180, lr = 0.0001I0313 15:03:45.718956 9249 solver.cpp:239] Iteration 200 (0.0165195 iter/s, 1210.69s/20 iters), loss = 33.0793I0313 15:03:45.718984 9249 solver.cpp:258] Train net output #0: loss = 34.2235 (* 1 = 34.2235 loss)I0313 15:03:45.718991 9249 sgd_solver.cpp:112] Iteration 200, lr = 0.0001I0313 15:24:27.503715 9249 solver.cpp:239] Iteration 220 (0.0161059 iter/s, 1241.78s/20 iters), loss = 33.1698I0313 15:24:27.503741 9249 solver.cpp:258] Train net output #0: loss = 45.0323 (* 1 = 45.0323 loss)I0313 15:24:27.503748 9249 sgd_solver.cpp:112] Iteration 220, lr = 0.0001I0313 15:44:53.585564 9249 solver.cpp:239] Iteration 240 (0.0163121 iter/s, 1226.08s/20 iters), loss = 35.7697I0313 15:44:53.585592 9249 solver.cpp:258] Train net output #0: loss = 45.5302 (* 1 = 45.5302 loss)I0313 15:44:53.585598 9249 sgd_solver.cpp:112] Iteration 240, lr = 0.0001I0313 16:04:44.744935 9249 solver.cpp:239] Iteration 260 (0.0167904 iter/s, 1191.16s/20 iters), loss = 29.4003I0313 16:04:44.744963 9249 solver.cpp:258] Train net output #0: loss = 19.9242 (* 1 = 19.9242 loss)I0313 16:04:44.744969 9249 sgd_solver.cpp:112] Iteration 260, lr = 0.0001I0313 16:24:00.216655 9249 solver.cpp:239] Iteration 280 (0.017309 iter/s, 1155.47s/20 iters), loss = 24.0391I0313 16:24:00.216681 9249 solver.cpp:258] Train net output #0: loss = 35.6398 (* 1 = 35.6398 loss)I0313 16:24:00.216687 9249 sgd_solver.cpp:112] Iteration 280, lr = 0.0001I0313 16:43:22.672458 9249 solver.cpp:239] Iteration 300 (0.017205 iter/s, 1162.45s/20 iters), loss = 33.2369I0313 16:43:22.672485 9249 solver.cpp:258] Train net output #0: loss = 38.8301 (* 1 = 38.8301 loss)I0313 16:43:22.672492 9249 sgd_solver.cpp:112] Iteration 300, lr = 0.0001I0313 17:02:56.876072 9249 solver.cpp:239] Iteration 320 (0.0170328 iter/s, 1174.2s/20 iters), loss = 33.7243I0313 17:02:56.876101 9249 solver.cpp:258] Train net output #0: loss = 33.6585 (* 1 = 33.6585 loss)I0313 17:02:56.876106 9249 sgd_solver.cpp:112] Iteration 320, lr = 0.0001 可以看到在未更改其他网络参数的情况下，loss居高不下，本文着重与对Data layer的理解，下一篇文章将对网络结构内部进行优化，以达到点云数据loss达到预期的目标。 以上。 参考文献： FCN学习:Semantic Segmentation AlexNet]]></content>
      <categories>
        <category>DL</category>
      </categories>
      <tags>
        <tag>DL</tag>
        <tag>PCL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Caffe源代码学习 — AlexNet(Caffenet.py)]]></title>
    <url>%2F2018%2F03%2F08%2FCaffe%E6%BA%90%E4%BB%A3%E7%A0%81%E5%AD%A6%E4%B9%A0%E4%B9%8BAlexNet_Caffenet_py%2F</url>
    <content type="text"><![CDATA[导言源码位置：caffe/examples/pycaffe/caffenet.py该文件源代码是经典模型AlexNet的Caffe实现，有兴趣的小伙伴去拜读一下论文: ImageNet Classification with Deep Convolutional Neural Networks.pdf). 源码解读1. 导入模块123from __future__ import print_functionfrom caffe import layers as L, params as P, to_protofrom caffe.proto import caffe_pb2 2. 定义Layer函数包括： 卷积层（Convolution Layer）、全连接层（Full Connected Layer）和池化层（Pooling Layer） 2.1 卷积层（Convolution Layer）函数1234def conv_relu(bottom, ks, nout, stride=1, pad=0, group=1): conv = L.Convolution(bottom, kernel_size=ks, stride=stride, num_output=nout, pad=pad, group=group) return conv, L.ReLU(conv, in_place=True) 1. 函数输入 bottom - 输入节点（blob）名 ks - 卷积核尺寸（kernel size） nout - 输出深度尺寸（number output） stride - 卷积核滑窗距离 pad - 图像边缘添加尺寸，即在图像周围一周添加尺寸为pad的空白像素 group - 将数据进行分开训练堆数目 2. 调用Caffe卷基层生成函数 conv = L.Convolution(bottom, kernel_size=ks, stride=stride,num_output=nout, pad=pad, group=group) 3. 返回参数 conv - 卷积层配置 L.ReLU(conv, in_place=True) - 卷积后的数据经过ReLU激活函数得到的数据2.2 全连接层（Full Connected Layer）123def fc_relu(bottom, nout): fc = L.InnerProduct(bottom, num_output=nout) return fc, L.ReLU(fc, in_place=True) 1. 调用Caffe内积函数 fc = L.InnerProduct(bottom, num_output=nout) 2. 返回参数 fc, L.ReLU(fc, in_place=True) - 全连接分类之后数据通过ReLU函数2.3 池化层（Pooling Layer）12def max_pool(bottom, ks, stride=1): return L.Pooling(bottom, pool=P.Pooling.MAX, kernel_size=ks, stride=stride) 调用Caffe池化层生成函数 L.Pooling)（） pool=P.Pooling.MAX - 池化类型选择MAX，即取模板内最大值输出3. 定义网络结构1234567891011121314151617181920212223242526data, label = L.Data(source=lmdb, backend=P.Data.LMDB, batch_size=batch_size, ntop=2, transform_param=dict(crop_size=227, mean_value=[104, 117, 123], mirror=True)) # the net itself conv1, relu1 = conv_relu(data, 11, 96, stride=4) pool1 = max_pool(relu1, 3, stride=2) norm1 = L.LRN(pool1, local_size=5, alpha=1e-4, beta=0.75) conv2, relu2 = conv_relu(norm1, 5, 256, pad=2, group=2) pool2 = max_pool(relu2, 3, stride=2) norm2 = L.LRN(pool2, local_size=5, alpha=1e-4, beta=0.75) conv3, relu3 = conv_relu(norm2, 3, 384, pad=1) conv4, relu4 = conv_relu(relu3, 3, 384, pad=1, group=2) conv5, relu5 = conv_relu(relu4, 3, 256, pad=1, group=2) pool5 = max_pool(relu5, 3, stride=2) fc6, relu6 = fc_relu(pool5, 4096) drop6 = L.Dropout(relu6, in_place=True) fc7, relu7 = fc_relu(drop6, 4096) drop7 = L.Dropout(relu7, in_place=True) fc8 = L.InnerProduct(drop7, num_output=1000) loss = L.SoftmaxWithLoss(fc8, label) if include_acc: acc = L.Accuracy(fc8, label) return to_proto(loss, acc) else: return to_proto(loss) 1. 函数输入 lmdb - 文件名 batch_size - 每次训练输入样本数目 include_acc - 加速？ 2. 调用Caffe数据层输入函数（Data）L.Data(source=lmdb, backend=P.Data.LMDB, batch_size=batch_size, ntop=2, transform_param=dict(crop_size=227, mean_value=[104, 117, 123], mirror=True)) backend - 数据类型 ntop - 输出blob数目，因为数据层处理数据输出data和label，所以值为 2 transform_param - 对单个图片处理： crop_size图片剪裁大小，mean_valueRGB图像需要减去的值（目的是更好突出特征）和mirror镜像处理。 3. 网络结构此博客绘制了AlexNet网络结构图和数据流动图，方便直观理解网络结构，可移步：深度学习之图像分类模型AlexNet解读第1-5层为卷积层，如下表所示：| Layer | Operation | Output || :—- | :————————————— | :——————————-: || Data | crop_size:227, mean_value: [104, 117, 123], mirror: true | data: 227x227x3; label: 227x227x1 || 1 | conv1 -&gt; relu1 -&gt; pool1 -&gt; norm1 | 27x27x96 || 2 | conv2 -&gt; relu2 -&gt; pool2 -&gt; norm2 | 13x13x256 || 3 | conv3 -&gt; relu3 | 11x11x384 || 4 | conv4 -&gt; relu4 | 11x11x384 || 5 | conv5 -&gt; relu5 -&gt; pool5 | 6x6x256 || 6 | fc6 -&gt; relu6 -&gt; drop6 | 4096 || 7 | fc7 -&gt; relu7 -&gt; drop7 | 4096 || 8 | fc8 -&gt; loss | 1000 | 以第1层代码为例进行分析: 第1层 = 卷积层（conv1+relu1） + 池化层（pool1） + 归一化（norm1） （1）. 第1层 - 卷积层（conv1+relu1）作用：提取局部特征，使用ReLU作为CNN的激活函数，并验证其效果在较深的网络超过了Sigmoid，成功解决了Sigmoid在网络较深时的梯度弥散问题。conv1, relu1 = conv_relu(data, 11, 96, stride=4) 数据：数据层输出data数据 卷积核大小： 11 输出节点深度： 96 滑窗距离： 4 （2）. 第1层 - 池化层（pool1）作用：提取最大值，避免平均池化的模糊化效果。在AlexNet中提出让步长比池化核的尺寸小，这样池化层的输出之间会有重叠和覆盖，提升了特征的丰富性。pool1 = max_pool(relu1, 3, stride=2) 数据： relu1 模板核大小： 3 滑窗距离： 2 （3）. 第1层 - 局部响应归一化（Local Response Normalize）（norm1）作用：对局部神经元的活动创建竞争机制，使得其中响应比较大的值变得相对更大，并抑制其他反馈较小的神经元，增强了模型的泛化能力norm1 = L.LRN(pool1, local_size=5, alpha=1e-4, beta=0.75) 数据： pool1 取值模板尺寸： 5 alpha: 0.0001 beta: 0.75 ###4. 输出网络结构文件（.prototxt）123456def make_net(): with open(&apos;train.prototxt&apos;, &apos;w&apos;) as f: print(caffenet(&apos;/path/to/caffe-train-lmdb&apos;), file=f) with open(&apos;test.prototxt&apos;, &apos;w&apos;) as f: print(caffenet(&apos;/path/to/caffe-val-lmdb&apos;, batch_size=50, include_acc=True), file=f) ###5. 运行12if __name__ == &apos;__main__&apos;: make_net() 总结Caffene.py是入门Caffe较好的源代码，结合原论文看，同时能加深对网络结构的理解，补充理论知识。下面根据这个example形式构建自己的网络结构，其中第一步，也是学习深度学习最重要的一步，编写自己的数据类型接口层程序。 以上。 附： AlexNet网络总结 深度学习之图像分类模型AlexNet解读]]></content>
      <categories>
        <category>Caffe</category>
      </categories>
      <tags>
        <tag>Caffe</tag>
        <tag>Python</tag>
        <tag>DL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NumPy学习笔记]]></title>
    <url>%2F2018%2F03%2F08%2FNumpy%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[新建Numpy结构型数据： 12345import numpy as np student = np.dtype([(&apos;name&apos;,&apos;S20&apos;), (&apos;age&apos;,np.int8), (&apos;marks&apos;, np.float4)]) a = np.array([(&apos;abc&apos;, 21, 50),(&apos;xyz&apos;, 18, 75)], dtype = student) print a#输出[(&apos;abc&apos;, 21, 50.0), (&apos;xyz&apos;, 18, 75.0)] 其中，string类型数据用S20表示（20可更改），其余数据类型如np.int8和np.float4均有内建数据表示。 Numpy数组属性：调整数组大小12345678import numpy as np a = np.array([[1,2,3],[4,5,6]]) b = a.reshape(3,2) print b#输出[[1, 2] [3, 4] [5, 6]] 三维数组1234567891011121314151617# 一维数组 import numpy as np a = np.arange(24) a.ndim # 现在调整其大小# 2*4*3： 2个二维数组，每个数组大小4*3b = a.reshape(2,4,3) print b #输出# b 现在拥有三个维度[[[ 0, 1, 2] [ 3, 4, 5] [ 6, 7, 8] [ 9, 10, 11]] [[12, 13, 14] [15, 16, 17] [18, 19, 20] [21, 22, 23]]] Numpy 来自现有数据的数组12345678# 将列表转换为 ndarray import numpy as np x = [1,2,3] a = np.asarray(x) print a#输出[1 2 3] 12345678# 来自元组的 ndarray import numpy as np x = (1,2,3) a = np.asarray(x) print a#输出[1 2 3] Numpy-frombuffer123456import numpy as np s = &apos;Hello World&apos; a = np.frombuffer(s, dtype = &apos;S1&apos;) print a#输出[&apos;H&apos; &apos;e&apos; &apos;l&apos; &apos;l&apos; &apos;o&apos; &apos; &apos; &apos;W&apos; &apos;o&apos; &apos;r&apos; &apos;l&apos; &apos;d&apos;] Numpy-切片和索引基本切片是 Python 中基本切片概念到 n 维的扩展。 通过将start，stop和step参数提供给内置的slice函数来构造一个 Python slice对象。 此slice对象被传递给数组来提取数组的一部分。123456import numpy as npa = np.arange(10)s = slice(2,7,1) # 2返回值为2的索引，7返回值为7的索引，1为步长print a[s]#输出[2 3 4 5 6] 在上面的例子中，ndarray对象由arange()函数创建。 然后，分别用起始，终止和步长值2，7和2定义切片对象。 当这个切片对象传递给ndarray时，会对它的一部分进行切片，从索引2到7，步长为2。 NumPy - 数组上的迭代12345678910111213141516import numpy as npa = np.arange(0,60,5) a = a.reshape(3,4) print &apos;原始数组是：&apos; print a print &apos;\n&apos; print &apos;修改后的数组是：&apos; for x in np.nditer(a): print x#输出原始数组是：[[ 0 5 10 15] [20 25 30 35] [40 45 50 55]]修改后的数组是：0 5 10 15 20 25 30 35 40 45 50 55 注意：迭代的顺序匹配数组的内容布局，而不考虑特定的排序。 这可以通过迭代上述数组的转置来看到。1234567891011121314151617181920212223import numpy as np a = np.arange(0,60,5) a = a.reshape(3,4) print &apos;原始数组是：&apos; print a print &apos;\n&apos; print &apos;原始数组的转置是：&apos; b = a.Tprint bprint &apos;\n&apos; print &apos;修改后的数组是：&apos; for x in np.nditer(b): print x,#输出原始数组是： [[ 0 5 10 15] [20 25 30 35] [40 45 50 55]] 原始数组的转置是： [[ 0 20 40] [ 5 25 45] [10 30 50] [15 35 55]] 修改后的数组是： 0 5 10 15 20 25 30 35 40 45 50 55 numpy.ndarray.flatten 该函数返回折叠为一维的数组副本，函数接受下列参数：1ndarray.flatten(order) 其中： order：C – 按行，F– 按列，A – 原顺序，k – 元素在内存中的出现顺序。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>DL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ROS Node 之间通信打断操作实例]]></title>
    <url>%2F2018%2F03%2F08%2FROS_Node%E4%B9%8B%E9%97%B4%E9%80%9A%E4%BF%A1%E6%89%93%E6%96%AD%E6%93%8D%E4%BD%9C%E5%AE%9E%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[前言公司开发小工具，对文件夹下点云PCD文件进行读取和相应操作，目标功能： 读取文件夹下PCD文件，按照文件名进行排序； 通过Qt开发UI界面，界面包括操作按钮： continue: 循环播放PCD文件并发布 next，pre： 后一帧或前一帧PCD文件 save index： 保存当前帧PCD文件名到.txt文件 当continue操作正在进行时，点击其余按钮，实现打断停止功能 分析 将UI界面和实际后台操作分开进行多线程操作，否则在进行continue过程中时，无法通过外部改变判断条件进行打断； ROS的一个Node默认为是一个进程，所以采用double Node实现多线程； ROS的单个Node可以同时实现subscribe和publish多个消息。本文假设UI界面为Node 1，包括：读取PCD文件，对点击操作进行反应并发送按钮消息到后端；后台实现为Node 2，包括：按钮消息的实现代码。 UI界面 Node 2关键代码由于ROSNode之间特殊的通信机制，如果将条件判断机制放在Node 2的子函数中，那么Node 2在接收Node 1的消息时，如果continue操作正在进行，则必须当continue执行完毕之后再收到Node 1的消息。所以，必须将判断条件房子ROS的Master部分，通过Master对Node 1当前消息进行反应，可实时打断Node 2正在进行的continue操作，马上进行当前消息的操作。以下是Node 2中的main函数的ROS循环部分代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455enum recv_sign &#123; none_sign = 0, stop_sign = 1, continue_sign = 2, load_sign = 3 &#125;;while(ros::ok()) &#123; if( g_con_signal == continue_sign ) &#123; if ( g_cur_index &lt; g_pcd_filelist.size() - 1 ) &#123; publishPCD(); g_cur_index++; &#125; else g_cur_index = 0; &#125; else if ( g_con_signal == stop_sign ) &#123; if ( g_pcd_info == &quot;pre_pcd_signal&quot; ) &#123; if ( g_cur_index &gt; 0 ) &#123; g_cur_index--; publishPCD(); &#125; else std::cerr &lt;&lt; &quot;Reach 1st file!!&quot; &lt;&lt; std::endl; &#125; else if ( g_pcd_info == &quot;next_pcd_signal&quot; ) &#123; if ( g_cur_index &lt; g_pcd_filelist.size() - 1 ) g_cur_index ++; else g_cur_index = 0; publishPCD(); &#125; else if ( g_pcd_info == &quot;save_index&quot; ) &#123; g_outfile &lt;&lt; g_pcd_filelist[g_cur_index] &lt;&lt; std::endl; &#125; g_con_signal = none_sign; &#125; else if ( g_con_signal == load_sign ) &#123; g_file_root_path = g_pcd_info; std::cout &lt;&lt; g_file_root_path &lt;&lt; std::endl; read_filelists( g_file_root_path + &quot;/&quot;, g_pcd_filelist, &quot;.pcd&quot; ); for (int i = 0; i &lt; g_pcd_filelist.size(); ++i) &#123; std::cout &lt;&lt; g_pcd_filelist[i] &lt;&lt; std::endl; &#125; g_con_signal = none_sign; &#125; ros::spinOnce(); &#125; 代码说明： Node 1同时发送2个std_msgs::String：g_con_signal用于控制是否执行循环条件；g_pcd_info用于在不执行continue操作时进行细分操作划分，包括：save index操作的文件路径和loadPCD文件时文件路径。 g_con_signal可以取4个值：enum recv_sign { none_sign = 0, stop_sign = 1, continue_sign = 2, load_sign = 3 };，分别对应不同操作，其中none_sign用于执行除continue操作之外的跳出当前循环，达到只需执行一次的目的，防止陷入死循环（无线循环）。 ros::spinOnce()用于刷新ROS执行条件，每次进入while(ros::ok())循环时，就会内部条件进行判断。 后续：Node 2当前帧文件名返回给Node 1用于显示于UI界面功能尚待加入。 以上。]]></content>
      <categories>
        <category>ROS</category>
      </categories>
      <tags>
        <tag>ROS</tag>
        <tag>PCL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[开篇2017]]></title>
    <url>%2F2017%2F06%2F26%2F%E5%BC%80%E7%AF%872017%2F</url>
    <content type="text"><![CDATA[欢迎来到曾泽宇的个人博客，此博客将用作记录自己的一些技术上的问题和解决方法，借助这一个平台，希望给自己一个记录，让自己多年以后回过头来看看自己也曾经少年过，哈哈！]]></content>
      <tags>
        <tag>MarkDown Writting</tag>
        <tag>Personal</tag>
      </tags>
  </entry>
</search>